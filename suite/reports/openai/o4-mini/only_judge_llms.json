{
    "created": 1752613371.6235478,
    "duration": 31.24422574043274,
    "exitcode": 1,
    "root": "/Users/rahulparundekar/workspaces/elevate-human-experiences/elevate",
    "environment": {},
    "summary": {
        "passed": 4,
        "failed": 1,
        "total": 5,
        "collected": 5
    },
    "collectors": [
        {
            "nodeid": "",
            "outcome": "passed",
            "result": [
                {
                    "nodeid": "tests/test_only_judge_llms.py",
                    "type": "Module"
                }
            ]
        },
        {
            "nodeid": "tests/test_only_judge_llms.py",
            "outcome": "passed",
            "result": [
                {
                    "nodeid": "tests/test_only_judge_llms.py::test_summary_evaluation",
                    "type": "Coroutine",
                    "lineno": 30
                },
                {
                    "nodeid": "tests/test_only_judge_llms.py::test_conversational_evaluation",
                    "type": "Coroutine",
                    "lineno": 52
                },
                {
                    "nodeid": "tests/test_only_judge_llms.py::test_creative_writing_evaluation",
                    "type": "Coroutine",
                    "lineno": 77
                },
                {
                    "nodeid": "tests/test_only_judge_llms.py::test_instructional_evaluation",
                    "type": "Coroutine",
                    "lineno": 101
                },
                {
                    "nodeid": "tests/test_only_judge_llms.py::test_poetic_evaluation",
                    "type": "Coroutine",
                    "lineno": 125
                }
            ]
        }
    ],
    "tests": [
        {
            "nodeid": "tests/test_only_judge_llms.py::test_summary_evaluation",
            "lineno": 30,
            "outcome": "passed",
            "keywords": [
                "test_summary_evaluation",
                "asyncio",
                "pytestmark",
                "test_only_judge_llms.py",
                "tests",
                "elevate",
                ""
            ],
            "setup": {
                "duration": 0.00032837502658367157,
                "outcome": "passed"
            },
            "call": {
                "duration": 4.612590916920453,
                "outcome": "passed"
            },
            "teardown": {
                "duration": 0.002237624954432249,
                "outcome": "passed"
            }
        },
        {
            "nodeid": "tests/test_only_judge_llms.py::test_conversational_evaluation",
            "lineno": 52,
            "outcome": "failed",
            "keywords": [
                "test_conversational_evaluation",
                "asyncio",
                "pytestmark",
                "test_only_judge_llms.py",
                "tests",
                "elevate",
                ""
            ],
            "setup": {
                "duration": 0.0003454168327152729,
                "outcome": "passed"
            },
            "call": {
                "duration": 3.652800541371107,
                "outcome": "failed",
                "crash": {
                    "path": "/Users/rahulparundekar/workspaces/elevate-human-experiences/elevate/tests/test_only_judge_llms.py",
                    "lineno": 69,
                    "message": "assert 5 <= 3\n +  where 5 = ConversationCriteria(relevance=5, helpfulness=4, conciseness=5).relevance"
                },
                "traceback": [
                    {
                        "path": "tests/test_only_judge_llms.py",
                        "lineno": 69,
                        "message": "AssertionError"
                    }
                ],
                "longrepr": "settings = Settings(with_model='openai/o4-mini')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_conversational_evaluation(settings: Any) -> None:\n        class ConversationCriteria(BaseModel):\n            relevance: int = Field(..., description=\"How relevant is the reply? (1-5)\")\n            helpfulness: int = Field(..., description=\"How helpful is the response? (1-5)\")\n            conciseness: int = Field(..., description=\"How concise is the answer? (1-5)\")\n    \n        # This reply doesn't provide useful info about a product launch, so we expect lower scores for relevance/helpfulness.\n        sample_text = (\n            \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. \"\n            \"It's a beautiful day, though. How have you been?\"\n        )\n        judge = OnlyJudgeLLMs(with_model=settings.with_model)\n        result: Any = await judge.evaluate(sample_text, ConversationCriteria)\n    \n        # Relevance should be lower because it doesn't address the product launch well\n>       assert 1 <= result.relevance <= 3\nE       assert 5 <= 3\nE        +  where 5 = ConversationCriteria(relevance=5, helpfulness=4, conciseness=5).relevance\n\ntests/test_only_judge_llms.py:69: AssertionError"
            },
            "teardown": {
                "duration": 0.0005898750387132168,
                "outcome": "passed"
            }
        },
        {
            "nodeid": "tests/test_only_judge_llms.py::test_creative_writing_evaluation",
            "lineno": 77,
            "outcome": "passed",
            "keywords": [
                "test_creative_writing_evaluation",
                "asyncio",
                "pytestmark",
                "test_only_judge_llms.py",
                "tests",
                "elevate",
                ""
            ],
            "setup": {
                "duration": 0.00014733290299773216,
                "outcome": "passed"
            },
            "call": {
                "duration": 7.248557500075549,
                "outcome": "passed"
            },
            "teardown": {
                "duration": 0.0006287922151386738,
                "outcome": "passed"
            }
        },
        {
            "nodeid": "tests/test_only_judge_llms.py::test_instructional_evaluation",
            "lineno": 101,
            "outcome": "passed",
            "keywords": [
                "test_instructional_evaluation",
                "asyncio",
                "pytestmark",
                "test_only_judge_llms.py",
                "tests",
                "elevate",
                ""
            ],
            "setup": {
                "duration": 0.00016737496480345726,
                "outcome": "passed"
            },
            "call": {
                "duration": 8.841564041096717,
                "outcome": "passed"
            },
            "teardown": {
                "duration": 0.0013947500847280025,
                "outcome": "passed"
            }
        },
        {
            "nodeid": "tests/test_only_judge_llms.py::test_poetic_evaluation",
            "lineno": 125,
            "outcome": "passed",
            "keywords": [
                "test_poetic_evaluation",
                "asyncio",
                "pytestmark",
                "test_only_judge_llms.py",
                "tests",
                "elevate",
                ""
            ],
            "setup": {
                "duration": 0.0004453333094716072,
                "outcome": "passed"
            },
            "call": {
                "duration": 5.587980541866273,
                "outcome": "passed"
            },
            "teardown": {
                "duration": 0.001396875362843275,
                "outcome": "passed"
            }
        }
    ]
}
