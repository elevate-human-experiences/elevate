{"created": 1749488805.7061398, "duration": 20.23307967185974, "exitcode": 1, "root": " Developer/OpenSource/elevate", "environment": {}, "summary": {"passed": 31, "failed": 26, "total": 57, "collected": 57}, "collectors": [{"nodeid": "", "outcome": "passed", "result": [{"nodeid": ".", "type": "Dir"}]}, {"nodeid": "agent", "outcome": "passed", "result": []}, {"nodeid": "src/common", "outcome": "passed", "result": []}, {"nodeid": "src/elevate", "outcome": "passed", "result": []}, {"nodeid": "src", "outcome": "passed", "result": [{"nodeid": "src/common", "type": "Package"}, {"nodeid": "src/elevate", "type": "Package"}]}, {"nodeid": "suite/reports/report_gemini", "outcome": "passed", "result": []}, {"nodeid": "suite/reports/report_groq", "outcome": "passed", "result": []}, {"nodeid": "suite/reports", "outcome": "passed", "result": [{"nodeid": "suite/reports/report_gemini", "type": "Dir"}, {"nodeid": "suite/reports/report_groq", "type": "Dir"}]}, {"nodeid": "suite", "outcome": "passed", "result": [{"nodeid": "suite/reports", "type": "Dir"}]}, {"nodeid": "tests/test_elevate.py", "outcome": "passed", "result": [{"nodeid": "tests/test_elevate.py::test_elevate", "type": "Coroutine", "lineno": 27}]}, {"nodeid": "tests/test_only_audiocast.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_audiocast.py::test_create_default_audiocast", "type": "Coroutine", "lineno": 35}, {"nodeid": "tests/test_only_audiocast.py::test_create_two_people_podcast", "type": "Coroutine", "lineno": 45}, {"nodeid": "tests/test_only_audiocast.py::test_create_art_teacher_conversation", "type": "Coroutine", "lineno": 90}, {"nodeid": "tests/test_only_audiocast.py::test_create_children_storytelling_session", "type": "Coroutine", "lineno": 133}]}, {"nodeid": "tests/test_only_email.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_email.py::test_personal_email", "type": "Coroutine", "lineno": 36}, {"nodeid": "tests/test_only_email.py::test_professional_email", "type": "Coroutine", "lineno": 47}, {"nodeid": "tests/test_only_email.py::test_marketing_email", "type": "Coroutine", "lineno": 58}, {"nodeid": "tests/test_only_email.py::test_resignation_email", "type": "Coroutine", "lineno": 81}, {"nodeid": "tests/test_only_email.py::test_workplace_conflict_email", "type": "Coroutine", "lineno": 93}, {"nodeid": "tests/test_only_email.py::test_bill_dispute_email", "type": "Coroutine", "lineno": 105}, {"nodeid": "tests/test_only_email.py::test_baby_shower_invite_email", "type": "Coroutine", "lineno": 117}, {"nodeid": "tests/test_only_email.py::test_urgent_meeting_email", "type": "Coroutine", "lineno": 129}]}, {"nodeid": "tests/test_only_json.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_json.py::test_calendar_event", "type": "Coroutine", "lineno": 33}, {"nodeid": "tests/test_only_json.py::test_extraction_of_contact_info", "type": "Coroutine", "lineno": 57}, {"nodeid": "tests/test_only_json.py::test_nested_structures", "type": "Coroutine", "lineno": 85}, {"nodeid": "tests/test_only_json.py::test_cyclic_relationships", "type": "Coroutine", "lineno": 113}, {"nodeid": "tests/test_only_json.py::test_conversion_while_extracting", "type": "Coroutine", "lineno": 138}, {"nodeid": "tests/test_only_json.py::test_different_field_descriptions", "type": "Coroutine", "lineno": 159}, {"nodeid": "tests/test_only_json.py::test_data_formats", "type": "Coroutine", "lineno": 179}, {"nodeid": "tests/test_only_json.py::test_datetime_parsing", "type": "Coroutine", "lineno": 200}, {"nodeid": "tests/test_only_json.py::test_optional_fields", "type": "Coroutine", "lineno": 221}, {"nodeid": "tests/test_only_json.py::test_special_characters_and_lists", "type": "Coroutine", "lineno": 245}]}, {"nodeid": "tests/test_only_judge_llms.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_judge_llms.py::test_summary_evaluation", "type": "Coroutine", "lineno": 30}, {"nodeid": "tests/test_only_judge_llms.py::test_conversational_evaluation", "type": "Coroutine", "lineno": 52}, {"nodeid": "tests/test_only_judge_llms.py::test_creative_writing_evaluation", "type": "Coroutine", "lineno": 77}, {"nodeid": "tests/test_only_judge_llms.py::test_instructional_evaluation", "type": "Coroutine", "lineno": 101}, {"nodeid": "tests/test_only_judge_llms.py::test_poetic_evaluation", "type": "Coroutine", "lineno": 125}]}, {"nodeid": "tests/test_only_markdown.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_markdown.py::test_hastily_copied_html_conversion", "type": "Coroutine", "lineno": 36}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_word_doc_conversion", "type": "Coroutine", "lineno": 54}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_db_output_conversion", "type": "Coroutine", "lineno": 72}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_blog_post_conversion", "type": "Coroutine", "lineno": 81}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_complex_unformatted_conversion", "type": "Coroutine", "lineno": 96}]}, {"nodeid": "tests/test_only_python.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_python.py::test_generate_code_simple_function_generation", "type": "Coroutine", "lineno": 37}, {"nodeid": "tests/test_only_python.py::test_api_call", "type": "Coroutine", "lineno": 53}, {"nodeid": "tests/test_only_python.py::test_internet_connection", "type": "Coroutine", "lineno": 82}, {"nodeid": "tests/test_only_python.py::test_data_structure_code", "type": "Coroutine", "lineno": 100}, {"nodeid": "tests/test_only_python.py::test_data_visualization", "type": "Coroutine", "lineno": 122}, {"nodeid": "tests/test_only_python.py::test_only_email_code_generation", "type": "Coroutine", "lineno": 142}]}, {"nodeid": "tests/test_only_rephrase.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_rephrase.py::test_rephase_text_formal", "type": "Coroutine", "lineno": 36}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_informal", "type": "Coroutine", "lineno": 47}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_urgent", "type": "Coroutine", "lineno": 58}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_enthusiastic", "type": "Coroutine", "lineno": 69}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_informative", "type": "Coroutine", "lineno": 80}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_apologetic", "type": "Coroutine", "lineno": 91}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_friendly", "type": "Function", "lineno": 102}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_technical", "type": "Function", "lineno": 112}]}, {"nodeid": "tests/test_only_shell.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_shell.py::test_simple_shell_command", "type": "Coroutine", "lineno": 36}]}, {"nodeid": "tests/test_only_summary.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_summary.py::test_simple_text_summary", "type": "Coroutine", "lineno": 36}, {"nodeid": "tests/test_only_summary.py::test_news_article_summary", "type": "Coroutine", "lineno": 49}, {"nodeid": "tests/test_only_summary.py::test_research_paper_summary", "type": "Coroutine", "lineno": 62}, {"nodeid": "tests/test_only_summary.py::test_technical_manual_summary", "type": "Coroutine", "lineno": 75}, {"nodeid": "tests/test_only_summary.py::test_business_report_summary", "type": "Coroutine", "lineno": 87}, {"nodeid": "tests/test_only_summary.py::test_legal_document_summary", "type": "Coroutine", "lineno": 100}, {"nodeid": "tests/test_only_summary.py::test_blog_post_summary", "type": "Coroutine", "lineno": 113}, {"nodeid": "tests/test_only_summary.py::test_fiction_excerpt_summary", "type": "Coroutine", "lineno": 125}]}, {"nodeid": "tests/test_only_video.py", "outcome": "passed", "result": [{"nodeid": "tests/test_only_video.py::test_simple_blog_generation", "type": "Coroutine", "lineno": 36}]}, {"nodeid": "tests", "outcome": "passed", "result": [{"nodeid": "tests/test_elevate.py", "type": "Module"}, {"nodeid": "tests/test_only_audiocast.py", "type": "Module"}, {"nodeid": "tests/test_only_email.py", "type": "Module"}, {"nodeid": "tests/test_only_json.py", "type": "Module"}, {"nodeid": "tests/test_only_judge_llms.py", "type": "Module"}, {"nodeid": "tests/test_only_markdown.py", "type": "Module"}, {"nodeid": "tests/test_only_python.py", "type": "Module"}, {"nodeid": "tests/test_only_rephrase.py", "type": "Module"}, {"nodeid": "tests/test_only_shell.py", "type": "Module"}, {"nodeid": "tests/test_only_summary.py", "type": "Module"}, {"nodeid": "tests/test_only_video.py", "type": "Module"}]}, {"nodeid": ".", "outcome": "passed", "result": [{"nodeid": "agent", "type": "Package"}, {"nodeid": "src", "type": "Dir"}, {"nodeid": "suite", "type": "Package"}, {"nodeid": "tests", "type": "Package"}]}], "tests": [{"nodeid": "tests/test_elevate.py::test_elevate", "lineno": 27, "outcome": "passed", "keywords": ["test_elevate", "asyncio", "pytestmark", "test_elevate.py", "tests", "elevate", ""], "setup": {"duration": 0.00041579201933927834, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488786.330404, "msecs": 330.0, "relativeCreated": 929.6081066131592, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.00013679199037142098, "outcome": "passed"}, "teardown": {"duration": 0.00012000001152046025, "outcome": "passed"}}, {"nodeid": "tests/test_only_audiocast.py::test_create_default_audiocast", "lineno": 35, "outcome": "failed", "keywords": ["test_create_default_audiocast", "asyncio", "pytestmark", "test_only_audiocast.py", "tests", "elevate", ""], "setup": {"duration": 0.00021966701024211943, "outcome": "passed", "stdout": "INFO: Using model: groq/llama-3.3-70b-versatile\nDEBUG: Using selector: KqueueSelector\n", "log": [{"name": "conftest", "msg": "Using model: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/conftest.py", "filename": "conftest.py", "module": "conftest", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 39, "funcName": "settings", "created": 1749488786.3311539, "msecs": 331.0, "relativeCreated": 930.3579330444336, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}, {"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488786.331229, "msecs": 331.0, "relativeCreated": 930.4330348968506, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.00011712501873262227, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"}, "traceback": [{"path": "tests/test_only_audiocast.py", "lineno": 39, "message": ""}, {"path": "src/elevate/only_audiocast.py", "lineno": 146, "message": "in __init__"}, {"path": ".venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "OpenAIError"}], "longrepr": "settings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_create_default_audiocast(settings: Any) -> None:\n        content = \"San Francisco is a city in California. It is known for the Golden Gate Bridge.\"\n>       audiocast = OnlyAudiocast(with_model=settings.with_model)\n\ntests/test_only_audiocast.py:39: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_audiocast.py:146: in __init__\n    self.client = OpenAI()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.OpenAI object at 0x10e2dd190>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n.venv/lib/python3.12/site-packages/openai/_client.py:116: OpenAIError"}, "teardown": {"duration": 0.00020341601339168847, "outcome": "passed"}}, {"nodeid": "tests/test_only_audiocast.py::test_create_two_people_podcast", "lineno": 45, "outcome": "failed", "keywords": ["test_create_two_people_podcast", "asyncio", "pytestmark", "test_only_audiocast.py", "tests", "elevate", ""], "setup": {"duration": 0.00020829099230468273, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488786.376464, "msecs": 376.0, "relativeCreated": 975.6679534912109, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.0001384170027449727, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"}, "traceback": [{"path": "tests/test_only_audiocast.py", "lineno": 83, "message": ""}, {"path": "src/elevate/only_audiocast.py", "lineno": 146, "message": "in __init__"}, {"path": ".venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "OpenAIError"}], "longrepr": "settings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_create_two_people_podcast(settings: Any) -> None:\n        content = \"\"\"The standard cosmological model rests on a foundational assumption: that the universe is homogeneous and isotropic on large scales. However, this assumption has long been questioned due to observable evidence of substantial cosmic inhomogeneities\u2014galaxy clusters, vast voids, and filamentary structures spanning billions of light-years. Such uneven distributions of matter challenge the premise that the universe expands uniformly. Post-doctorate astrophysics researchers are increasingly examining the implications of these structures, realizing that overlooking them might significantly distort our understanding of cosmic expansion.\n    \n    One emerging theory gaining traction is timescape cosmology, which offers a compelling alternative to the elusive concept of dark energy. Timescape theory posits that different regions of the universe experience their own distinct \"timescapes\"\u2014unique expansion histories shaped by local gravitational environments. In this model, what appears to be an accelerating universe (currently explained by invoking dark energy) is instead interpreted as an observational artifact arising from comparing clocks and rulers in regions with varying gravitational conditions. Essentially, the accelerated expansion emerges naturally from general relativity when accounting properly for gravitational inhomogeneities, eliminating the need for mysterious forms of energy.\n    \n    This theory is resonating within the astrophysics community largely because of its elegant explanatory power and reduction in speculative components. Timescape cosmology aligns closely with recent astronomical observations highlighting significant cosmic structures, offering an interpretation firmly grounded in established physics rather than unknown entities. As post-doc students delve deeper into the precision cosmology era, leveraging data from advanced telescopes and gravitational-wave observatories, exploring and refining timescape models becomes a promising frontier\u2014one that may reshape our fundamental understanding of the universe and its evolution.\"\"\"\n    \n        cast_configuration = CastConfiguration(\n            speakers=[\n                SpeakerConfig(\n                    name=\"Professor Mitchell\",\n                    background=\"Expert in astrophysics focusing on gravitational inhomogeneities\",\n                    expertise=\"high\",\n                    speaking_style=\"interview\",\n                    level_of_expertise=\"post-doc\",\n                    focus_aspect=\"timescape cosmology\",\n                    depth=\"high\",\n                ),\n                SpeakerConfig(\n                    name=\"Professor Carter\",\n                    background=\"Renowned cosmologist with extensive research in timescape theory\",\n                    expertise=\"high\",\n                    speaking_style=\"conversational\",\n                    level_of_expertise=\"post-doc\",\n                    focus_aspect=\"observational cosmology\",\n                    depth=\"high\",\n                ),\n            ],\n            listener=ListenerConfig(\n                expertise=\"advanced\",\n                summary_of_similar_content=[\"Post-doctoral astrophysics discussions\"],\n                level_of_expertise=\"high\",\n                depth=\"high\",\n            ),\n        )\n    \n>       audiocast = OnlyAudiocast(with_model=settings.with_model)\n\ntests/test_only_audiocast.py:83: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_audiocast.py:146: in __init__\n    self.client = OpenAI()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.OpenAI object at 0x10e2df980>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n.venv/lib/python3.12/site-packages/openai/_client.py:116: OpenAIError"}, "teardown": {"duration": 0.0001624169817660004, "outcome": "passed"}}, {"nodeid": "tests/test_only_audiocast.py::test_create_art_teacher_conversation", "lineno": 90, "outcome": "failed", "keywords": ["test_create_art_teacher_conversation", "asyncio", "pytestmark", "test_only_audiocast.py", "tests", "elevate", ""], "setup": {"duration": 0.00017549999756738544, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488786.381613, "msecs": 381.0, "relativeCreated": 980.8170795440674, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.00012004200834780931, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"}, "traceback": [{"path": "tests/test_only_audiocast.py", "lineno": 126, "message": ""}, {"path": "src/elevate/only_audiocast.py", "lineno": 146, "message": "in __init__"}, {"path": ".venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "OpenAIError"}], "longrepr": "settings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_create_art_teacher_conversation(settings: Any) -> None:\n        content = \"\"\"Impressionism was a revolutionary art movement originating in France in the late 19th century. Impressionist painters, such as Monet, Renoir, and Degas, focused on capturing the immediate impression of a scene, particularly the effects of light, color, and atmosphere. Instead of precise detail, their paintings were characterized by quick brushstrokes and vibrant colors, conveying mood and emotion rather than realistic accuracy.\n    \n    An important hallmark of impressionism was painting en plein air, meaning outdoors. Artists abandoned studios to paint directly from nature, allowing them to observe and capture the changing qualities of light firsthand. This fresh perspective challenged traditional academic standards and faced initial resistance, eventually transforming public appreciation and paving the way for modern art.\"\"\"\n    \n        cast_configuration = CastConfiguration(\n            speakers=[\n                SpeakerConfig(\n                    name=\"Ms. Lucy\",\n                    background=\"High school art teacher passionate about art history\",\n                    expertise=\"high\",\n                    speaking_style=\"friendly, inspiring\",\n                    level_of_expertise=\"art educator\",\n                    focus_aspect=\"Impressionism`s visual techniques\",\n                    depth=\"medium\",\n                ),\n                SpeakerConfig(\n                    name=\"Alex\",\n                    background=\"Curious student interested in painting\",\n                    expertise=\"novice\",\n                    speaking_style=\"curious, enthusiastic\",\n                    level_of_expertise=\"high school student\",\n                    focus_aspect=\"Student perspectives and questions\",\n                    depth=\"medium\",\n                ),\n            ],\n            listener=ListenerConfig(\n                expertise=\"beginner\",\n                summary_of_similar_content=[\"basic art movements overview\"],\n                level_of_expertise=\"high school student\",\n                depth=\"low\",\n            ),\n        )\n    \n>       audiocast = OnlyAudiocast(with_model=settings.with_model)\n\ntests/test_only_audiocast.py:126: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_audiocast.py:146: in __init__\n    self.client = OpenAI()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.OpenAI object at 0x10e2df440>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n.venv/lib/python3.12/site-packages/openai/_client.py:116: OpenAIError"}, "teardown": {"duration": 0.0002969579945784062, "outcome": "passed"}}, {"nodeid": "tests/test_only_audiocast.py::test_create_children_storytelling_session", "lineno": 133, "outcome": "failed", "keywords": ["test_create_children_storytelling_session", "asyncio", "pytestmark", "test_only_audiocast.py", "tests", "elevate", ""], "setup": {"duration": 0.00031633401522412896, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488786.388087, "msecs": 388.0, "relativeCreated": 987.2910976409912, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.0001568750012665987, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"}, "traceback": [{"path": "tests/test_only_audiocast.py", "lineno": 160, "message": ""}, {"path": "src/elevate/only_audiocast.py", "lineno": 146, "message": "in __init__"}, {"path": ".venv/lib/python3.12/site-packages/openai/_client.py", "lineno": 116, "message": "OpenAIError"}], "longrepr": "settings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_create_children_storytelling_session(settings: Any) -> None:\n        content = \"\"\"In the magical kingdom of Eldoria lived a brave little fox named Finley. Unlike other foxes, Finley dreamed of flying. Each evening, Finley climbed the tallest oak and gazed at the stars, whispering a wish to soar among them. One night, a wise old owl named Orion overheard Finley's wish and decided to help.\n    \n    With the owl`s guidance, Finley set off on a journey through enchanted forests and shimmering lakes to find the legendary Wings of Eldoria. Along the way, Finley faced challenges and made many animal friends who offered their help. Together, they discovered courage, friendship, and that true magic comes from believing in yourself.\"\"\"\n    \n        cast_configuration = CastConfiguration(\n            speakers=[\n                SpeakerConfig(\n                    name=\"Narrator\",\n                    background=\"Experienced children's storyteller\",\n                    expertise=\"high\",\n                    speaking_style=\"animated, expressive, multiple character voices\",\n                    level_of_expertise=\"professional storyteller\",\n                    focus_aspect=\"Engaging young audience through expressive narration\",\n                    depth=\"light\",\n                )\n            ],\n            listener=ListenerConfig(\n                expertise=\"beginner\",\n                summary_of_similar_content=[\"Classic fairytales\", \"Animal stories\"],\n                level_of_expertise=\"children\",\n                depth=\"light\",\n            ),\n        )\n    \n>       audiocast = OnlyAudiocast(with_model=settings.with_model)\n\ntests/test_only_audiocast.py:160: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_audiocast.py:146: in __init__\n    self.client = OpenAI()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <openai.OpenAI object at 0x10e2decf0>\n\n    def __init__(\n        self,\n        *,\n        api_key: str | None = None,\n        organization: str | None = None,\n        project: str | None = None,\n        base_url: str | httpx.URL | None = None,\n        websocket_base_url: str | httpx.URL | None = None,\n        timeout: Union[float, Timeout, None, NotGiven] = NOT_GIVEN,\n        max_retries: int = DEFAULT_MAX_RETRIES,\n        default_headers: Mapping[str, str] | None = None,\n        default_query: Mapping[str, object] | None = None,\n        # Configure a custom httpx client.\n        # We provide a `DefaultHttpxClient` class that you can pass to retain the default values we use for `limits`, `timeout` & `follow_redirects`.\n        # See the [httpx documentation](https://www.python-httpx.org/api/#client) for more details.\n        http_client: httpx.Client | None = None,\n        # Enable or disable schema validation for data returned by the API.\n        # When enabled an error APIResponseValidationError is raised\n        # if the API responds with invalid data for the expected schema.\n        #\n        # This parameter may be removed or changed in the future.\n        # If you rely on this feature, please open a GitHub issue\n        # outlining your use-case to help us decide if it should be\n        # part of our public interface in the future.\n        _strict_response_validation: bool = False,\n    ) -> None:\n        \"\"\"Construct a new synchronous OpenAI client instance.\n    \n        This automatically infers the following arguments from their corresponding environment variables if they are not provided:\n        - `api_key` from `OPENAI_API_KEY`\n        - `organization` from `OPENAI_ORG_ID`\n        - `project` from `OPENAI_PROJECT_ID`\n        \"\"\"\n        if api_key is None:\n            api_key = os.environ.get(\"OPENAI_API_KEY\")\n        if api_key is None:\n>           raise OpenAIError(\n                \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n            )\nE           openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\n\n.venv/lib/python3.12/site-packages/openai/_client.py:116: OpenAIError"}, "teardown": {"duration": 0.00024033299996517599, "outcome": "passed"}}, {"nodeid": "tests/test_only_email.py::test_personal_email", "lineno": 36, "outcome": "passed", "keywords": ["test_personal_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.00019774999236688018, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488786.393615, "msecs": 393.0, "relativeCreated": 992.81907081604, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.7733310830080882, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e2deea0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x109a6cb60>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'999'), (b'x-ratelimit-remaining-tokens', b'11862'), (b'x-ratelimit-reset-requests', b'1m26.4s'), (b'x-ratelimit-reset-tokens', b'690ms'), (b'x-request-id', b'req_01jxatgq35ehpv49gc418n65jy'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=P5eXo1Jb8NYiZwA_NKEp6meY.zwX_ko6kPCaJ5m5hY4-1749488787-1.0.1.1-Nax4zS.nKTOB8h3YXY4EtWspPbk1OGJQmWIlh32iMsQoCRmqkKX.zSCmQxsN.uH7Y6jbM6bbbx5EoN.pvBixNxoiWgAbDAHgZ3It7r392n4; path=/; expires=Mon, 09-Jun-25 17:36:27 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23833aff0406f-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-d03c3966-0add-4abf-a956-00908f790d81\", \"object\": \"chat.completion\", \"created\": 1749488786, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Happy Birthday, John!\\n\\nDear John,\\n\\nI wanted to take a moment to wish you a happy birthday and express my warmest wishes on your special day. May it be filled with love, laughter, and all your favorite things. I hope you get to celebrate with the people who matter most to you and create some unforgettable memories.\\n\\nAs you mark another year of life, I hope you feel grateful for the incredible experiences you've had, the people you've met, and the lessons you've learned. You deserve to be celebrated, and I'm honored to be a part of your life.\\n\\nHere's to another amazing year ahead! May all your dreams and aspirations come true, and may you continue to shine your light for all to see.\\n\\nWith love and best wishes,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.055714076, \"prompt_tokens\": 138, \"prompt_time\": 0.009163594, \"completion_tokens\": 161, \"completion_time\": 0.585454545, \"total_tokens\": 299, \"total_time\": 0.594618139}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgq35ehpv49gc418n65jy\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.141999999999999e-05, completion_tokens_cost_usd_dollar: 0.00012718999999999998\nDEBUG: response_cost: 0.00020860999999999997\nDEBUG: Subject: Happy Birthday, John!\n\nDear John,\n\nI wanted to take a moment to wish you a happy birthday and express my warmest wishes on your special day. May it be filled with love, laughter, and all your favorite things. I hope you get to celebrate with the people who matter most to you and create some unforgettable memories.\n\nAs you mark another year of life, I hope you feel grateful for the incredible experiences you've had, the people you've met, and the lessons you've learned. You deserve to be celebrated, and I'm honored to be a part of your life.\n\nHere's to another amazing year ahead! May all your dreams and aspirations come true, and may you continue to shine your light for all to see.\n\nWith love and best wishes,\n[Your Name]\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d5970>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:26 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], 'thinking': None}\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:26 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-d03c3966-0add-4abf-a956-00908f790d81\", \"object\": \"chat.completion\", \"created\": 1749488786, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Happy Birthday, John!\\n\\nDear John,\\n\\nI wanted to take a moment to wish you a happy birthday and express my warmest wishes on your special day. May it be filled with love, laughter, and all your favorite things. I hope you get to celebrate with the people who matter most to you and create some unforgettable memories.\\n\\nAs you mark another year of life, I hope you feel grateful for the incredible experiences you've had, the people you've met, and the lessons you've learned. You deserve to be celebrated, and I'm honored to be a part of your life.\\n\\nHere's to another amazing year ahead! May all your dreams and aspirations come true, and may you continue to shine your light for all to see.\\n\\nWith love and best wishes,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.055714076, \"prompt_tokens\": 138, \"prompt_time\": 0.009163594, \"completion_tokens\": 161, \"completion_time\": 0.585454545, \"total_tokens\": 299, \"total_time\": 0.594618139}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgq35ehpv49gc418n65jy\"}}\n\n\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.141999999999999e-05, completion_tokens_cost_usd_dollar: 0.00012718999999999998\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00020860999999999997\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d5970>>\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488786.3938282, "msecs": 393.0, "relativeCreated": 993.0322170257568, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488786.393881, "msecs": 393.0, "relativeCreated": 993.0851459503174, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488786.393903, "msecs": 393.0, "relativeCreated": 993.1070804595947, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488786.3939211, "msecs": 393.0, "relativeCreated": 993.1252002716064, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488786.393992, "msecs": 393.0, "relativeCreated": 993.1960105895996, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488786.394014, "msecs": 394.0, "relativeCreated": 993.217945098877, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488786.394033, "msecs": 394.0, "relativeCreated": 993.2370185852051, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488786.395482, "msecs": 395.0, "relativeCreated": 994.6861267089844, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488786.395547, "msecs": 395.0, "relativeCreated": 994.7509765625, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488786.3955688, "msecs": 395.0, "relativeCreated": 994.7729110717773, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488786.395597, "msecs": 395.0, "relativeCreated": 994.8010444641113, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488786.395624, "msecs": 395.0, "relativeCreated": 994.8279857635498, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:26"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A birthday wishes for John Doe.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488786.395678, "msecs": 395.0, "relativeCreated": 994.8821067810059, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:26"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.400098, "msecs": 400.0, "relativeCreated": 999.3021488189697, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e2deea0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.412524, "msecs": 412.0, "relativeCreated": 1011.728048324585, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.4125729, "msecs": 412.0, "relativeCreated": 1011.7769241333008, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x109a6cb60>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.4235048, "msecs": 423.0, "relativeCreated": 1022.7088928222656, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.423619, "msecs": 423.0, "relativeCreated": 1022.8230953216553, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.42374, "msecs": 423.0, "relativeCreated": 1022.9439735412598, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.423761, "msecs": 423.0, "relativeCreated": 1022.9649543762207, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.4238331, "msecs": 423.0, "relativeCreated": 1023.0371952056885, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488786.4238532, "msecs": 423.0, "relativeCreated": 1023.057222366333, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:27 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'999'), (b'x-ratelimit-remaining-tokens', b'11862'), (b'x-ratelimit-reset-requests', b'1m26.4s'), (b'x-ratelimit-reset-tokens', b'690ms'), (b'x-request-id', b'req_01jxatgq35ehpv49gc418n65jy'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=P5eXo1Jb8NYiZwA_NKEp6meY.zwX_ko6kPCaJ5m5hY4-1749488787-1.0.1.1-Nax4zS.nKTOB8h3YXY4EtWspPbk1OGJQmWIlh32iMsQoCRmqkKX.zSCmQxsN.uH7Y6jbM6bbbx5EoN.pvBixNxoiWgAbDAHgZ3It7r392n4; path=/; expires=Mon, 09-Jun-25 17:36:27 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23833aff0406f-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.162478, "msecs": 162.0, "relativeCreated": 1761.6820335388184, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488787.163402, "msecs": 163.0, "relativeCreated": 1762.606143951416, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.1638029, "msecs": 163.0, "relativeCreated": 1763.0069255828857, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.1642509, "msecs": 164.0, "relativeCreated": 1763.4549140930176, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.1643798, "msecs": 164.0, "relativeCreated": 1763.5838985443115, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.1645188, "msecs": 164.0, "relativeCreated": 1763.7228965759277, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488787.164785, "msecs": 164.0, "relativeCreated": 1763.988971710205, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-d03c3966-0add-4abf-a956-00908f790d81\", \"object\": \"chat.completion\", \"created\": 1749488786, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Happy Birthday, John!\\n\\nDear John,\\n\\nI wanted to take a moment to wish you a happy birthday and express my warmest wishes on your special day. May it be filled with love, laughter, and all your favorite things. I hope you get to celebrate with the people who matter most to you and create some unforgettable memories.\\n\\nAs you mark another year of life, I hope you feel grateful for the incredible experiences you've had, the people you've met, and the lessons you've learned. You deserve to be celebrated, and I'm honored to be a part of your life.\\n\\nHere's to another amazing year ahead! May all your dreams and aspirations come true, and may you continue to shine your light for all to see.\\n\\nWith love and best wishes,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.055714076, \"prompt_tokens\": 138, \"prompt_time\": 0.009163594, \"completion_tokens\": 161, \"completion_time\": 0.585454545, \"total_tokens\": 299, \"total_time\": 0.594618139}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgq35ehpv49gc418n65jy\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.165028, "msecs": 165.0, "relativeCreated": 1764.2321586608887, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488787.165474, "msecs": 165.0, "relativeCreated": 1764.6780014038086, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488787.165833, "msecs": 165.0, "relativeCreated": 1765.0370597839355, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488787.166015, "msecs": 166.0, "relativeCreated": 1765.21897315979, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.141999999999999e-05, completion_tokens_cost_usd_dollar: 0.00012718999999999998", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488787.1660988, "msecs": 166.0, "relativeCreated": 1765.3028964996338, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "response_cost: 0.00020860999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488787.166184, "msecs": 166.0, "relativeCreated": 1765.388011932373, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11", "asctime": "22:36:27"}, {"name": "root", "msg": "Subject: Happy Birthday, John!\n\nDear John,\n\nI wanted to take a moment to wish you a happy birthday and express my warmest wishes on your special day. May it be filled with love, laughter, and all your favorite things. I hope you get to celebrate with the people who matter most to you and create some unforgettable memories.\n\nAs you mark another year of life, I hope you feel grateful for the incredible experiences you've had, the people you've met, and the lessons you've learned. You deserve to be celebrated, and I'm honored to be a part of your life.\n\nHere's to another amazing year ahead! May all your dreams and aspirations come true, and may you continue to shine your light for all to see.\n\nWith love and best wishes,\n[Your Name]", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 45, "funcName": "test_personal_email", "created": 1749488787.166276, "msecs": 166.0, "relativeCreated": 1765.4800415039062, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-11"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d5970>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.166424, "msecs": 166.0, "relativeCreated": 1765.6280994415283, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-13", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488787.166555, "msecs": 166.0, "relativeCreated": 1765.758991241455, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-13", "asctime": "22:36:27"}]}, "teardown": {"duration": 0.0021748750004917383, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.141999999999999e-05, completion_tokens_cost_usd_dollar: 0.00012718999999999998\nDEBUG: response_cost: 0.00020860999999999997\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00020860999999999997\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:27 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.141999999999999e-05, completion_tokens_cost_usd_dollar: 0.00012718999999999998\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00020860999999999997\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00020860999999999997\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.167594, "msecs": 167.0, "relativeCreated": 1766.7980194091797, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488787.1680892, "msecs": 168.0, "relativeCreated": 1767.2932147979736, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488787.168194, "msecs": 168.0, "relativeCreated": 1767.3981189727783, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.168406, "msecs": 168.0, "relativeCreated": 1767.6100730895996, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488787.168481, "msecs": 168.0, "relativeCreated": 1767.6851749420166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488787.1685572, "msecs": 168.0, "relativeCreated": 1767.76123046875, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.141999999999999e-05, completion_tokens_cost_usd_dollar: 0.00012718999999999998", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488787.1686149, "msecs": 168.0, "relativeCreated": 1767.8189277648926, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "response_cost: 0.00020860999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488787.168682, "msecs": 168.0, "relativeCreated": 1767.8861618041992, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00020860999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488787.168725, "msecs": 168.0, "relativeCreated": 1767.9290771484375, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488787.168847, "msecs": 168.0, "relativeCreated": 1768.0511474609375, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488787.168908, "msecs": 168.0, "relativeCreated": 1768.1121826171875, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-14", "asctime": "22:36:27"}]}}, {"nodeid": "tests/test_only_email.py::test_professional_email", "lineno": 47, "outcome": "passed", "keywords": ["test_professional_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.000701832992490381, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488787.1724439, "msecs": 172.0, "relativeCreated": 1771.6479301452637, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.020718832995044068, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\nDEBUG: An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed\nDEBUG: Error: An unexpected error occurred while generating the email.\n", "stderr": "\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:27 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], 'thinking': None}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.173095, "msecs": 173.0, "relativeCreated": 1772.29905128479, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.1732202, "msecs": 173.0, "relativeCreated": 1772.4242210388184, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.173274, "msecs": 173.0, "relativeCreated": 1772.4781036376953, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.173333, "msecs": 173.0, "relativeCreated": 1772.5369930267334, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488787.1735911, "msecs": 173.0, "relativeCreated": 1772.7952003479004, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.173706, "msecs": 173.0, "relativeCreated": 1772.9101181030273, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488787.173789, "msecs": 173.0, "relativeCreated": 1772.9930877685547, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488787.174994, "msecs": 174.0, "relativeCreated": 1774.198055267334, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488787.175104, "msecs": 175.0, "relativeCreated": 1774.3079662322998, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488787.175158, "msecs": 175.0, "relativeCreated": 1774.3620872497559, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.1752331, "msecs": 175.0, "relativeCreated": 1774.4371891021729, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488787.1753001, "msecs": 175.0, "relativeCreated": 1774.5041847229004, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Email to boss regarding, asking a sick leave for 3 days.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488787.1754591, "msecs": 175.0, "relativeCreated": 1774.6632099151611, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.176414, "msecs": 176.0, "relativeCreated": 1775.618076324463, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.176831, "msecs": 176.0, "relativeCreated": 1776.0350704193115, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.176883, "msecs": 176.0, "relativeCreated": 1776.0870456695557, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.17714, "msecs": 177.0, "relativeCreated": 1776.3440608978271, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.177239, "msecs": 177.0, "relativeCreated": 1776.4430046081543, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.177438, "msecs": 177.0, "relativeCreated": 1776.642084121704, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.177494, "msecs": 177.0, "relativeCreated": 1776.698112487793, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.177705, "msecs": 177.0, "relativeCreated": 1776.9091129302979, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488787.177815, "msecs": 177.0, "relativeCreated": 1777.0190238952637, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488787.178339, "msecs": 178.0, "relativeCreated": 1777.543067932129, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488787.1907961, "msecs": 190.0, "relativeCreated": 1790.0002002716064, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16", "asctime": "22:36:27"}, {"name": "root", "msg": "An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_email.py", "filename": "only_email.py", "module": "only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 137, "funcName": "generate_email", "created": 1749488787.1933062, "msecs": 193.0, "relativeCreated": 1792.5102710723877, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}, {"name": "root", "msg": "Error: An unexpected error occurred while generating the email.", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 56, "funcName": "test_professional_email", "created": 1749488787.1933608, "msecs": 193.0, "relativeCreated": 1792.564868927002, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-16"}]}, "teardown": {"duration": 0.0003905420016963035, "outcome": "passed"}}, {"nodeid": "tests/test_only_email.py::test_marketing_email", "lineno": 58, "outcome": "passed", "keywords": ["test_marketing_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.0003570829867385328, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488787.1945112, "msecs": 194.0, "relativeCreated": 1793.715238571167, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 1.7145218749938067, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bd0d0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e3d6240>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'998'), (b'x-ratelimit-remaining-tokens', b'10904'), (b'x-ratelimit-reset-requests', b'2m52.006999999s'), (b'x-ratelimit-reset-tokens', b'5.476s'), (b'x-request-id', b'req_01jxatgqvwfpjt5wvrcs8vb821'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238389e253acb-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-5af27fee-25b2-47be-9ba0-bc45c009493e\", \"object\": \"chat.completion\", \"created\": 1749488787, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Revolutionize Your Email Game with Emailer!\\n\\nHey University Students,\\n\\nAre you tired of spending hours crafting the perfect email? Do you wish there was a way to generate professional-looking emails in no time? Well, wish no more! We're excited to introduce you to Emailer, a game-changing Python library that's about to become your new best friend.\\n\\nImagine being able to create email drafts in the blink of an eye, without sacrificing quality or professionalism. With Emailer, you can do just that! Our library uses cutting-edge technology to generate emails based on your input, saving you time and effort. But that's not all - we're also giving you the freedom to experiment with different Large Language Models (LLMs) to find the one that works best for you. We call this amazing feature BYOL, or Bring Your Own LLM.\\n\\nBut what really sets Emailer apart is its commitment to openness and community. As a completely open-source library, you can trust that you're getting a tool that's transparent, customizable, and constantly improving. Our community is always working to make Emailer better, and we invite you to join in on the fun!\\n\\nSo, what are the benefits of using Emailer? Here are just a few:\\n\\n* **Lightning-fast email generation**: Get high-quality email drafts in seconds, not hours.\\n* **BYOL (Bring Your Own LLM)**: Try out different LLMs to find the one that suits your style and needs.\\n* **Completely open source**: Join a community that's passionate about making Emailer the best it can be.\\n\\nWe're thrilled to share Emailer with you, and we can't wait to see what you create with it! Whether you're a student looking to streamline your email workflow or a developer interested in exploring the latest advancements in LLMs, Emailer is the perfect tool for you.\\n\\nReady to learn more about Emailer and start generating amazing emails? Click here to know more and get started on your email revolution!\\n\\nBest,\\nThe Emailer Team\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.052285202, \"prompt_tokens\": 767, \"prompt_time\": 0.060102058, \"completion_tokens\": 413, \"completion_time\": 1.501818182, \"total_tokens\": 1180, \"total_time\": 1.56192024}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgqvwfpjt5wvrcs8vb821\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00045253, completion_tokens_cost_usd_dollar: 0.00032627\nDEBUG: response_cost: 0.0007788\nDEBUG: Subject: Revolutionize Your Email Game with Emailer!\n\nHey University Students,\n\nAre you tired of spending hours crafting the perfect email? Do you wish there was a way to generate professional-looking emails in no time? Well, wish no more! We're excited to introduce you to Emailer, a game-changing Python library that's about to become your new best friend.\n\nImagine being able to create email drafts in the blink of an eye, without sacrificing quality or professionalism. With Emailer, you can do just that! Our library uses cutting-edge technology to generate emails based on your input, saving you time and effort. But that's not all - we're also giving you the freedom to experiment with different Large Language Models (LLMs) to find the one that works best for you. We call this amazing feature BYOL, or Bring Your Own LLM.\n\nBut what really sets Emailer apart is its commitment to openness and community. As a completely open-source library, you can trust that you're getting a tool that's transparent, customizable, and constantly improving. Our community is always working to make Emailer better, and we invite you to join in on the fun!\n\nSo, what are the benefits of using Emailer? Here are just a few:\n\n* **Lightning-fast email generation**: Get high-quality email drafts in seconds, not hours.\n* **BYOL (Bring Your Own LLM)**: Try out different LLMs to find the one that suits your style and needs.\n* **Completely open source**: Join a community that's passionate about making Emailer the best it can be.\n\nWe're thrilled to share Emailer with you, and we can't wait to see what you create with it! Whether you're a student looking to streamline your email workflow or a developer interested in exploring the latest advancements in LLMs, Emailer is the perfect tool for you.\n\nReady to learn more about Emailer and start generating amazing emails? Click here to know more and get started on your email revolution!\n\nBest,\nThe Emailer Team\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d4e00>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:27 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], 'thinking': None}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:27 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-5af27fee-25b2-47be-9ba0-bc45c009493e\", \"object\": \"chat.completion\", \"created\": 1749488787, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Revolutionize Your Email Game with Emailer!\\n\\nHey University Students,\\n\\nAre you tired of spending hours crafting the perfect email? Do you wish there was a way to generate professional-looking emails in no time? Well, wish no more! We're excited to introduce you to Emailer, a game-changing Python library that's about to become your new best friend.\\n\\nImagine being able to create email drafts in the blink of an eye, without sacrificing quality or professionalism. With Emailer, you can do just that! Our library uses cutting-edge technology to generate emails based on your input, saving you time and effort. But that's not all - we're also giving you the freedom to experiment with different Large Language Models (LLMs) to find the one that works best for you. We call this amazing feature BYOL, or Bring Your Own LLM.\\n\\nBut what really sets Emailer apart is its commitment to openness and community. As a completely open-source library, you can trust that you're getting a tool that's transparent, customizable, and constantly improving. Our community is always working to make Emailer better, and we invite you to join in on the fun!\\n\\nSo, what are the benefits of using Emailer? Here are just a few:\\n\\n* **Lightning-fast email generation**: Get high-quality email drafts in seconds, not hours.\\n* **BYOL (Bring Your Own LLM)**: Try out different LLMs to find the one that suits your style and needs.\\n* **Completely open source**: Join a community that's passionate about making Emailer the best it can be.\\n\\nWe're thrilled to share Emailer with you, and we can't wait to see what you create with it! Whether you're a student looking to streamline your email workflow or a developer interested in exploring the latest advancements in LLMs, Emailer is the perfect tool for you.\\n\\nReady to learn more about Emailer and start generating amazing emails? Click here to know more and get started on your email revolution!\\n\\nBest,\\nThe Emailer Team\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.052285202, \"prompt_tokens\": 767, \"prompt_time\": 0.060102058, \"completion_tokens\": 413, \"completion_time\": 1.501818182, \"total_tokens\": 1180, \"total_time\": 1.56192024}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgqvwfpjt5wvrcs8vb821\"}}\n\n\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00045253, completion_tokens_cost_usd_dollar: 0.00032627\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.0007788\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d4e00>>\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.194902, "msecs": 194.0, "relativeCreated": 1794.1060066223145, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.194975, "msecs": 194.0, "relativeCreated": 1794.1789627075195, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.19503, "msecs": 195.0, "relativeCreated": 1794.234037399292, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.19507, "msecs": 195.0, "relativeCreated": 1794.274091720581, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488787.19518, "msecs": 195.0, "relativeCreated": 1794.3840026855469, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.195222, "msecs": 195.0, "relativeCreated": 1794.4259643554688, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488787.1952598, "msecs": 195.0, "relativeCreated": 1794.463872909546, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488787.1957161, "msecs": 195.0, "relativeCreated": 1794.9202060699463, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488787.195839, "msecs": 195.0, "relativeCreated": 1795.0429916381836, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488787.1959, "msecs": 195.0, "relativeCreated": 1795.1040267944336, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488787.1959538, "msecs": 195.0, "relativeCreated": 1795.1579093933105, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488787.196039, "msecs": 196.0, "relativeCreated": 1795.2430248260498, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting persuasive and effective marketing emails designed to promote products, services, and brands, and drive conversions. Your goal is to write an engaging email that captures the recipient\\'s attention, highlights the value proposition, and encourages a specific action (e.g., clicking a link, making a purchase). You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**INPUT:**\\n\\n1.  **Target Audience:** (Describe the intended recipient segment. Be specific. E.g., \"Existing customers who purchased the \\'Pro\\' plan\")\\n\\n2.  **Product/Service:** (Clearly describe the product, service, or brand being promoted. Include key features and benefits. E.g., \"Our new line of eco-friendly running shoes,\")\\n\\n3.  **Primary Goal:** (What is the desired outcome of this email? Be specific. E.g.\"Encourage users to upgrade to the \\'Pro\\' plan,\")\\n\\n4.  **Key Selling Points:** (List 3-5 compelling reasons why the target audience should take the desired action. Focus on the benefits and value they\\'ll receive. E.g.\"Easy-to-use features.\")\\n\\n5.  **Call to Action:** (Specify the desired action and how to perform it. Be clear and concise. E.g. \"Visit our website and explore our new collection.\")\\n\\n6. **Content length:** (Specify the desired word count E.g. 200 words)\\n\\n7.  **(Optional) Desired Tone:** (What overall tone do you want the email to convey? E.g., \"Enthusiastic and energetic\") If this is omitted, aim for a persuasive and benefit-driven tone.\\n\\n**Guidelines for writing an email:**\\n1. Align your subject line and email content\\n2. Create relevancy\\n3. Personalize the email\\n4. Explain benefits\\n5. Be personable\\n\\n**PROCESS:**\\n\\n1.  **Understand:** Carefully review the \"Target Audience,\" \"Product/Service,\" \"Primary Goal,\" \"Key Selling Points,\" \"Call to Action,\" \"Content length\" and \"Desired Tone\" (if provided).\\n2.  **Persuade & Engage:** Craft an email that effectively highlights the value proposition, addresses the target audience\\'s needs and desires, and encourages them to take the specified action.\\n3.  **Structure:** Follow the guidelines to structure email.\\n4.  **Output:** Output ONLY the complete email, including subject line, salutation, body, and closing. Do not include any extra comments or other text.\\n\\n*OUTPUT:*\\nProvide the complete marketing email, ready to be sent.\\n'}, {'role': 'user', 'content': '\\n    Email for a python library which generates professional emails based on user input.\\n\\n    Important input:\\n    1. Target Audience: University Students.\\n    2. Product/Service: Emailer, a Python library which can generate emails of different kinds based on user input. Developers can try out different LLMs while generating emails to see which suits best. This functionality of the library should be called BYOL (Bring Your Own LLM).\\n    3. Primary Goal: Let developers know about this library.\\n    4. Key Selling Points:\\n                        a. Generate email drafts in the blink of an eye.\\n                        b. Bring Your Own LLM.\\n                        c. Completely open source.\\n    5. Call to Action: Click here to know more.\\n    6. Content Length: 400 words.\\n    7. Desired Tone: A light and energetic tone.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488787.1961632, "msecs": 196.0, "relativeCreated": 1795.3672409057617, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:27"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.196512, "msecs": 196.0, "relativeCreated": 1795.7160472869873, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bd0d0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.205207, "msecs": 205.0, "relativeCreated": 1804.4111728668213, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.2052732, "msecs": 205.0, "relativeCreated": 1804.4772148132324, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e3d6240>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.216013, "msecs": 216.0, "relativeCreated": 1815.2170181274414, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.216157, "msecs": 216.0, "relativeCreated": 1815.3610229492188, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.216322, "msecs": 216.0, "relativeCreated": 1815.526008605957, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.216358, "msecs": 216.0, "relativeCreated": 1815.5620098114014, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.216479, "msecs": 216.0, "relativeCreated": 1815.683126449585, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488787.216518, "msecs": 216.0, "relativeCreated": 1815.7219886779785, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:28 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'998'), (b'x-ratelimit-remaining-tokens', b'10904'), (b'x-ratelimit-reset-requests', b'2m52.006999999s'), (b'x-ratelimit-reset-tokens', b'5.476s'), (b'x-request-id', b'req_01jxatgqvwfpjt5wvrcs8vb821'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238389e253acb-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.9047391, "msecs": 904.0, "relativeCreated": 3503.9432048797607, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488788.9054878, "msecs": 905.0, "relativeCreated": 3504.6918392181396, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.905729, "msecs": 905.0, "relativeCreated": 3504.9331188201904, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.906071, "msecs": 906.0, "relativeCreated": 3505.275011062622, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.906197, "msecs": 906.0, "relativeCreated": 3505.401134490967, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.906372, "msecs": 906.0, "relativeCreated": 3505.5761337280273, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488788.90685, "msecs": 906.0, "relativeCreated": 3506.054162979126, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-5af27fee-25b2-47be-9ba0-bc45c009493e\", \"object\": \"chat.completion\", \"created\": 1749488787, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Revolutionize Your Email Game with Emailer!\\n\\nHey University Students,\\n\\nAre you tired of spending hours crafting the perfect email? Do you wish there was a way to generate professional-looking emails in no time? Well, wish no more! We're excited to introduce you to Emailer, a game-changing Python library that's about to become your new best friend.\\n\\nImagine being able to create email drafts in the blink of an eye, without sacrificing quality or professionalism. With Emailer, you can do just that! Our library uses cutting-edge technology to generate emails based on your input, saving you time and effort. But that's not all - we're also giving you the freedom to experiment with different Large Language Models (LLMs) to find the one that works best for you. We call this amazing feature BYOL, or Bring Your Own LLM.\\n\\nBut what really sets Emailer apart is its commitment to openness and community. As a completely open-source library, you can trust that you're getting a tool that's transparent, customizable, and constantly improving. Our community is always working to make Emailer better, and we invite you to join in on the fun!\\n\\nSo, what are the benefits of using Emailer? Here are just a few:\\n\\n* **Lightning-fast email generation**: Get high-quality email drafts in seconds, not hours.\\n* **BYOL (Bring Your Own LLM)**: Try out different LLMs to find the one that suits your style and needs.\\n* **Completely open source**: Join a community that's passionate about making Emailer the best it can be.\\n\\nWe're thrilled to share Emailer with you, and we can't wait to see what you create with it! Whether you're a student looking to streamline your email workflow or a developer interested in exploring the latest advancements in LLMs, Emailer is the perfect tool for you.\\n\\nReady to learn more about Emailer and start generating amazing emails? Click here to know more and get started on your email revolution!\\n\\nBest,\\nThe Emailer Team\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.052285202, \"prompt_tokens\": 767, \"prompt_time\": 0.060102058, \"completion_tokens\": 413, \"completion_time\": 1.501818182, \"total_tokens\": 1180, \"total_time\": 1.56192024}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgqvwfpjt5wvrcs8vb821\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.907306, "msecs": 907.0, "relativeCreated": 3506.510019302368, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488788.907889, "msecs": 907.0, "relativeCreated": 3507.0929527282715, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488788.908185, "msecs": 908.0, "relativeCreated": 3507.3890686035156, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488788.908372, "msecs": 908.0, "relativeCreated": 3507.5759887695312, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00045253, completion_tokens_cost_usd_dollar: 0.00032627", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488788.9084601, "msecs": 908.0, "relativeCreated": 3507.664203643799, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "response_cost: 0.0007788", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488788.908534, "msecs": 908.0, "relativeCreated": 3507.7381134033203, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19", "asctime": "22:36:28"}, {"name": "root", "msg": "Subject: Revolutionize Your Email Game with Emailer!\n\nHey University Students,\n\nAre you tired of spending hours crafting the perfect email? Do you wish there was a way to generate professional-looking emails in no time? Well, wish no more! We're excited to introduce you to Emailer, a game-changing Python library that's about to become your new best friend.\n\nImagine being able to create email drafts in the blink of an eye, without sacrificing quality or professionalism. With Emailer, you can do just that! Our library uses cutting-edge technology to generate emails based on your input, saving you time and effort. But that's not all - we're also giving you the freedom to experiment with different Large Language Models (LLMs) to find the one that works best for you. We call this amazing feature BYOL, or Bring Your Own LLM.\n\nBut what really sets Emailer apart is its commitment to openness and community. As a completely open-source library, you can trust that you're getting a tool that's transparent, customizable, and constantly improving. Our community is always working to make Emailer better, and we invite you to join in on the fun!\n\nSo, what are the benefits of using Emailer? Here are just a few:\n\n* **Lightning-fast email generation**: Get high-quality email drafts in seconds, not hours.\n* **BYOL (Bring Your Own LLM)**: Try out different LLMs to find the one that suits your style and needs.\n* **Completely open source**: Join a community that's passionate about making Emailer the best it can be.\n\nWe're thrilled to share Emailer with you, and we can't wait to see what you create with it! Whether you're a student looking to streamline your email workflow or a developer interested in exploring the latest advancements in LLMs, Emailer is the perfect tool for you.\n\nReady to learn more about Emailer and start generating amazing emails? Click here to know more and get started on your email revolution!\n\nBest,\nThe Emailer Team", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 79, "funcName": "test_marketing_email", "created": 1749488788.908619, "msecs": 908.0, "relativeCreated": 3507.8229904174805, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-19"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d4e00>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.908703, "msecs": 908.0, "relativeCreated": 3507.9071521759033, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-21", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488788.90876, "msecs": 908.0, "relativeCreated": 3507.9641342163086, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-21", "asctime": "22:36:28"}]}, "teardown": {"duration": 0.002366250002523884, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00045253, completion_tokens_cost_usd_dollar: 0.00032627\nDEBUG: response_cost: 0.0007788\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.0007788\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:28 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00045253, completion_tokens_cost_usd_dollar: 0.00032627\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.0007788\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.0007788\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.90973, "msecs": 909.0, "relativeCreated": 3508.9340209960938, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488788.910125, "msecs": 910.0, "relativeCreated": 3509.329080581665, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488788.910252, "msecs": 910.0, "relativeCreated": 3509.456157684326, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.910443, "msecs": 910.0, "relativeCreated": 3509.6471309661865, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488788.9105642, "msecs": 910.0, "relativeCreated": 3509.76824760437, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488788.9106848, "msecs": 910.0, "relativeCreated": 3509.8888874053955, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00045253, completion_tokens_cost_usd_dollar: 0.00032627", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488788.910767, "msecs": 910.0, "relativeCreated": 3509.9711418151855, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "response_cost: 0.0007788", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488788.91085, "msecs": 910.0, "relativeCreated": 3510.054111480713, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.0007788", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488788.910903, "msecs": 910.0, "relativeCreated": 3510.1070404052734, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488788.911049, "msecs": 911.0, "relativeCreated": 3510.2529525756836, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488788.911156, "msecs": 911.0, "relativeCreated": 3510.3600025177, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-22", "asctime": "22:36:28"}]}}, {"nodeid": "tests/test_only_email.py::test_resignation_email", "lineno": 81, "outcome": "passed", "keywords": ["test_resignation_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.0007469579868484288, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488788.912683, "msecs": 912.0, "relativeCreated": 3511.8870735168457, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.012499667005613446, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\nDEBUG: An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed\nDEBUG: Error: An unexpected error occurred while generating the email.\n", "stderr": "\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:28 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], 'thinking': None}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.913377, "msecs": 913.0, "relativeCreated": 3512.5811100006104, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.9135108, "msecs": 913.0, "relativeCreated": 3512.7148628234863, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.913559, "msecs": 913.0, "relativeCreated": 3512.763023376465, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.913603, "msecs": 913.0, "relativeCreated": 3512.8071308135986, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488788.913794, "msecs": 913.0, "relativeCreated": 3512.998104095459, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.913849, "msecs": 913.0, "relativeCreated": 3513.0531787872314, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488788.91392, "msecs": 913.0, "relativeCreated": 3513.1239891052246, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488788.9146628, "msecs": 914.0, "relativeCreated": 3513.866901397705, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488788.914769, "msecs": 914.0, "relativeCreated": 3513.9729976654053, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488788.9148328, "msecs": 914.0, "relativeCreated": 3514.0368938446045, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.914915, "msecs": 914.0, "relativeCreated": 3514.1191482543945, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488788.914996, "msecs": 914.0, "relativeCreated": 3514.19997215271, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Please draft a formal resignation email addressed to my supervisor. I am resigning effective [date]\\n    and would like to express my gratitude for the opportunities provided. I am willing to help with the transition.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488788.915169, "msecs": 915.0, "relativeCreated": 3514.3730640411377, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.917047, "msecs": 917.0, "relativeCreated": 3516.2510871887207, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.917494, "msecs": 917.0, "relativeCreated": 3516.698122024536, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.91756, "msecs": 917.0, "relativeCreated": 3516.7641639709473, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.917755, "msecs": 917.0, "relativeCreated": 3516.9589519500732, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.917792, "msecs": 917.0, "relativeCreated": 3516.996145248413, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.917973, "msecs": 917.0, "relativeCreated": 3517.177104949951, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.918022, "msecs": 918.0, "relativeCreated": 3517.225980758667, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.918217, "msecs": 918.0, "relativeCreated": 3517.421007156372, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488788.918319, "msecs": 918.0, "relativeCreated": 3517.5230503082275, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488788.918714, "msecs": 918.0, "relativeCreated": 3517.918109893799, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488788.921938, "msecs": 921.0, "relativeCreated": 3521.14200592041, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24", "asctime": "22:36:28"}, {"name": "root", "msg": "An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_email.py", "filename": "only_email.py", "module": "only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 137, "funcName": "generate_email", "created": 1749488788.925248, "msecs": 925.0, "relativeCreated": 3524.451971054077, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}, {"name": "root", "msg": "Error: An unexpected error occurred while generating the email.", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 91, "funcName": "test_resignation_email", "created": 1749488788.925344, "msecs": 925.0, "relativeCreated": 3524.548053741455, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-24"}]}, "teardown": {"duration": 0.0005236249999143183, "outcome": "passed"}}, {"nodeid": "tests/test_only_email.py::test_workplace_conflict_email", "lineno": 93, "outcome": "passed", "keywords": ["test_workplace_conflict_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.00033145901397801936, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488788.9268339, "msecs": 926.0, "relativeCreated": 3526.0379314422607, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 1.311868874996435, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bf1a0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bf0b0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'996'), (b'x-ratelimit-remaining-tokens', b'10601'), (b'x-ratelimit-reset-requests', b'5m45.563s'), (b'x-ratelimit-reset-tokens', b'6.991999999s'), (b'x-request-id', b'req_01jxatgsj1fv48m4yhdermq0da'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2384378e93af5-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-63bb0926-03af-442c-bd65-71460844619e\", \"object\": \"chat.completion\", \"created\": 1749488789, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Resolving Recent Workplace Interactions and Moving Forward\\n\\nDear Colleague,\\n\\nI hope this email finds you well. I am writing to address some concerns I have regarding our recent interactions in the workplace. While I value our working relationship and the contributions you make to our team, I have noticed that our communication has become strained, and I believe it is essential that we discuss these issues to find a resolution.\\n\\nSpecifically, I have felt that our conversations have been misinterpreted, and misunderstandings have led to unnecessary tension. I am committed to maintaining a positive and respectful work environment, and I believe that open and honest communication is crucial in achieving this goal. I would like to propose that we schedule a meeting to discuss these matters further, with the aim of clearing up any misunderstandings and finding a way to move forward in a constructive and collaborative manner.\\n\\nThe meeting would provide an opportunity for us to share our perspectives, listen to each other's concerns, and work together to find a resolution that benefits both of us and the team as a whole. I am confident that by talking through our differences and finding common ground, we can strengthen our working relationship and improve our overall team dynamics.\\n\\nWould you be available to meet at your earliest convenience? I am flexible and can work around your schedule to find a time that suits you. I look forward to hearing from you and working together to resolve our differences.\\n\\nBest regards,\\n\\n[Your Name]\\n[Your Title]\\n[Company Name]\\n[Contact Information]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049749562000000004, \"prompt_tokens\": 222, \"prompt_time\": 0.053217748, \"completion_tokens\": 300, \"completion_time\": 1.0909090909999999, \"total_tokens\": 522, \"total_time\": 1.1441268390000001}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgsj1fv48m4yhdermq0da\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00013098, completion_tokens_cost_usd_dollar: 0.000237\nDEBUG: response_cost: 0.00036797999999999996\nDEBUG: Subject: Resolving Recent Workplace Interactions and Moving Forward\n\nDear Colleague,\n\nI hope this email finds you well. I am writing to address some concerns I have regarding our recent interactions in the workplace. While I value our working relationship and the contributions you make to our team, I have noticed that our communication has become strained, and I believe it is essential that we discuss these issues to find a resolution.\n\nSpecifically, I have felt that our conversations have been misinterpreted, and misunderstandings have led to unnecessary tension. I am committed to maintaining a positive and respectful work environment, and I believe that open and honest communication is crucial in achieving this goal. I would like to propose that we schedule a meeting to discuss these matters further, with the aim of clearing up any misunderstandings and finding a way to move forward in a constructive and collaborative manner.\n\nThe meeting would provide an opportunity for us to share our perspectives, listen to each other's concerns, and work together to find a resolution that benefits both of us and the team as a whole. I am confident that by talking through our differences and finding common ground, we can strengthen our working relationship and improve our overall team dynamics.\n\nWould you be available to meet at your earliest convenience? I am flexible and can work around your schedule to find a time that suits you. I look forward to hearing from you and working together to resolve our differences.\n\nBest regards,\n\n[Your Name]\n[Your Title]\n[Company Name]\n[Contact Information]\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5bc8c0>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:28 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], 'thinking': None}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:28 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-63bb0926-03af-442c-bd65-71460844619e\", \"object\": \"chat.completion\", \"created\": 1749488789, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Resolving Recent Workplace Interactions and Moving Forward\\n\\nDear Colleague,\\n\\nI hope this email finds you well. I am writing to address some concerns I have regarding our recent interactions in the workplace. While I value our working relationship and the contributions you make to our team, I have noticed that our communication has become strained, and I believe it is essential that we discuss these issues to find a resolution.\\n\\nSpecifically, I have felt that our conversations have been misinterpreted, and misunderstandings have led to unnecessary tension. I am committed to maintaining a positive and respectful work environment, and I believe that open and honest communication is crucial in achieving this goal. I would like to propose that we schedule a meeting to discuss these matters further, with the aim of clearing up any misunderstandings and finding a way to move forward in a constructive and collaborative manner.\\n\\nThe meeting would provide an opportunity for us to share our perspectives, listen to each other's concerns, and work together to find a resolution that benefits both of us and the team as a whole. I am confident that by talking through our differences and finding common ground, we can strengthen our working relationship and improve our overall team dynamics.\\n\\nWould you be available to meet at your earliest convenience? I am flexible and can work around your schedule to find a time that suits you. I look forward to hearing from you and working together to resolve our differences.\\n\\nBest regards,\\n\\n[Your Name]\\n[Your Title]\\n[Company Name]\\n[Contact Information]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049749562000000004, \"prompt_tokens\": 222, \"prompt_time\": 0.053217748, \"completion_tokens\": 300, \"completion_time\": 1.0909090909999999, \"total_tokens\": 522, \"total_time\": 1.1441268390000001}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgsj1fv48m4yhdermq0da\"}}\n\n\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00013098, completion_tokens_cost_usd_dollar: 0.000237\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00036797999999999996\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5bc8c0>>\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.9271, "msecs": 927.0, "relativeCreated": 3526.304006576538, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.9271529, "msecs": 927.0, "relativeCreated": 3526.3569355010986, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.927177, "msecs": 927.0, "relativeCreated": 3526.381015777588, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.92721, "msecs": 927.0, "relativeCreated": 3526.414155960083, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488788.927315, "msecs": 927.0, "relativeCreated": 3526.5190601348877, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.9273431, "msecs": 927.0, "relativeCreated": 3526.5471935272217, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488788.92737, "msecs": 927.0, "relativeCreated": 3526.57413482666, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488788.9279842, "msecs": 927.0, "relativeCreated": 3527.188301086426, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488788.928064, "msecs": 928.0, "relativeCreated": 3527.268171310425, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488788.9281092, "msecs": 928.0, "relativeCreated": 3527.313232421875, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488788.928166, "msecs": 928.0, "relativeCreated": 3527.369976043701, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488788.928229, "msecs": 928.0, "relativeCreated": 3527.433156967163, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email addressing a workplace conflict with a colleague. The email should highlight concerns\\n    regarding recent interactions, propose a meeting to discuss the issues, and aim for a resolution while remaining professional.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488788.9283478, "msecs": 928.0, "relativeCreated": 3527.5518894195557, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:28"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.92897, "msecs": 928.0, "relativeCreated": 3528.1741619110107, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bf1a0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.937345, "msecs": 937.0, "relativeCreated": 3536.5490913391113, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.937421, "msecs": 937.0, "relativeCreated": 3536.6251468658447, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bf0b0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.951066, "msecs": 951.0, "relativeCreated": 3550.2700805664062, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.9512498, "msecs": 951.0, "relativeCreated": 3550.4539012908936, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.9514189, "msecs": 951.0, "relativeCreated": 3550.6229400634766, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.951463, "msecs": 951.0, "relativeCreated": 3550.6670475006104, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.951604, "msecs": 951.0, "relativeCreated": 3550.8079528808594, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488788.951639, "msecs": 951.0, "relativeCreated": 3550.8430004119873, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'996'), (b'x-ratelimit-remaining-tokens', b'10601'), (b'x-ratelimit-reset-requests', b'5m45.563s'), (b'x-ratelimit-reset-tokens', b'6.991999999s'), (b'x-request-id', b'req_01jxatgsj1fv48m4yhdermq0da'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2384378e93af5-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.234796, "msecs": 234.0, "relativeCreated": 4834.000110626221, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488790.235372, "msecs": 235.0, "relativeCreated": 4834.57612991333, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.235631, "msecs": 235.0, "relativeCreated": 4834.835052490234, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.2361999, "msecs": 236.0, "relativeCreated": 4835.403919219971, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.2363641, "msecs": 236.0, "relativeCreated": 4835.568189620972, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.236512, "msecs": 236.0, "relativeCreated": 4835.716009140015, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488790.236765, "msecs": 236.0, "relativeCreated": 4835.968971252441, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-63bb0926-03af-442c-bd65-71460844619e\", \"object\": \"chat.completion\", \"created\": 1749488789, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Resolving Recent Workplace Interactions and Moving Forward\\n\\nDear Colleague,\\n\\nI hope this email finds you well. I am writing to address some concerns I have regarding our recent interactions in the workplace. While I value our working relationship and the contributions you make to our team, I have noticed that our communication has become strained, and I believe it is essential that we discuss these issues to find a resolution.\\n\\nSpecifically, I have felt that our conversations have been misinterpreted, and misunderstandings have led to unnecessary tension. I am committed to maintaining a positive and respectful work environment, and I believe that open and honest communication is crucial in achieving this goal. I would like to propose that we schedule a meeting to discuss these matters further, with the aim of clearing up any misunderstandings and finding a way to move forward in a constructive and collaborative manner.\\n\\nThe meeting would provide an opportunity for us to share our perspectives, listen to each other's concerns, and work together to find a resolution that benefits both of us and the team as a whole. I am confident that by talking through our differences and finding common ground, we can strengthen our working relationship and improve our overall team dynamics.\\n\\nWould you be available to meet at your earliest convenience? I am flexible and can work around your schedule to find a time that suits you. I look forward to hearing from you and working together to resolve our differences.\\n\\nBest regards,\\n\\n[Your Name]\\n[Your Title]\\n[Company Name]\\n[Contact Information]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049749562000000004, \"prompt_tokens\": 222, \"prompt_time\": 0.053217748, \"completion_tokens\": 300, \"completion_time\": 1.0909090909999999, \"total_tokens\": 522, \"total_time\": 1.1441268390000001}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgsj1fv48m4yhdermq0da\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.2370281, "msecs": 237.0, "relativeCreated": 4836.2321853637695, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488790.2374709, "msecs": 237.0, "relativeCreated": 4836.674928665161, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488790.237746, "msecs": 237.0, "relativeCreated": 4836.950063705444, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488790.237897, "msecs": 237.0, "relativeCreated": 4837.100982666016, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00013098, completion_tokens_cost_usd_dollar: 0.000237", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488790.2379782, "msecs": 237.0, "relativeCreated": 4837.182283401489, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "response_cost: 0.00036797999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488790.238049, "msecs": 238.0, "relativeCreated": 4837.253093719482, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27", "asctime": "22:36:30"}, {"name": "root", "msg": "Subject: Resolving Recent Workplace Interactions and Moving Forward\n\nDear Colleague,\n\nI hope this email finds you well. I am writing to address some concerns I have regarding our recent interactions in the workplace. While I value our working relationship and the contributions you make to our team, I have noticed that our communication has become strained, and I believe it is essential that we discuss these issues to find a resolution.\n\nSpecifically, I have felt that our conversations have been misinterpreted, and misunderstandings have led to unnecessary tension. I am committed to maintaining a positive and respectful work environment, and I believe that open and honest communication is crucial in achieving this goal. I would like to propose that we schedule a meeting to discuss these matters further, with the aim of clearing up any misunderstandings and finding a way to move forward in a constructive and collaborative manner.\n\nThe meeting would provide an opportunity for us to share our perspectives, listen to each other's concerns, and work together to find a resolution that benefits both of us and the team as a whole. I am confident that by talking through our differences and finding common ground, we can strengthen our working relationship and improve our overall team dynamics.\n\nWould you be available to meet at your earliest convenience? I am flexible and can work around your schedule to find a time that suits you. I look forward to hearing from you and working together to resolve our differences.\n\nBest regards,\n\n[Your Name]\n[Your Title]\n[Company Name]\n[Contact Information]", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 103, "funcName": "test_workplace_conflict_email", "created": 1749488790.238134, "msecs": 238.0, "relativeCreated": 4837.337970733643, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-27"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5bc8c0>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.238234, "msecs": 238.0, "relativeCreated": 4837.438106536865, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-29", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488790.238362, "msecs": 238.0, "relativeCreated": 4837.566137313843, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-29", "asctime": "22:36:30"}]}, "teardown": {"duration": 0.0018292080203536898, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00013098, completion_tokens_cost_usd_dollar: 0.000237\nDEBUG: response_cost: 0.00036797999999999996\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00036797999999999996\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:30 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00013098, completion_tokens_cost_usd_dollar: 0.000237\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00036797999999999996\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00036797999999999996\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.2393339, "msecs": 239.0, "relativeCreated": 4838.537931442261, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488790.239666, "msecs": 239.0, "relativeCreated": 4838.870048522949, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488790.2397592, "msecs": 239.0, "relativeCreated": 4838.963270187378, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.239918, "msecs": 239.0, "relativeCreated": 4839.12205696106, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488790.2399821, "msecs": 239.0, "relativeCreated": 4839.186191558838, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488790.240051, "msecs": 240.0, "relativeCreated": 4839.255094528198, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00013098, completion_tokens_cost_usd_dollar: 0.000237", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488790.240103, "msecs": 240.0, "relativeCreated": 4839.307069778442, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "response_cost: 0.00036797999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488790.24016, "msecs": 240.0, "relativeCreated": 4839.364051818848, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00036797999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488790.2402, "msecs": 240.0, "relativeCreated": 4839.404106140137, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488790.2403042, "msecs": 240.0, "relativeCreated": 4839.508295059204, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488790.24036, "msecs": 240.0, "relativeCreated": 4839.564085006714, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-30", "asctime": "22:36:30"}]}}, {"nodeid": "tests/test_only_email.py::test_bill_dispute_email", "lineno": 105, "outcome": "passed", "keywords": ["test_bill_dispute_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.0006130420078989118, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488790.241657, "msecs": 241.0, "relativeCreated": 4840.861082077026, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.012822290998883545, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\nDEBUG: An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed\nDEBUG: Error: An unexpected error occurred while generating the email.\n", "stderr": "\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], 'thinking': None}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.2422478, "msecs": 242.0, "relativeCreated": 4841.45188331604, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.242326, "msecs": 242.0, "relativeCreated": 4841.530084609985, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.242369, "msecs": 242.0, "relativeCreated": 4841.572999954224, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.242409, "msecs": 242.0, "relativeCreated": 4841.613054275513, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488790.2426271, "msecs": 242.0, "relativeCreated": 4841.831207275391, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.24268, "msecs": 242.0, "relativeCreated": 4841.884136199951, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488790.242733, "msecs": 242.0, "relativeCreated": 4841.937065124512, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488790.2433941, "msecs": 243.0, "relativeCreated": 4842.59819984436, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488790.243486, "msecs": 243.0, "relativeCreated": 4842.689990997314, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488790.2435358, "msecs": 243.0, "relativeCreated": 4842.739820480347, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.243604, "msecs": 243.0, "relativeCreated": 4842.80800819397, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488790.243675, "msecs": 243.0, "relativeCreated": 4842.879056930542, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Compose an email to dispute an unneeded bill. Include details that the bill appears to be erroneous and\\n    request a prompt review or cancellation of the charges.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488790.24381, "msecs": 243.0, "relativeCreated": 4843.0140018463135, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.244728, "msecs": 244.0, "relativeCreated": 4843.932151794434, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.245236, "msecs": 245.0, "relativeCreated": 4844.43998336792, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.245409, "msecs": 245.0, "relativeCreated": 4844.613075256348, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.245714, "msecs": 245.0, "relativeCreated": 4844.918012619019, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.245792, "msecs": 245.0, "relativeCreated": 4844.995975494385, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.246322, "msecs": 246.0, "relativeCreated": 4845.5259799957275, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.246398, "msecs": 246.0, "relativeCreated": 4845.602035522461, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.2466528, "msecs": 246.0, "relativeCreated": 4845.8569049835205, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488790.246788, "msecs": 246.0, "relativeCreated": 4845.992088317871, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488790.247201, "msecs": 247.0, "relativeCreated": 4846.405029296875, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488790.250447, "msecs": 250.0, "relativeCreated": 4849.651098251343, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32", "asctime": "22:36:30"}, {"name": "root", "msg": "An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_email.py", "filename": "only_email.py", "module": "only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 137, "funcName": "generate_email", "created": 1749488790.2544918, "msecs": 254.0, "relativeCreated": 4853.695869445801, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}, {"name": "root", "msg": "Error: An unexpected error occurred while generating the email.", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 115, "funcName": "test_bill_dispute_email", "created": 1749488790.2545831, "msecs": 254.0, "relativeCreated": 4853.787183761597, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-32"}]}, "teardown": {"duration": 0.0008192079840227962, "outcome": "passed"}}, {"nodeid": "tests/test_only_email.py::test_baby_shower_invite_email", "lineno": 117, "outcome": "passed", "keywords": ["test_baby_shower_invite_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.00067254199529998, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488790.256518, "msecs": 256.0, "relativeCreated": 4855.721950531006, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 1.3138759169960395, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f149520>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bec90>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'995'), (b'x-ratelimit-remaining-tokens', b'9956'), (b'x-ratelimit-reset-requests', b'7m10.674999999s'), (b'x-ratelimit-reset-tokens', b'10.218999999s'), (b'x-request-id', b'req_01jxatgtvhehqr4bkcpzprygkp'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2384bcc603a19-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-fa9109b6-21c7-420d-a899-cd50dbce5047\", \"object\": \"chat.completion\", \"created\": 1749488790, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: You're Invited: Join us for a Joyful Baby Shower Celebration!\\n\\nDear Loved Ones,\\n\\nWe are beyond thrilled to invite you to a baby shower celebration in honor of the upcoming arrival of our little bundle of joy! This special occasion is a wonderful opportunity for us to share our excitement with the people we love and cherish the most.\\n\\nThe baby shower is scheduled to take place on Saturday, April 17th, at 2:00 PM, at our home, located at 123 Main Street. We have planned a fun-filled afternoon with games, delicious treats, and great company. It will be a wonderful chance to catch up with each other, offer advice, and shower the baby with love and gifts.\\n\\nWe are really looking forward to sharing this special moment with all of you and creating unforgettable memories together. Your presence would mean the world to us, and we can't wait to celebrate with you. Please feel free to reach out to us if you have any questions or need more information.\\n\\nDate: Saturday, April 17th\\nTime: 2:00 PM\\nVenue: 123 Main Street (our home)\\n\\nWe are excited to see you there and share the joy of this new chapter in our lives. Thank you for being an important part of our journey, and we look forward to celebrating with you soon!\\n\\nWith love and appreciation,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.051551903, \"prompt_tokens\": 168, \"prompt_time\": 0.012072437, \"completion_tokens\": 279, \"completion_time\": 1.117751182, \"total_tokens\": 447, \"total_time\": 1.129823619}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgtvhehqr4bkcpzprygkp\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 9.912e-05, completion_tokens_cost_usd_dollar: 0.00022040999999999999\nDEBUG: response_cost: 0.00031953\nDEBUG: Subject: You're Invited: Join us for a Joyful Baby Shower Celebration!\n\nDear Loved Ones,\n\nWe are beyond thrilled to invite you to a baby shower celebration in honor of the upcoming arrival of our little bundle of joy! This special occasion is a wonderful opportunity for us to share our excitement with the people we love and cherish the most.\n\nThe baby shower is scheduled to take place on Saturday, April 17th, at 2:00 PM, at our home, located at 123 Main Street. We have planned a fun-filled afternoon with games, delicious treats, and great company. It will be a wonderful chance to catch up with each other, offer advice, and shower the baby with love and gifts.\n\nWe are really looking forward to sharing this special moment with all of you and creating unforgettable memories together. Your presence would mean the world to us, and we can't wait to celebrate with you. Please feel free to reach out to us if you have any questions or need more information.\n\nDate: Saturday, April 17th\nTime: 2:00 PM\nVenue: 123 Main Street (our home)\n\nWe are excited to see you there and share the joy of this new chapter in our lives. Thank you for being an important part of our journey, and we look forward to celebrating with you soon!\n\nWith love and appreciation,\n[Your Name]\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5bc680>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:30 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], 'thinking': None}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:30 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-fa9109b6-21c7-420d-a899-cd50dbce5047\", \"object\": \"chat.completion\", \"created\": 1749488790, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: You're Invited: Join us for a Joyful Baby Shower Celebration!\\n\\nDear Loved Ones,\\n\\nWe are beyond thrilled to invite you to a baby shower celebration in honor of the upcoming arrival of our little bundle of joy! This special occasion is a wonderful opportunity for us to share our excitement with the people we love and cherish the most.\\n\\nThe baby shower is scheduled to take place on Saturday, April 17th, at 2:00 PM, at our home, located at 123 Main Street. We have planned a fun-filled afternoon with games, delicious treats, and great company. It will be a wonderful chance to catch up with each other, offer advice, and shower the baby with love and gifts.\\n\\nWe are really looking forward to sharing this special moment with all of you and creating unforgettable memories together. Your presence would mean the world to us, and we can't wait to celebrate with you. Please feel free to reach out to us if you have any questions or need more information.\\n\\nDate: Saturday, April 17th\\nTime: 2:00 PM\\nVenue: 123 Main Street (our home)\\n\\nWe are excited to see you there and share the joy of this new chapter in our lives. Thank you for being an important part of our journey, and we look forward to celebrating with you soon!\\n\\nWith love and appreciation,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.051551903, \"prompt_tokens\": 168, \"prompt_time\": 0.012072437, \"completion_tokens\": 279, \"completion_time\": 1.117751182, \"total_tokens\": 447, \"total_time\": 1.129823619}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgtvhehqr4bkcpzprygkp\"}}\n\n\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 9.912e-05, completion_tokens_cost_usd_dollar: 0.00022040999999999999\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031953\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5bc680>>\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.257121, "msecs": 257.0, "relativeCreated": 4856.325149536133, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.257231, "msecs": 257.0, "relativeCreated": 4856.435060501099, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.257279, "msecs": 257.0, "relativeCreated": 4856.482982635498, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.257364, "msecs": 257.0, "relativeCreated": 4856.568098068237, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488790.257495, "msecs": 257.0, "relativeCreated": 4856.698989868164, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.257538, "msecs": 257.0, "relativeCreated": 4856.742143630981, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488790.2575638, "msecs": 257.0, "relativeCreated": 4856.767892837524, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488790.2579489, "msecs": 257.0, "relativeCreated": 4857.152938842773, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488790.2580428, "msecs": 258.0, "relativeCreated": 4857.246875762939, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488790.258095, "msecs": 258.0, "relativeCreated": 4857.299089431763, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488790.258155, "msecs": 258.0, "relativeCreated": 4857.359170913696, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488790.258215, "msecs": 258.0, "relativeCreated": 4857.419013977051, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    Create a cheerful email invitation for a baby shower. Include details such as date, time, venue, and a welcoming message\\n    inviting close friends and family to join in the celebration.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488790.258322, "msecs": 258.0, "relativeCreated": 4857.526063919067, "thread": 6107705344, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:30"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.25889, "msecs": 258.0, "relativeCreated": 4858.093976974487, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f149520>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.268955, "msecs": 268.0, "relativeCreated": 4868.159055709839, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.269042, "msecs": 269.0, "relativeCreated": 4868.246078491211, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e5bec90>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.283988, "msecs": 283.0, "relativeCreated": 4883.19206237793, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.284232, "msecs": 284.0, "relativeCreated": 4883.435964584351, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.284415, "msecs": 284.0, "relativeCreated": 4883.619070053101, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.2844522, "msecs": 284.0, "relativeCreated": 4883.65626335144, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.28455, "msecs": 284.0, "relativeCreated": 4883.754014968872, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488790.284585, "msecs": 284.0, "relativeCreated": 4883.7890625, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:31 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'995'), (b'x-ratelimit-remaining-tokens', b'9956'), (b'x-ratelimit-reset-requests', b'7m10.674999999s'), (b'x-ratelimit-reset-tokens', b'10.218999999s'), (b'x-request-id', b'req_01jxatgtvhehqr4bkcpzprygkp'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2384bcc603a19-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.5669482, "msecs": 566.0, "relativeCreated": 6166.152238845825, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488791.5674949, "msecs": 567.0, "relativeCreated": 6166.698932647705, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.567704, "msecs": 567.0, "relativeCreated": 6166.908025741577, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.56806, "msecs": 568.0, "relativeCreated": 6167.263984680176, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.568182, "msecs": 568.0, "relativeCreated": 6167.386054992676, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.568324, "msecs": 568.0, "relativeCreated": 6167.52815246582, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488791.568618, "msecs": 568.0, "relativeCreated": 6167.8221225738525, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-fa9109b6-21c7-420d-a899-cd50dbce5047\", \"object\": \"chat.completion\", \"created\": 1749488790, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: You're Invited: Join us for a Joyful Baby Shower Celebration!\\n\\nDear Loved Ones,\\n\\nWe are beyond thrilled to invite you to a baby shower celebration in honor of the upcoming arrival of our little bundle of joy! This special occasion is a wonderful opportunity for us to share our excitement with the people we love and cherish the most.\\n\\nThe baby shower is scheduled to take place on Saturday, April 17th, at 2:00 PM, at our home, located at 123 Main Street. We have planned a fun-filled afternoon with games, delicious treats, and great company. It will be a wonderful chance to catch up with each other, offer advice, and shower the baby with love and gifts.\\n\\nWe are really looking forward to sharing this special moment with all of you and creating unforgettable memories together. Your presence would mean the world to us, and we can't wait to celebrate with you. Please feel free to reach out to us if you have any questions or need more information.\\n\\nDate: Saturday, April 17th\\nTime: 2:00 PM\\nVenue: 123 Main Street (our home)\\n\\nWe are excited to see you there and share the joy of this new chapter in our lives. Thank you for being an important part of our journey, and we look forward to celebrating with you soon!\\n\\nWith love and appreciation,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.051551903, \"prompt_tokens\": 168, \"prompt_time\": 0.012072437, \"completion_tokens\": 279, \"completion_time\": 1.117751182, \"total_tokens\": 447, \"total_time\": 1.129823619}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgtvhehqr4bkcpzprygkp\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.568864, "msecs": 568.0, "relativeCreated": 6168.068170547485, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488791.569289, "msecs": 569.0, "relativeCreated": 6168.493032455444, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488791.5696168, "msecs": 569.0, "relativeCreated": 6168.820858001709, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488791.569799, "msecs": 569.0, "relativeCreated": 6169.003009796143, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 9.912e-05, completion_tokens_cost_usd_dollar: 0.00022040999999999999", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488791.569901, "msecs": 569.0, "relativeCreated": 6169.105052947998, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031953", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488791.570068, "msecs": 570.0, "relativeCreated": 6169.271945953369, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35", "asctime": "22:36:31"}, {"name": "root", "msg": "Subject: You're Invited: Join us for a Joyful Baby Shower Celebration!\n\nDear Loved Ones,\n\nWe are beyond thrilled to invite you to a baby shower celebration in honor of the upcoming arrival of our little bundle of joy! This special occasion is a wonderful opportunity for us to share our excitement with the people we love and cherish the most.\n\nThe baby shower is scheduled to take place on Saturday, April 17th, at 2:00 PM, at our home, located at 123 Main Street. We have planned a fun-filled afternoon with games, delicious treats, and great company. It will be a wonderful chance to catch up with each other, offer advice, and shower the baby with love and gifts.\n\nWe are really looking forward to sharing this special moment with all of you and creating unforgettable memories together. Your presence would mean the world to us, and we can't wait to celebrate with you. Please feel free to reach out to us if you have any questions or need more information.\n\nDate: Saturday, April 17th\nTime: 2:00 PM\nVenue: 123 Main Street (our home)\n\nWe are excited to see you there and share the joy of this new chapter in our lives. Thank you for being an important part of our journey, and we look forward to celebrating with you soon!\n\nWith love and appreciation,\n[Your Name]", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 127, "funcName": "test_baby_shower_invite_email", "created": 1749488791.570166, "msecs": 570.0, "relativeCreated": 6169.370174407959, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-35"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5bc680>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.570255, "msecs": 570.0, "relativeCreated": 6169.459104537964, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-37", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488791.570325, "msecs": 570.0, "relativeCreated": 6169.528961181641, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-37", "asctime": "22:36:31"}]}, "teardown": {"duration": 0.0018816670053638518, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 9.912e-05, completion_tokens_cost_usd_dollar: 0.00022040999999999999\nDEBUG: response_cost: 0.00031953\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00031953\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:31 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 9.912e-05, completion_tokens_cost_usd_dollar: 0.00022040999999999999\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031953\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00031953\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.571289, "msecs": 571.0, "relativeCreated": 6170.493125915527, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488791.5716531, "msecs": 571.0, "relativeCreated": 6170.857191085815, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488791.571758, "msecs": 571.0, "relativeCreated": 6170.96209526062, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.5719428, "msecs": 571.0, "relativeCreated": 6171.146869659424, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488791.5720232, "msecs": 572.0, "relativeCreated": 6171.227216720581, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488791.572087, "msecs": 572.0, "relativeCreated": 6171.29111289978, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 9.912e-05, completion_tokens_cost_usd_dollar: 0.00022040999999999999", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488791.57214, "msecs": 572.0, "relativeCreated": 6171.344041824341, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031953", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488791.5721998, "msecs": 572.0, "relativeCreated": 6171.403884887695, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00031953", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488791.5722399, "msecs": 572.0, "relativeCreated": 6171.443939208984, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488791.572356, "msecs": 572.0, "relativeCreated": 6171.560049057007, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488791.572418, "msecs": 572.0, "relativeCreated": 6171.622037887573, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-38", "asctime": "22:36:31"}]}}, {"nodeid": "tests/test_only_email.py::test_urgent_meeting_email", "lineno": 129, "outcome": "passed", "keywords": ["test_urgent_meeting_email", "asyncio", "pytestmark", "test_only_email.py", "tests", "elevate", ""], "setup": {"duration": 0.0005596250121016055, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488791.573587, "msecs": 573.0, "relativeCreated": 6172.791004180908, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.012454790994524956, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\nDEBUG: An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed\nDEBUG: Error: An unexpected error occurred while generating the email.\n", "stderr": "\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:31 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], 'thinking': None}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.574114, "msecs": 574.0, "relativeCreated": 6173.318147659302, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.574212, "msecs": 574.0, "relativeCreated": 6173.4161376953125, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.57426, "msecs": 574.0, "relativeCreated": 6173.464059829712, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.574315, "msecs": 574.0, "relativeCreated": 6173.519134521484, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488791.574551, "msecs": 574.0, "relativeCreated": 6173.755168914795, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.5746188, "msecs": 574.0, "relativeCreated": 6173.82287979126, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488791.574677, "msecs": 574.0, "relativeCreated": 6173.881053924561, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488791.5753212, "msecs": 575.0, "relativeCreated": 6174.525260925293, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488791.575399, "msecs": 575.0, "relativeCreated": 6174.60298538208, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488791.575456, "msecs": 575.0, "relativeCreated": 6174.659967422485, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.575517, "msecs": 575.0, "relativeCreated": 6174.721002578735, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488791.575582, "msecs": 575.0, "relativeCreated": 6174.78609085083, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful professional emails. Your goal is to write a fomral tone email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n**Guidelines for generating an email:**\\n1. Start with an interesting subject line\\n2. Give greetings\\n3. Write the core email body\\n4. Include a closing line\\n5. End with a signature\\n6. Showcase professional etiquette\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n\\n'}, {'role': 'user', 'content': '\\n    Draft an email to request an urgent meeting regarding unforeseen project issues that need immediate attention.\\n    Please include proposed meeting times and emphasize the urgency of the situation.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488791.575715, "msecs": 575.0, "relativeCreated": 6174.919128417969, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.576582, "msecs": 576.0, "relativeCreated": 6175.786018371582, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.577204, "msecs": 577.0, "relativeCreated": 6176.408052444458, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.577419, "msecs": 577.0, "relativeCreated": 6176.623106002808, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.5777862, "msecs": 577.0, "relativeCreated": 6176.990270614624, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.577868, "msecs": 577.0, "relativeCreated": 6177.072048187256, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.578038, "msecs": 578.0, "relativeCreated": 6177.242040634155, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.57809, "msecs": 578.0, "relativeCreated": 6177.294015884399, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.578292, "msecs": 578.0, "relativeCreated": 6177.495956420898, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488791.5784068, "msecs": 578.0, "relativeCreated": 6177.610874176025, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488791.578809, "msecs": 578.0, "relativeCreated": 6178.01308631897, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488791.582479, "msecs": 582.0, "relativeCreated": 6181.68306350708, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40", "asctime": "22:36:31"}, {"name": "root", "msg": "An error occurred: litellm.APIError: APIError: GroqException - Event loop is closed", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_email.py", "filename": "only_email.py", "module": "only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 137, "funcName": "generate_email", "created": 1749488791.5859818, "msecs": 585.0, "relativeCreated": 6185.18590927124, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}, {"name": "root", "msg": "Error: An unexpected error occurred while generating the email.", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_email.py", "filename": "test_only_email.py", "module": "test_only_email", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 139, "funcName": "test_urgent_meeting_email", "created": 1749488791.586056, "msecs": 586.0, "relativeCreated": 6185.260057449341, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-40"}]}, "teardown": {"duration": 0.0005291670095175505, "outcome": "passed"}}, {"nodeid": "tests/test_only_json.py::test_calendar_event", "lineno": 33, "outcome": "passed", "keywords": ["test_calendar_event", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.0004939170030411333, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488791.5878148, "msecs": 587.0, "relativeCreated": 6187.018871307373, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.8633127919747494, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'List 5 important events in the XIX century'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'List 5 important events in the XIX century'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'List 5 important events in the XIX century'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f1845c0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f14a3f0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'994'), (b'x-ratelimit-remaining-tokens', b'9939'), (b'x-ratelimit-reset-requests', b'8m37.060999999s'), (b'x-ratelimit-reset-tokens', b'10.305s'), (b'x-request-id', b'req_01jxatgw5cfv48xgq920fnt5ke'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238541cd13c7c-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-f52854b3-c039-4061-806a-3930f4f1069f\", \"object\": \"chat.completion\", \"created\": 1749488791, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_abhy\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"events\\\":[{\\\"date\\\":\\\"1803-05-18\\\",\\\"name\\\":\\\"Napoleonic Wars began\\\",\\\"participants\\\":[\\\"France\\\",\\\"Europe\\\"]},{\\\"date\\\":\\\"1837-06-20\\\",\\\"name\\\":\\\"Accession of Queen Victoria\\\",\\\"participants\\\":[\\\"United Kingdom\\\"]},{\\\"date\\\":\\\"1848-02-21\\\",\\\"name\\\":\\\"Karl Marx published The Communist Manifesto\\\",\\\"participants\\\":[\\\"Karl Marx\\\",\\\"Friedrich Engels\\\"]},{\\\"date\\\":\\\"1859-11-24\\\",\\\"name\\\":\\\"Charles Darwin published On the Origin of Species\\\",\\\"participants\\\":[\\\"Charles Darwin\\\"]},{\\\"date\\\":\\\"1865-04-09\\\",\\\"name\\\":\\\"American Civil War ended\\\",\\\"participants\\\":[\\\"United States\\\",\\\"Confederate States of America\\\"]}]}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.051409703, \"prompt_tokens\": 410, \"prompt_time\": 0.026126089, \"completion_tokens\": 176, \"completion_time\": 0.64, \"total_tokens\": 586, \"total_time\": 0.666126089}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgw5cfv48xgq920fnt5ke\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002419, completion_tokens_cost_usd_dollar: 0.00013904\nDEBUG: response_cost: 0.00038093999999999997\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14a150>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'List 5 important events in the XIX century'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:31 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'List 5 important events in the XIX century'}], 'thinking': None}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:31 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'List 5 important events in the XIX century'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-f52854b3-c039-4061-806a-3930f4f1069f\", \"object\": \"chat.completion\", \"created\": 1749488791, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_abhy\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"events\\\":[{\\\"date\\\":\\\"1803-05-18\\\",\\\"name\\\":\\\"Napoleonic Wars began\\\",\\\"participants\\\":[\\\"France\\\",\\\"Europe\\\"]},{\\\"date\\\":\\\"1837-06-20\\\",\\\"name\\\":\\\"Accession of Queen Victoria\\\",\\\"participants\\\":[\\\"United Kingdom\\\"]},{\\\"date\\\":\\\"1848-02-21\\\",\\\"name\\\":\\\"Karl Marx published The Communist Manifesto\\\",\\\"participants\\\":[\\\"Karl Marx\\\",\\\"Friedrich Engels\\\"]},{\\\"date\\\":\\\"1859-11-24\\\",\\\"name\\\":\\\"Charles Darwin published On the Origin of Species\\\",\\\"participants\\\":[\\\"Charles Darwin\\\"]},{\\\"date\\\":\\\"1865-04-09\\\",\\\"name\\\":\\\"American Civil War ended\\\",\\\"participants\\\":[\\\"United States\\\",\\\"Confederate States of America\\\"]}]}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.051409703, \"prompt_tokens\": 410, \"prompt_time\": 0.026126089, \"completion_tokens\": 176, \"completion_time\": 0.64, \"total_tokens\": 586, \"total_time\": 0.666126089}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgw5cfv48xgq920fnt5ke\"}}\n\n\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002419, completion_tokens_cost_usd_dollar: 0.00013904\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00038093999999999997\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14a150>>\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.592556, "msecs": 592.0, "relativeCreated": 6191.760063171387, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.592694, "msecs": 592.0, "relativeCreated": 6191.8981075286865, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'List 5 important events in the XIX century'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.592739, "msecs": 592.0, "relativeCreated": 6191.943168640137, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.59278, "msecs": 592.0, "relativeCreated": 6191.984176635742, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488791.593108, "msecs": 593.0, "relativeCreated": 6192.312002182007, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.5931919, "msecs": 593.0, "relativeCreated": 6192.395925521851, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488791.593243, "msecs": 593.0, "relativeCreated": 6192.446947097778, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488791.593962, "msecs": 593.0, "relativeCreated": 6193.166017532349, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'List 5 important events in the XIX century'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488791.594044, "msecs": 594.0, "relativeCreated": 6193.24803352356, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'EventsList', 'schema': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488791.594104, "msecs": 594.0, "relativeCreated": 6193.308115005493, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488791.594196, "msecs": 594.0, "relativeCreated": 6193.400144577026, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488791.594275, "msecs": 594.0, "relativeCreated": 6193.479061126709, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'List 5 important events in the XIX century'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'CalendarEvent': {'description': 'A calendar event with relevant details (name, date, participants).', 'properties': {'name': {'description': 'Name of the event in string format.', 'title': 'Name', 'type': 'string'}, 'date': {'description': \"Date of the event in 'YYYY-MM-DD' or similar string format.\", 'title': 'Date', 'type': 'string'}, 'participants': {'description': 'List of participants; each participant name is a string.', 'items': {'type': 'string'}, 'title': 'Participants', 'type': 'array'}}, 'required': ['name', 'date', 'participants'], 'title': 'CalendarEvent', 'type': 'object'}}, 'description': 'A container for multiple CalendarEvent objects, stored in a list.', 'properties': {'events': {'description': 'A JSON array of CalendarEvent objects.', 'items': {'$ref': '#/$defs/CalendarEvent'}, 'title': 'Events', 'type': 'array'}}, 'required': ['events'], 'title': 'EventsList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488791.594389, "msecs": 594.0, "relativeCreated": 6193.5930252075195, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:31"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.5952199, "msecs": 595.0, "relativeCreated": 6194.4239139556885, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f1845c0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.604175, "msecs": 604.0, "relativeCreated": 6203.379154205322, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.604222, "msecs": 604.0, "relativeCreated": 6203.426122665405, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f14a3f0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.6154602, "msecs": 615.0, "relativeCreated": 6214.6642208099365, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.6156611, "msecs": 615.0, "relativeCreated": 6214.865207672119, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.615861, "msecs": 615.0, "relativeCreated": 6215.065002441406, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.615895, "msecs": 615.0, "relativeCreated": 6215.099096298218, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.616006, "msecs": 616.0, "relativeCreated": 6215.2099609375, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488791.616035, "msecs": 616.0, "relativeCreated": 6215.23904800415, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:32 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'994'), (b'x-ratelimit-remaining-tokens', b'9939'), (b'x-ratelimit-reset-requests', b'8m37.060999999s'), (b'x-ratelimit-reset-tokens', b'10.305s'), (b'x-request-id', b'req_01jxatgw5cfv48xgq920fnt5ke'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238541cd13c7c-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.385979, "msecs": 385.0, "relativeCreated": 6985.183000564575, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488792.386513, "msecs": 386.0, "relativeCreated": 6985.717058181763, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.3867671, "msecs": 386.0, "relativeCreated": 6985.971212387085, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.387056, "msecs": 387.0, "relativeCreated": 6986.260175704956, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.3871949, "msecs": 387.0, "relativeCreated": 6986.398935317993, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.387466, "msecs": 387.0, "relativeCreated": 6986.670017242432, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488792.387954, "msecs": 387.0, "relativeCreated": 6987.1580600738525, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-f52854b3-c039-4061-806a-3930f4f1069f\", \"object\": \"chat.completion\", \"created\": 1749488791, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_abhy\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"events\\\":[{\\\"date\\\":\\\"1803-05-18\\\",\\\"name\\\":\\\"Napoleonic Wars began\\\",\\\"participants\\\":[\\\"France\\\",\\\"Europe\\\"]},{\\\"date\\\":\\\"1837-06-20\\\",\\\"name\\\":\\\"Accession of Queen Victoria\\\",\\\"participants\\\":[\\\"United Kingdom\\\"]},{\\\"date\\\":\\\"1848-02-21\\\",\\\"name\\\":\\\"Karl Marx published The Communist Manifesto\\\",\\\"participants\\\":[\\\"Karl Marx\\\",\\\"Friedrich Engels\\\"]},{\\\"date\\\":\\\"1859-11-24\\\",\\\"name\\\":\\\"Charles Darwin published On the Origin of Species\\\",\\\"participants\\\":[\\\"Charles Darwin\\\"]},{\\\"date\\\":\\\"1865-04-09\\\",\\\"name\\\":\\\"American Civil War ended\\\",\\\"participants\\\":[\\\"United States\\\",\\\"Confederate States of America\\\"]}]}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.051409703, \"prompt_tokens\": 410, \"prompt_time\": 0.026126089, \"completion_tokens\": 176, \"completion_time\": 0.64, \"total_tokens\": 586, \"total_time\": 0.666126089}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgw5cfv48xgq920fnt5ke\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.388274, "msecs": 388.0, "relativeCreated": 6987.478017807007, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488792.4507709, "msecs": 450.0, "relativeCreated": 7049.9749183654785, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488792.450942, "msecs": 450.0, "relativeCreated": 7050.146102905273, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488792.4510138, "msecs": 451.0, "relativeCreated": 7050.217866897583, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002419, completion_tokens_cost_usd_dollar: 0.00013904", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488792.451052, "msecs": 451.0, "relativeCreated": 7050.256013870239, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "response_cost: 0.00038093999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488792.451091, "msecs": 451.0, "relativeCreated": 7050.295114517212, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-43", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14a150>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.451226, "msecs": 451.0, "relativeCreated": 7050.430059432983, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-45", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488792.451261, "msecs": 451.0, "relativeCreated": 7050.465106964111, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-45", "asctime": "22:36:32"}]}, "teardown": {"duration": 0.0007736660190857947, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002419, completion_tokens_cost_usd_dollar: 0.00013904\nDEBUG: response_cost: 0.00038093999999999997\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00038093999999999997\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:32 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002419, completion_tokens_cost_usd_dollar: 0.00013904\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00038093999999999997\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00038093999999999997\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.451576, "msecs": 451.0, "relativeCreated": 7050.7800579071045, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488792.451702, "msecs": 451.0, "relativeCreated": 7050.906181335449, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488792.451742, "msecs": 451.0, "relativeCreated": 7050.945997238159, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.4518049, "msecs": 451.0, "relativeCreated": 7051.008939743042, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488792.4518368, "msecs": 451.0, "relativeCreated": 7051.040887832642, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488792.451872, "msecs": 451.0, "relativeCreated": 7051.076173782349, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002419, completion_tokens_cost_usd_dollar: 0.00013904", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488792.451898, "msecs": 451.0, "relativeCreated": 7051.102161407471, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "response_cost: 0.00038093999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488792.451927, "msecs": 451.0, "relativeCreated": 7051.131010055542, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00038093999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488792.451947, "msecs": 451.0, "relativeCreated": 7051.1510372161865, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488792.4520152, "msecs": 452.0, "relativeCreated": 7051.21922492981, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488792.452047, "msecs": 452.0, "relativeCreated": 7051.251173019409, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-46", "asctime": "22:36:32"}]}}, {"nodeid": "tests/test_only_json.py::test_extraction_of_contact_info", "lineno": 57, "outcome": "failed", "keywords": ["test_extraction_of_contact_info", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.00022604200057685375, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488792.4524958, "msecs": 452.0, "relativeCreated": 7051.699876785278, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.005453457997646183, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_json.py", "lineno": 81, "message": ""}, {"path": "src/elevate/only_json.py", "lineno": 62, "message": "in parse"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], response_format={'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], response_format={'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:32 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], 'thinking': None}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.4531672, "msecs": 453.0, "relativeCreated": 7052.371263504028, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.453214, "msecs": 453.0, "relativeCreated": 7052.417993545532, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], response_format={'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.453235, "msecs": 453.0, "relativeCreated": 7052.438974380493, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.453255, "msecs": 453.0, "relativeCreated": 7052.459001541138, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488792.45332, "msecs": 453.0, "relativeCreated": 7052.524089813232, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.453344, "msecs": 453.0, "relativeCreated": 7052.548170089722, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488792.453365, "msecs": 453.0, "relativeCreated": 7052.569150924683, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488792.45362, "msecs": 453.0, "relativeCreated": 7052.824020385742, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488792.453687, "msecs": 453.0, "relativeCreated": 7052.89101600647, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488792.453714, "msecs": 453.0, "relativeCreated": 7052.917957305908, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.453751, "msecs": 453.0, "relativeCreated": 7052.955150604248, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488792.453795, "msecs": 453.0, "relativeCreated": 7052.999019622803, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Basic contact information with mandatory fields: name, email, phone.', 'properties': {'name': {'description': \"The contact's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'email': {'description': \"The contact's email address in string format, e.g., 'johndoe@example.com'.\", 'title': 'Email', 'type': 'string'}, 'phone': {'description': \"The contact's phone number in string format, e.g., '555-1234'.\", 'title': 'Phone', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488792.453851, "msecs": 453.0, "relativeCreated": 7053.055047988892, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.4542232, "msecs": 454.0, "relativeCreated": 7053.427219390869, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.454384, "msecs": 454.0, "relativeCreated": 7053.588151931763, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.454408, "msecs": 454.0, "relativeCreated": 7053.611993789673, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.454485, "msecs": 454.0, "relativeCreated": 7053.689002990723, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.454504, "msecs": 454.0, "relativeCreated": 7053.708076477051, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.45455, "msecs": 454.0, "relativeCreated": 7053.754091262817, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.454571, "msecs": 454.0, "relativeCreated": 7053.775072097778, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.454645, "msecs": 454.0, "relativeCreated": 7053.8489818573, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488792.4546938, "msecs": 454.0, "relativeCreated": 7053.897857666016, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488792.454819, "msecs": 454.0, "relativeCreated": 7054.023027420044, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488792.456311, "msecs": 456.0, "relativeCreated": 7055.5150508880615, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-48", "asctime": "22:36:32"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-e1faad9f-2340-44cd-8803-fa7fe627af9e', created=1749488792, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f148f20>, stream = False\ndata = {'messages': [{'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234....-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...': {...}, 'phone': {...}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\", 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = None, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None\nresponse_format = {'json_schema': {'name': 'ContactInfo', 'schema': {'description': 'Basic contact information with mandatory fields: na...one', 'type': 'string'}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', ...}}, 'type': 'json_schema'}\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'aaddc007-41f2-4698-bc16-591ac54a8022', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f148f20>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234. I'll be available most weekdays.\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-e1faad9f-2340-44cd-8803-fa7fe627af9e', created=1749488792, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f148f20>, stream = False\ndata = {'messages': [{'content': \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234....-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...': {...}, 'phone': {...}}, 'required': ['name', 'email', 'phone'], 'title': 'ContactInfo', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_extraction_of_contact_info(settings: Any) -> None:\n        \"\"\"Test extracting basic contact information from unstructured text.\"\"\"\n    \n        class ContactInfo(BaseModel):\n            \"\"\"Basic contact information with mandatory fields: name, email, phone.\"\"\"\n    \n            name: str = Field(..., description=\"The contact's full name in string format.\")\n            email: str = Field(\n                ...,\n                description=\"The contact's email address in string format, e.g., 'johndoe@example.com'.\",\n            )\n            phone: str = Field(\n                ...,\n                description=\"The contact's phone number in string format, e.g., '555-1234'.\",\n            )\n    \n        text = (\n            \"Hello, my name is John Doe. You can reach me at johndoe@example.com or call me at 555-1234.\"\n            \" I'll be available most weekdays.\"\n        )\n    \n        only_json = OnlyJson(with_model=settings.with_model)\n>       contact = await only_json.parse(content=text, schema=ContactInfo)\n\ntests/test_only_json.py:81: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_json.py:62: in parse\n    resp = await acompletion(model=self.model_id, messages=messages, response_format=json_schema)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'aaddc007-41f2-4698-bc16-591ac54a8022', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f148f20>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003139580076094717, "outcome": "passed"}}, {"nodeid": "tests/test_only_json.py::test_nested_structures", "lineno": 85, "outcome": "passed", "keywords": ["test_nested_structures", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.0002262089983560145, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488792.6039839, "msecs": 603.0, "relativeCreated": 7203.187942504883, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.4029489169770386, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f507320>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5071d0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'993'), (b'x-ratelimit-remaining-tokens', b'9527'), (b'x-ratelimit-reset-requests', b'10m3.791s'), (b'x-ratelimit-reset-tokens', b'12.363s'), (b'x-request-id', b'req_01jxatgx4yfv4bsvav5egvr8s9'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2385a68ec85b9-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-0ed177a2-856d-4be1-9fc4-ccaee69e7524\", \"object\": \"chat.completion\", \"created\": 1749488792, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_xxch\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"company_name\\\":\\\"Acme Corp\\\",\\\"departments\\\":[{\\\"headcount\\\":25,\\\"manager\\\":\\\"Alice Johnson\\\",\\\"name\\\":\\\"R\\\\u0026D\\\"},{\\\"headcount\\\":15,\\\"manager\\\":\\\"Bob Smith\\\",\\\"name\\\":\\\"Marketing\\\"}]}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.056103032000000004, \"prompt_tokens\": 450, \"prompt_time\": 0.032270189, \"completion_tokens\": 57, \"completion_time\": 0.207272727, \"total_tokens\": 507, \"total_time\": 0.239542916}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgx4yfv4bsvav5egvr8s9\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002655, completion_tokens_cost_usd_dollar: 4.5029999999999994e-05\nDEBUG: response_cost: 0.00031053\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e4cd7c0>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:32 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], 'thinking': None}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:32 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-0ed177a2-856d-4be1-9fc4-ccaee69e7524\", \"object\": \"chat.completion\", \"created\": 1749488792, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_xxch\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"company_name\\\":\\\"Acme Corp\\\",\\\"departments\\\":[{\\\"headcount\\\":25,\\\"manager\\\":\\\"Alice Johnson\\\",\\\"name\\\":\\\"R\\\\u0026D\\\"},{\\\"headcount\\\":15,\\\"manager\\\":\\\"Bob Smith\\\",\\\"name\\\":\\\"Marketing\\\"}]}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.056103032000000004, \"prompt_tokens\": 450, \"prompt_time\": 0.032270189, \"completion_tokens\": 57, \"completion_time\": 0.207272727, \"total_tokens\": 507, \"total_time\": 0.239542916}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgx4yfv4bsvav5egvr8s9\"}}\n\n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002655, completion_tokens_cost_usd_dollar: 4.5029999999999994e-05\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031053\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e4cd7c0>>\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.604797, "msecs": 604.0, "relativeCreated": 7204.000949859619, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.604844, "msecs": 604.0, "relativeCreated": 7204.048156738281, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.604864, "msecs": 604.0, "relativeCreated": 7204.067945480347, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.604882, "msecs": 604.0, "relativeCreated": 7204.086065292358, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488792.60494, "msecs": 604.0, "relativeCreated": 7204.14400100708, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.604963, "msecs": 604.0, "relativeCreated": 7204.167127609253, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488792.60498, "msecs": 604.0, "relativeCreated": 7204.184055328369, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488792.605197, "msecs": 605.0, "relativeCreated": 7204.401016235352, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488792.605258, "msecs": 605.0, "relativeCreated": 7204.462051391602, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Company', 'schema': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488792.6052961, "msecs": 605.0, "relativeCreated": 7204.500198364258, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488792.6053371, "msecs": 605.0, "relativeCreated": 7204.541206359863, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488792.60537, "msecs": 605.0, "relativeCreated": 7204.574108123779, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Acme Corp has 2 departments. The first is R&D, managed by Alice Johnson with 25 people. The second is Marketing, managed by Bob Smith with 15 people.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Department': {'description': 'Data about a single department, including its name, manager, and headcount.', 'properties': {'name': {'description': 'Name of the department in string format.', 'title': 'Name', 'type': 'string'}, 'manager': {'description': 'Full name of the manager in string format.', 'title': 'Manager', 'type': 'string'}, 'headcount': {'description': 'Number of people in the department as an integer.', 'title': 'Headcount', 'type': 'integer'}}, 'required': ['name', 'manager', 'headcount'], 'title': 'Department', 'type': 'object'}}, 'description': 'Represents a single company, including a list of its departments.', 'properties': {'company_name': {'description': 'Name of the company in string format.', 'title': 'Company Name', 'type': 'string'}, 'departments': {'description': 'A JSON array of Department objects.', 'items': {'$ref': '#/$defs/Department'}, 'title': 'Departments', 'type': 'array'}}, 'required': ['company_name', 'departments'], 'title': 'Company', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488792.60542, "msecs": 605.0, "relativeCreated": 7204.624176025391, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:32"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.605699, "msecs": 605.0, "relativeCreated": 7204.903125762939, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f507320>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.614163, "msecs": 614.0, "relativeCreated": 7213.366985321045, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.614194, "msecs": 614.0, "relativeCreated": 7213.397979736328, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5071d0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.628593, "msecs": 628.0, "relativeCreated": 7227.797031402588, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.628692, "msecs": 628.0, "relativeCreated": 7227.895975112915, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.6287968, "msecs": 628.0, "relativeCreated": 7228.00087928772, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.62882, "msecs": 628.0, "relativeCreated": 7228.024005889893, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.628909, "msecs": 628.0, "relativeCreated": 7228.113174438477, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488792.628932, "msecs": 628.0, "relativeCreated": 7228.13606262207, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'993'), (b'x-ratelimit-remaining-tokens', b'9527'), (b'x-ratelimit-reset-requests', b'10m3.791s'), (b'x-ratelimit-reset-tokens', b'12.363s'), (b'x-request-id', b'req_01jxatgx4yfv4bsvav5egvr8s9'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2385a68ec85b9-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.000196, "msecs": 0.0, "relativeCreated": 7599.400043487549, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488793.0008352, "msecs": 0.0, "relativeCreated": 7600.03924369812, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.001173, "msecs": 1.0, "relativeCreated": 7600.377082824707, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.001916, "msecs": 1.0, "relativeCreated": 7601.1199951171875, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.002059, "msecs": 2.0, "relativeCreated": 7601.263046264648, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.002187, "msecs": 2.0, "relativeCreated": 7601.391077041626, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488793.002409, "msecs": 2.0, "relativeCreated": 7601.6130447387695, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-0ed177a2-856d-4be1-9fc4-ccaee69e7524\", \"object\": \"chat.completion\", \"created\": 1749488792, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_xxch\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"company_name\\\":\\\"Acme Corp\\\",\\\"departments\\\":[{\\\"headcount\\\":25,\\\"manager\\\":\\\"Alice Johnson\\\",\\\"name\\\":\\\"R\\\\u0026D\\\"},{\\\"headcount\\\":15,\\\"manager\\\":\\\"Bob Smith\\\",\\\"name\\\":\\\"Marketing\\\"}]}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.056103032000000004, \"prompt_tokens\": 450, \"prompt_time\": 0.032270189, \"completion_tokens\": 57, \"completion_time\": 0.207272727, \"total_tokens\": 507, \"total_time\": 0.239542916}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgx4yfv4bsvav5egvr8s9\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.002701, "msecs": 2.0, "relativeCreated": 7601.905107498169, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488793.0056021, "msecs": 5.0, "relativeCreated": 7604.806184768677, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488793.005963, "msecs": 5.0, "relativeCreated": 7605.1671504974365, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.0061371, "msecs": 6.0, "relativeCreated": 7605.341196060181, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002655, completion_tokens_cost_usd_dollar: 4.5029999999999994e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488793.006218, "msecs": 6.0, "relativeCreated": 7605.422019958496, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031053", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488793.006335, "msecs": 6.0, "relativeCreated": 7605.539083480835, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-51", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e4cd7c0>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.006516, "msecs": 6.0, "relativeCreated": 7605.720043182373, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-53", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488793.006643, "msecs": 6.0, "relativeCreated": 7605.847120285034, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-53", "asctime": "22:36:33"}]}, "teardown": {"duration": 0.0018444999877829105, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002655, completion_tokens_cost_usd_dollar: 4.5029999999999994e-05\nDEBUG: response_cost: 0.00031053\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00031053\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002655, completion_tokens_cost_usd_dollar: 4.5029999999999994e-05\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031053\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00031053\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.007562, "msecs": 7.0, "relativeCreated": 7606.765985488892, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.0078888, "msecs": 7.0, "relativeCreated": 7607.09285736084, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488793.008002, "msecs": 8.0, "relativeCreated": 7607.206106185913, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.008186, "msecs": 8.0, "relativeCreated": 7607.3901653289795, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488793.008245, "msecs": 8.0, "relativeCreated": 7607.449054718018, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.0083158, "msecs": 8.0, "relativeCreated": 7607.519865036011, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002655, completion_tokens_cost_usd_dollar: 4.5029999999999994e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488793.008367, "msecs": 8.0, "relativeCreated": 7607.571125030518, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031053", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488793.008427, "msecs": 8.0, "relativeCreated": 7607.630968093872, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00031053", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488793.008471, "msecs": 8.0, "relativeCreated": 7607.675075531006, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.008585, "msecs": 8.0, "relativeCreated": 7607.789039611816, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488793.008644, "msecs": 8.0, "relativeCreated": 7607.848167419434, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-54", "asctime": "22:36:33"}]}}, {"nodeid": "tests/test_only_json.py::test_cyclic_relationships", "lineno": 113, "outcome": "failed", "keywords": ["test_cyclic_relationships", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.0006490419909823686, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488793.009869, "msecs": 9.0, "relativeCreated": 7609.073162078857, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.013095666974550113, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_json.py", "lineno": 131, "message": ""}, {"path": "src/elevate/only_json.py", "lineno": 62, "message": "in parse"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], 'thinking': None}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.012394, "msecs": 12.0, "relativeCreated": 7611.598014831543, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.012589, "msecs": 12.0, "relativeCreated": 7611.793041229248, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.012655, "msecs": 12.0, "relativeCreated": 7611.859083175659, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.012707, "msecs": 12.0, "relativeCreated": 7611.911058425903, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.012894, "msecs": 12.0, "relativeCreated": 7612.097978591919, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.012955, "msecs": 12.0, "relativeCreated": 7612.159013748169, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488793.013006, "msecs": 13.0, "relativeCreated": 7612.210035324097, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488793.0137348, "msecs": 13.0, "relativeCreated": 7612.93888092041, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488793.013845, "msecs": 13.0, "relativeCreated": 7613.049030303955, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488793.013908, "msecs": 13.0, "relativeCreated": 7613.111972808838, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.013998, "msecs": 13.0, "relativeCreated": 7613.202095031738, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.0141098, "msecs": 14.0, "relativeCreated": 7613.313913345337, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'$defs': {'Employee': {'description': 'Represents an employee. The manager field references another Employee object (or None).', 'properties': {'name': {'description': \"Employee's full name in string format.\", 'title': 'Name', 'type': 'string'}, 'manager': {'anyOf': [{'$ref': '#/$defs/Employee'}, {'type': 'null'}], 'default': None, 'description': 'Reference to another Employee object acting as the manager, or null if none.'}}, 'required': ['name'], 'title': 'Employee', 'type': 'object'}}, '$ref': '#/$defs/Employee', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488793.0142982, "msecs": 14.0, "relativeCreated": 7613.502264022827, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.0154939, "msecs": 15.0, "relativeCreated": 7614.6979331970215, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.015938, "msecs": 15.0, "relativeCreated": 7615.142107009888, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.015994, "msecs": 15.0, "relativeCreated": 7615.198135375977, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.016196, "msecs": 16.0, "relativeCreated": 7615.400075912476, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.016236, "msecs": 16.0, "relativeCreated": 7615.440130233765, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.0163672, "msecs": 16.0, "relativeCreated": 7615.5712604522705, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.016408, "msecs": 16.0, "relativeCreated": 7615.612030029297, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.016562, "msecs": 16.0, "relativeCreated": 7615.7660484313965, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488793.016649, "msecs": 16.0, "relativeCreated": 7615.853071212769, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488793.016994, "msecs": 16.0, "relativeCreated": 7616.1980628967285, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488793.019788, "msecs": 19.0, "relativeCreated": 7618.99209022522, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-56", "asctime": "22:36:33"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-0b6741f7-64e1-42db-ba4f-500efd524a7e', created=1749488793, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d4ef0>, stream = False\ndata = {'messages': [{'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.', 'role': 'user...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...ll', 'parameters': {'$defs': {'Employee': {...}}, '$ref': '#/$defs/Employee', 'type': 'object'}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = None, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None\nresponse_format = {'json_schema': {'name': 'Employee', 'schema': {'$defs': {'Employee': {'description': 'Represents an employee. The man...required': ['name'], 'title': 'Employee', ...}}, '$ref': '#/$defs/Employee', 'type': 'object'}}, 'type': 'json_schema'}\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '6aaf58ce-0b2f-4258-9448-266f303e4c25', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d4ef0>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-0b6741f7-64e1-42db-ba4f-500efd524a7e', created=1749488793, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d4ef0>, stream = False\ndata = {'messages': [{'content': 'Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.', 'role': 'user...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...ll', 'parameters': {'$defs': {'Employee': {...}}, '$ref': '#/$defs/Employee', 'type': 'object'}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_cyclic_relationships(settings: Any) -> None:\n        \"\"\"Test parsing data where an Employee may reference another Employee as a manager.\"\"\"\n    \n        class Employee(BaseModel):\n            \"\"\"Represents an employee. The manager field references another Employee object (or None).\"\"\"\n    \n            name: str = Field(..., description=\"Employee's full name in string format.\")\n            manager: Optional[\"Employee\"] = Field(\n                None,\n                description=\"Reference to another Employee object acting as the manager, or null if none.\",\n            )\n            model_config = ConfigDict(arbitrary_types_allowed=True)\n    \n        text = \"Employee: Jane Smith. Manager: John Wilson. Manager of John Wilson is none.\"\n    \n        only_json = OnlyJson(with_model=settings.with_model)\n>       employee = await only_json.parse(content=text, schema=Employee)\n\ntests/test_only_json.py:131: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_json.py:62: in parse\n    resp = await acompletion(model=self.model_id, messages=messages, response_format=json_schema)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '6aaf58ce-0b2f-4258-9448-266f303e4c25', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e3d4ef0>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003429999924264848, "outcome": "passed"}}, {"nodeid": "tests/test_only_json.py::test_conversion_while_extracting", "lineno": 138, "outcome": "passed", "keywords": ["test_conversion_while_extracting", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.0005384169926401228, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488793.182867, "msecs": 182.0, "relativeCreated": 7782.071113586426, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.21031049999874085, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c3ec0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c3da0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'991'), (b'x-ratelimit-remaining-tokens', b'8783'), (b'x-ratelimit-reset-requests', b'12m57.407999999s'), (b'x-ratelimit-reset-tokens', b'16.083s'), (b'x-request-id', b'req_01jxatgxpzehrb2j95g63yy96q'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2385e0a964901-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-9f1b97e5-7407-481e-8e84-6514ea8351f9\", \"object\": \"chat.completion\", \"created\": 1749488793, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_8t5h\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"city\\\":\\\"Berlin\\\",\\\"temperature_celsius\\\":30}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049948982999999995, \"prompt_tokens\": 311, \"prompt_time\": 0.028690368, \"completion_tokens\": 17, \"completion_time\": 0.080045936, \"total_tokens\": 328, \"total_time\": 0.108736304}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgxpzehrb2j95g63yy96q\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00018349, completion_tokens_cost_usd_dollar: 1.3429999999999998e-05\nDEBUG: response_cost: 0.00019692\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c3560>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], 'thinking': None}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-9f1b97e5-7407-481e-8e84-6514ea8351f9\", \"object\": \"chat.completion\", \"created\": 1749488793, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_8t5h\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"city\\\":\\\"Berlin\\\",\\\"temperature_celsius\\\":30}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049948982999999995, \"prompt_tokens\": 311, \"prompt_time\": 0.028690368, \"completion_tokens\": 17, \"completion_time\": 0.080045936, \"total_tokens\": 328, \"total_time\": 0.108736304}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgxpzehrb2j95g63yy96q\"}}\n\n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00018349, completion_tokens_cost_usd_dollar: 1.3429999999999998e-05\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00019692\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c3560>>\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.183767, "msecs": 183.0, "relativeCreated": 7782.971143722534, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.183817, "msecs": 183.0, "relativeCreated": 7783.020973205566, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.183838, "msecs": 183.0, "relativeCreated": 7783.041954040527, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.183857, "msecs": 183.0, "relativeCreated": 7783.0610275268555, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.183923, "msecs": 183.0, "relativeCreated": 7783.127069473267, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.1839428, "msecs": 183.0, "relativeCreated": 7783.146858215332, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488793.18396, "msecs": 183.0, "relativeCreated": 7783.164024353027, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488793.18418, "msecs": 184.0, "relativeCreated": 7783.384084701538, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488793.1842382, "msecs": 184.0, "relativeCreated": 7783.442258834839, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'TemperatureReading', 'schema': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488793.184269, "msecs": 184.0, "relativeCreated": 7783.473014831543, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.1843, "msecs": 184.0, "relativeCreated": 7783.504009246826, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.184335, "msecs": 184.0, "relativeCreated": 7783.539056777954, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'The temperature in Berlin is 86 degrees Fahrenheit today.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A temperature reading in Celsius for a specific city, possibly converted from Fahrenheit.', 'properties': {'city': {'description': 'City name in string format.', 'title': 'City', 'type': 'string'}, 'temperature_celsius': {'description': 'Temperature in Celsius (float). Convert from Fahrenheit if needed.', 'title': 'Temperature Celsius', 'type': 'number'}}, 'required': ['city', 'temperature_celsius'], 'title': 'TemperatureReading', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488793.184384, "msecs": 184.0, "relativeCreated": 7783.588171005249, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.184661, "msecs": 184.0, "relativeCreated": 7783.864974975586, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c3ec0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.194061, "msecs": 194.0, "relativeCreated": 7793.265104293823, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.194096, "msecs": 194.0, "relativeCreated": 7793.300151824951, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c3da0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.208807, "msecs": 208.0, "relativeCreated": 7808.011054992676, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.208916, "msecs": 208.0, "relativeCreated": 7808.120012283325, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.209025, "msecs": 209.0, "relativeCreated": 7808.228969573975, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.209045, "msecs": 209.0, "relativeCreated": 7808.248996734619, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.209117, "msecs": 209.0, "relativeCreated": 7808.320999145508, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.2091331, "msecs": 209.0, "relativeCreated": 7808.337211608887, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'991'), (b'x-ratelimit-remaining-tokens', b'8783'), (b'x-ratelimit-reset-requests', b'12m57.407999999s'), (b'x-ratelimit-reset-tokens', b'16.083s'), (b'x-request-id', b'req_01jxatgxpzehrb2j95g63yy96q'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2385e0a964901-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.3887758, "msecs": 388.0, "relativeCreated": 7987.979888916016, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488793.3893619, "msecs": 389.0, "relativeCreated": 7988.565921783447, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.389581, "msecs": 389.0, "relativeCreated": 7988.785028457642, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.3898501, "msecs": 389.0, "relativeCreated": 7989.054203033447, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.389971, "msecs": 389.0, "relativeCreated": 7989.175081253052, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.390101, "msecs": 390.0, "relativeCreated": 7989.305019378662, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488793.390322, "msecs": 390.0, "relativeCreated": 7989.526033401489, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-9f1b97e5-7407-481e-8e84-6514ea8351f9\", \"object\": \"chat.completion\", \"created\": 1749488793, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_8t5h\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"city\\\":\\\"Berlin\\\",\\\"temperature_celsius\\\":30}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049948982999999995, \"prompt_tokens\": 311, \"prompt_time\": 0.028690368, \"completion_tokens\": 17, \"completion_time\": 0.080045936, \"total_tokens\": 328, \"total_time\": 0.108736304}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgxpzehrb2j95g63yy96q\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.390636, "msecs": 390.0, "relativeCreated": 7989.840030670166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488793.392249, "msecs": 392.0, "relativeCreated": 7991.453170776367, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488793.392543, "msecs": 392.0, "relativeCreated": 7991.747140884399, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.39271, "msecs": 392.0, "relativeCreated": 7991.9140338897705, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00018349, completion_tokens_cost_usd_dollar: 1.3429999999999998e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488793.392779, "msecs": 392.0, "relativeCreated": 7991.98317527771, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "response_cost: 0.00019692", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488793.392857, "msecs": 392.0, "relativeCreated": 7992.061138153076, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-59", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c3560>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.3930109, "msecs": 393.0, "relativeCreated": 7992.214918136597, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-61", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488793.3931599, "msecs": 393.0, "relativeCreated": 7992.363929748535, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-61", "asctime": "22:36:33"}]}, "teardown": {"duration": 0.0024157910083886236, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00018349, completion_tokens_cost_usd_dollar: 1.3429999999999998e-05\nDEBUG: response_cost: 0.00019692\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00019692\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00018349, completion_tokens_cost_usd_dollar: 1.3429999999999998e-05\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00019692\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00019692\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.394135, "msecs": 394.0, "relativeCreated": 7993.339061737061, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.3945029, "msecs": 394.0, "relativeCreated": 7993.706941604614, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488793.394604, "msecs": 394.0, "relativeCreated": 7993.808031082153, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.394791, "msecs": 394.0, "relativeCreated": 7993.994951248169, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488793.394882, "msecs": 394.0, "relativeCreated": 7994.086027145386, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.394995, "msecs": 394.0, "relativeCreated": 7994.19903755188, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00018349, completion_tokens_cost_usd_dollar: 1.3429999999999998e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488793.395128, "msecs": 395.0, "relativeCreated": 7994.332075119019, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "response_cost: 0.00019692", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488793.3952599, "msecs": 395.0, "relativeCreated": 7994.463920593262, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00019692", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488793.395332, "msecs": 395.0, "relativeCreated": 7994.5361614227295, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488793.395519, "msecs": 395.0, "relativeCreated": 7994.723081588745, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488793.395613, "msecs": 395.0, "relativeCreated": 7994.817018508911, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-62", "asctime": "22:36:33"}]}}, {"nodeid": "tests/test_only_json.py::test_different_field_descriptions", "lineno": 159, "outcome": "failed", "keywords": ["test_different_field_descriptions", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.000598000013269484, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488793.397048, "msecs": 397.0, "relativeCreated": 7996.252059936523, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.012021125003229827, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_json.py", "lineno": 174, "message": ""}, {"path": "src/elevate/only_json.py", "lineno": 62, "message": "in parse"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], 'thinking': None}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.398834, "msecs": 398.0, "relativeCreated": 7998.038053512573, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.398958, "msecs": 398.0, "relativeCreated": 7998.162031173706, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.399008, "msecs": 399.0, "relativeCreated": 7998.212099075317, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.399056, "msecs": 399.0, "relativeCreated": 7998.260021209717, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.3992379, "msecs": 399.0, "relativeCreated": 7998.441934585571, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.399293, "msecs": 399.0, "relativeCreated": 7998.497009277344, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488793.3993442, "msecs": 399.0, "relativeCreated": 7998.548269271851, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488793.400307, "msecs": 400.0, "relativeCreated": 7999.511003494263, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488793.40039, "msecs": 400.0, "relativeCreated": 7999.59397315979, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488793.40045, "msecs": 400.0, "relativeCreated": 7999.654054641724, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.400532, "msecs": 400.0, "relativeCreated": 7999.736070632935, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.400617, "msecs": 400.0, "relativeCreated": 7999.820947647095, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Product details, including SKU and quantity.', 'properties': {'title': {'description': 'Name of the product in string format.', 'title': 'Title', 'type': 'string'}, 'sku': {'description': 'Stock Keeping Unit in string format (unique identifier).', 'title': 'Sku', 'type': 'string'}, 'quantity': {'description': 'Number of items in stock as an integer.', 'title': 'Quantity', 'type': 'integer'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488793.400775, "msecs": 400.0, "relativeCreated": 7999.979019165039, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.4017038, "msecs": 401.0, "relativeCreated": 8000.907897949219, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.402129, "msecs": 402.0, "relativeCreated": 8001.332998275757, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.402196, "msecs": 402.0, "relativeCreated": 8001.399993896484, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.402459, "msecs": 402.0, "relativeCreated": 8001.662969589233, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.402554, "msecs": 402.0, "relativeCreated": 8001.758098602295, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.402768, "msecs": 402.0, "relativeCreated": 8001.971960067749, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.402824, "msecs": 402.0, "relativeCreated": 8002.027988433838, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.402993, "msecs": 402.0, "relativeCreated": 8002.197027206421, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488793.4030871, "msecs": 403.0, "relativeCreated": 8002.291202545166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488793.403467, "msecs": 403.0, "relativeCreated": 8002.671003341675, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488793.406178, "msecs": 406.0, "relativeCreated": 8005.382061004639, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-64", "asctime": "22:36:33"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-3533e212-5826-4663-bee5-43a843b3f42f', created=1749488793, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c0ec0>, stream = False\ndata = {'messages': [{'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inve...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...ku': {...}, 'title': {...}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = None, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None\nresponse_format = {'json_schema': {'name': 'Product', 'schema': {'description': 'Product details, including SKU and quantity.', 'propert...Title', 'type': 'string'}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', ...}}, 'type': 'json_schema'}\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '21689740-dcb0-496d-8720-4ecf670736c2', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c0ec0>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-3533e212-5826-4663-bee5-43a843b3f42f', created=1749488793, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c0ec0>, stream = False\ndata = {'messages': [{'content': 'We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inve...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...ku': {...}, 'title': {...}}, 'required': ['title', 'sku', 'quantity'], 'title': 'Product', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_different_field_descriptions(settings: Any) -> None:\n        \"\"\"Test fields with custom descriptions and validations for a product.\"\"\"\n    \n        class Product(BaseModel):\n            \"\"\"Product details, including SKU and quantity.\"\"\"\n    \n            title: str = Field(..., description=\"Name of the product in string format.\")\n            sku: str = Field(..., description=\"Stock Keeping Unit in string format (unique identifier).\")\n            quantity: int = Field(..., description=\"Number of items in stock as an integer.\")\n    \n        text = \"We have a new product called UltraWidget. SKU: UW-001. We currently have 500 pieces in inventory.\"\n    \n        only_json = OnlyJson(with_model=settings.with_model)\n>       product = await only_json.parse(content=text, schema=Product)\n\ntests/test_only_json.py:174: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_json.py:62: in parse\n    resp = await acompletion(model=self.model_id, messages=messages, response_format=json_schema)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '21689740-dcb0-496d-8720-4ecf670736c2', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c0ec0>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003276659990660846, "outcome": "passed"}}, {"nodeid": "tests/test_only_json.py::test_data_formats", "lineno": 179, "outcome": "passed", "keywords": ["test_data_formats", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.00021158301387913525, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488793.5657978, "msecs": 565.0, "relativeCreated": 8165.00186920166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.46396279099280946, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c9f70>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5382c0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'990'), (b'x-ratelimit-remaining-tokens', b'8534'), (b'x-ratelimit-reset-requests', b'14m23.607s'), (b'x-ratelimit-reset-tokens', b'17.328999999s'), (b'x-request-id', b'req_01jxatgy39fv4rnf8cd5w8ge12'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2386078f040c9-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-6bee0401-70da-408a-86ed-5da2b47f06af\", \"object\": \"chat.completion\", \"created\": 1749488793, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_k328\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"name\\\":\\\"Fancy Pen\\\",\\\"price\\\":5.99,\\\"stock\\\":100}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049394308, \"prompt_tokens\": 340, \"prompt_time\": 0.050530913, \"completion_tokens\": 25, \"completion_time\": 0.090909091, \"total_tokens\": 365, \"total_time\": 0.141440004}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgy39fv4rnf8cd5w8ge12\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002006, completion_tokens_cost_usd_dollar: 1.975e-05\nDEBUG: response_cost: 0.00022035\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f53aae0>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:33 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], 'thinking': None}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:33 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-6bee0401-70da-408a-86ed-5da2b47f06af\", \"object\": \"chat.completion\", \"created\": 1749488793, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_k328\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"name\\\":\\\"Fancy Pen\\\",\\\"price\\\":5.99,\\\"stock\\\":100}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049394308, \"prompt_tokens\": 340, \"prompt_time\": 0.050530913, \"completion_tokens\": 25, \"completion_time\": 0.090909091, \"total_tokens\": 365, \"total_time\": 0.141440004}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgy39fv4rnf8cd5w8ge12\"}}\n\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002006, completion_tokens_cost_usd_dollar: 1.975e-05\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00022035\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f53aae0>>\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.5663722, "msecs": 566.0, "relativeCreated": 8165.576219558716, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.566419, "msecs": 566.0, "relativeCreated": 8165.62294960022, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.566438, "msecs": 566.0, "relativeCreated": 8165.642023086548, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.566456, "msecs": 566.0, "relativeCreated": 8165.66014289856, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.5665169, "msecs": 566.0, "relativeCreated": 8165.7209396362305, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.56654, "msecs": 566.0, "relativeCreated": 8165.744066238403, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488793.566558, "msecs": 566.0, "relativeCreated": 8165.761947631836, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488793.566795, "msecs": 566.0, "relativeCreated": 8165.999174118042, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488793.566854, "msecs": 566.0, "relativeCreated": 8166.05806350708, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'StoreItem', 'schema': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488793.566884, "msecs": 566.0, "relativeCreated": 8166.088104248047, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488793.566918, "msecs": 566.0, "relativeCreated": 8166.121959686279, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488793.566955, "msecs": 566.0, "relativeCreated": 8166.159152984619, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Item: Fancy Pen. Price: $5.99. Stock: 100 units available.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'Represents a store item with a name, price, and available stock.', 'properties': {'name': {'description': 'Item name in string format.', 'title': 'Name', 'type': 'string'}, 'price': {'description': 'Item price as a float (e.g., 5.99).', 'title': 'Price', 'type': 'number'}, 'stock': {'description': 'Number of items in stock as an integer.', 'title': 'Stock', 'type': 'integer'}}, 'required': ['name', 'price', 'stock'], 'title': 'StoreItem', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488793.567005, "msecs": 567.0, "relativeCreated": 8166.208982467651, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:33"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.567276, "msecs": 567.0, "relativeCreated": 8166.48006439209, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c9f70>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.576052, "msecs": 576.0, "relativeCreated": 8175.256013870239, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.576085, "msecs": 576.0, "relativeCreated": 8175.289154052734, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5382c0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.588394, "msecs": 588.0, "relativeCreated": 8187.597990036011, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.588484, "msecs": 588.0, "relativeCreated": 8187.688112258911, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.588581, "msecs": 588.0, "relativeCreated": 8187.7851486206055, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.5886002, "msecs": 588.0, "relativeCreated": 8187.804222106934, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.588665, "msecs": 588.0, "relativeCreated": 8187.869071960449, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488793.588681, "msecs": 588.0, "relativeCreated": 8187.885046005249, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:33 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'990'), (b'x-ratelimit-remaining-tokens', b'8534'), (b'x-ratelimit-reset-requests', b'14m23.607s'), (b'x-ratelimit-reset-tokens', b'17.328999999s'), (b'x-request-id', b'req_01jxatgy39fv4rnf8cd5w8ge12'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2386078f040c9-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.02417, "msecs": 24.0, "relativeCreated": 8623.373985290527, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488794.024908, "msecs": 24.0, "relativeCreated": 8624.112129211426, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.025255, "msecs": 25.0, "relativeCreated": 8624.459028244019, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.025562, "msecs": 25.0, "relativeCreated": 8624.766111373901, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.025677, "msecs": 25.0, "relativeCreated": 8624.881029129028, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.0258062, "msecs": 25.0, "relativeCreated": 8625.010251998901, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488794.0260322, "msecs": 26.0, "relativeCreated": 8625.23627281189, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-6bee0401-70da-408a-86ed-5da2b47f06af\", \"object\": \"chat.completion\", \"created\": 1749488793, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_k328\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"name\\\":\\\"Fancy Pen\\\",\\\"price\\\":5.99,\\\"stock\\\":100}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049394308, \"prompt_tokens\": 340, \"prompt_time\": 0.050530913, \"completion_tokens\": 25, \"completion_time\": 0.090909091, \"total_tokens\": 365, \"total_time\": 0.141440004}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxatgy39fv4rnf8cd5w8ge12\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.0263062, "msecs": 26.0, "relativeCreated": 8625.510215759277, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488794.028102, "msecs": 28.0, "relativeCreated": 8627.30598449707, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488794.0285819, "msecs": 28.0, "relativeCreated": 8627.785921096802, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.028826, "msecs": 28.0, "relativeCreated": 8628.030061721802, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002006, completion_tokens_cost_usd_dollar: 1.975e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488794.028932, "msecs": 28.0, "relativeCreated": 8628.136157989502, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "response_cost: 0.00022035", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488794.0290592, "msecs": 29.0, "relativeCreated": 8628.263235092163, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-67", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f53aae0>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.029269, "msecs": 29.0, "relativeCreated": 8628.473043441772, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-69", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488794.029393, "msecs": 29.0, "relativeCreated": 8628.597021102905, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-69", "asctime": "22:36:34"}]}, "teardown": {"duration": 0.0019366659980732948, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002006, completion_tokens_cost_usd_dollar: 1.975e-05\nDEBUG: response_cost: 0.00022035\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00022035\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002006, completion_tokens_cost_usd_dollar: 1.975e-05\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00022035\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00022035\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.030446, "msecs": 30.0, "relativeCreated": 8629.650115966797, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.030815, "msecs": 30.0, "relativeCreated": 8630.018949508667, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488794.030907, "msecs": 30.0, "relativeCreated": 8630.1109790802, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.031094, "msecs": 31.0, "relativeCreated": 8630.298137664795, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488794.031164, "msecs": 31.0, "relativeCreated": 8630.367994308472, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.031233, "msecs": 31.0, "relativeCreated": 8630.437135696411, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0002006, completion_tokens_cost_usd_dollar: 1.975e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488794.0312908, "msecs": 31.0, "relativeCreated": 8630.494832992554, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "response_cost: 0.00022035", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488794.03135, "msecs": 31.0, "relativeCreated": 8630.55396080017, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00022035", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488794.031392, "msecs": 31.0, "relativeCreated": 8630.596160888672, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.031538, "msecs": 31.0, "relativeCreated": 8630.742073059082, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488794.031598, "msecs": 31.0, "relativeCreated": 8630.802154541016, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-70", "asctime": "22:36:34"}]}}, {"nodeid": "tests/test_only_json.py::test_datetime_parsing", "lineno": 200, "outcome": "failed", "keywords": ["test_datetime_parsing", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.0007063749944791198, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488794.032929, "msecs": 32.0, "relativeCreated": 8632.13300704956, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.012944042013259605, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_json.py", "lineno": 214, "message": ""}, {"path": "src/elevate/only_json.py", "lineno": 62, "message": "in parse"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], 'thinking': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.034657, "msecs": 34.0, "relativeCreated": 8633.861064910889, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.034784, "msecs": 34.0, "relativeCreated": 8633.98814201355, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.0348318, "msecs": 34.0, "relativeCreated": 8634.03582572937, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.034875, "msecs": 34.0, "relativeCreated": 8634.078979492188, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.035056, "msecs": 35.0, "relativeCreated": 8634.260177612305, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.035112, "msecs": 35.0, "relativeCreated": 8634.315967559814, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488794.035161, "msecs": 35.0, "relativeCreated": 8634.36508178711, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488794.03614, "msecs": 36.0, "relativeCreated": 8635.3440284729, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488794.036287, "msecs": 36.0, "relativeCreated": 8635.491132736206, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488794.036361, "msecs": 36.0, "relativeCreated": 8635.565042495728, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.03647, "msecs": 36.0, "relativeCreated": 8635.673999786377, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.036574, "msecs": 36.0, "relativeCreated": 8635.777950286865, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A meeting that has a topic and a start_time in datetime format.', 'properties': {'topic': {'description': 'Topic of the meeting in string format.', 'title': 'Topic', 'type': 'string'}, 'start_time': {'description': \"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\", 'format': 'date-time', 'title': 'Start Time', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488794.0367281, "msecs": 36.0, "relativeCreated": 8635.932207107544, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.0377128, "msecs": 37.0, "relativeCreated": 8636.916875839233, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.038167, "msecs": 38.0, "relativeCreated": 8637.371063232422, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.038243, "msecs": 38.0, "relativeCreated": 8637.447118759155, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.0384672, "msecs": 38.0, "relativeCreated": 8637.67123222351, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.0385149, "msecs": 38.0, "relativeCreated": 8637.718915939331, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.038657, "msecs": 38.0, "relativeCreated": 8637.861013412476, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.038731, "msecs": 38.0, "relativeCreated": 8637.935161590576, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.03889, "msecs": 38.0, "relativeCreated": 8638.093948364258, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488794.038995, "msecs": 38.0, "relativeCreated": 8638.199090957642, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488794.039333, "msecs": 39.0, "relativeCreated": 8638.537168502808, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488794.042655, "msecs": 42.0, "relativeCreated": 8641.85905456543, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-72", "asctime": "22:36:34"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-650163c0-8b98-434a-b868-0baae42f33f8', created=1749488794, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f53b560>, stream = False\ndata = {'messages': [{'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.', 'role': 'user'}], 'model': 'l...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...rt_time': {...}, 'topic': {...}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.', 'role': 'user'}], functions = None\nfunction_call = None, timeout = None, temperature = None, top_p = None, n = None, stream = None, stream_options = None\nstop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None, audio = None\npresence_penalty = None, frequency_penalty = None, logit_bias = None, user = None\nresponse_format = {'json_schema': {'name': 'Meeting', 'schema': {'description': 'A meeting that has a topic and a start_time in datetime...e': 'Topic', 'type': 'string'}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', ...}}, 'type': 'json_schema'}\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'a318399a-2d56-400d-924e-ece2952907ef', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f53b560>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-650163c0-8b98-434a-b868-0baae42f33f8', created=1749488794, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f53b560>, stream = False\ndata = {'messages': [{'content': 'Meeting about budget planning on March 10, 2025 at 2:30 PM.', 'role': 'user'}], 'model': 'l...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ...rt_time': {...}, 'topic': {...}}, 'required': ['topic', 'start_time'], 'title': 'Meeting', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_datetime_parsing(settings: Any) -> None:\n        \"\"\"Test parsing a datetime field from unstructured text.\"\"\"\n    \n        class Meeting(BaseModel):\n            \"\"\"A meeting that has a topic and a start_time in datetime format.\"\"\"\n    \n            topic: str = Field(..., description=\"Topic of the meeting in string format.\")\n            start_time: datetime = Field(..., description=\"Date/time of the meeting (e.g., '2025-03-10 14:30:00').\")\n    \n        text = \"Meeting about budget planning on March 10, 2025 at 2:30 PM.\"\n    \n        only_json = OnlyJson(with_model=settings.with_model)\n>       meeting = await only_json.parse(content=text, schema=Meeting)\n\ntests/test_only_json.py:214: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_json.py:62: in parse\n    resp = await acompletion(model=self.model_id, messages=messages, response_format=json_schema)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'a318399a-2d56-400d-924e-ece2952907ef', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f53b560>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003220000071451068, "outcome": "passed"}}, {"nodeid": "tests/test_only_json.py::test_optional_fields", "lineno": 221, "outcome": "failed", "keywords": ["test_optional_fields", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.00020491701434366405, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488794.204149, "msecs": 204.0, "relativeCreated": 8803.353071212769, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.22213312500389293, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/tests/test_only_json.py", "lineno": 242, "message": "AssertionError: assert 'Loves coding in Python' == 'Loves coding in Python.'\n  \n  - Loves coding in Python.\n  ?                       -\n  + Loves coding in Python"}, "traceback": [{"path": "tests/test_only_json.py", "lineno": 242, "message": "AssertionError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f54db80>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5cae70>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'989'), (b'x-ratelimit-remaining-tokens', b'8291'), (b'x-ratelimit-reset-requests', b'15m49.774999999s'), (b'x-ratelimit-reset-tokens', b'18.545s'), (b'x-request-id', b'req_01jxatgyptfv4t3q00krh85cvf'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23864696fff6a-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-64cd4c91-aad2-4550-a2fe-dc4d960fc916\", \"object\": \"chat.completion\", \"created\": 1749488794, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_fkyt\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"bio\\\":\\\"Loves coding in Python\\\",\\\"username\\\":\\\"techguy\\\",\\\"website\\\":null}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049471717, \"prompt_tokens\": 362, \"prompt_time\": 0.024831476, \"completion_tokens\": 26, \"completion_time\": 0.094545455, \"total_tokens\": 388, \"total_time\": 0.119376931}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgyptfv4t3q00krh85cvf\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00021358, completion_tokens_cost_usd_dollar: 2.054e-05\nDEBUG: response_cost: 0.00023412\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c93d0>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], 'thinking': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-64cd4c91-aad2-4550-a2fe-dc4d960fc916\", \"object\": \"chat.completion\", \"created\": 1749488794, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_fkyt\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"bio\\\":\\\"Loves coding in Python\\\",\\\"username\\\":\\\"techguy\\\",\\\"website\\\":null}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049471717, \"prompt_tokens\": 362, \"prompt_time\": 0.024831476, \"completion_tokens\": 26, \"completion_time\": 0.094545455, \"total_tokens\": 388, \"total_time\": 0.119376931}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgyptfv4t3q00krh85cvf\"}}\n\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00021358, completion_tokens_cost_usd_dollar: 2.054e-05\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00023412\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c93d0>>\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.2048519, "msecs": 204.0, "relativeCreated": 8804.05592918396, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.2049038, "msecs": 204.0, "relativeCreated": 8804.107904434204, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], response_format={'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.204923, "msecs": 204.0, "relativeCreated": 8804.126977920532, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.20494, "msecs": 204.0, "relativeCreated": 8804.144144058228, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.204997, "msecs": 204.0, "relativeCreated": 8804.201126098633, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.2050161, "msecs": 205.0, "relativeCreated": 8804.220199584961, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488794.2050319, "msecs": 205.0, "relativeCreated": 8804.235935211182, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488794.205247, "msecs": 205.0, "relativeCreated": 8804.450988769531, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488794.205282, "msecs": 205.0, "relativeCreated": 8804.48603630066, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'Profile', 'schema': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488794.205328, "msecs": 205.0, "relativeCreated": 8804.532051086426, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.205384, "msecs": 205.0, "relativeCreated": 8804.588079452515, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.205455, "msecs": 205.0, "relativeCreated": 8804.659128189087, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': 'Username: techguy. Bio: Loves coding in Python. (No website provided).'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A user profile with a mandatory username and optional bio and website fields.', 'properties': {'username': {'description': 'Unique username in string format.', 'title': 'Username', 'type': 'string'}, 'bio': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': \"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\", 'title': 'Bio'}, 'website': {'anyOf': [{'type': 'string'}, {'type': 'null'}], 'default': None, 'description': 'Website URL in string format (optional).', 'title': 'Website'}}, 'required': ['username'], 'title': 'Profile', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488794.205515, "msecs": 205.0, "relativeCreated": 8804.718971252441, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.2057831, "msecs": 205.0, "relativeCreated": 8804.98719215393, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f54db80>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.214195, "msecs": 214.0, "relativeCreated": 8813.399076461792, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.214228, "msecs": 214.0, "relativeCreated": 8813.431978225708, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5cae70>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.224073, "msecs": 224.0, "relativeCreated": 8823.276996612549, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.224162, "msecs": 224.0, "relativeCreated": 8823.366165161133, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.224261, "msecs": 224.0, "relativeCreated": 8823.46510887146, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.22428, "msecs": 224.0, "relativeCreated": 8823.484182357788, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.224343, "msecs": 224.0, "relativeCreated": 8823.54712486267, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.2243578, "msecs": 224.0, "relativeCreated": 8823.561906814575, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'989'), (b'x-ratelimit-remaining-tokens', b'8291'), (b'x-ratelimit-reset-requests', b'15m49.774999999s'), (b'x-ratelimit-reset-tokens', b'18.545s'), (b'x-request-id', b'req_01jxatgyptfv4t3q00krh85cvf'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23864696fff6a-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.420198, "msecs": 420.0, "relativeCreated": 9019.402027130127, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488794.420695, "msecs": 420.0, "relativeCreated": 9019.899129867554, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.4208949, "msecs": 420.0, "relativeCreated": 9020.09892463684, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.421192, "msecs": 421.0, "relativeCreated": 9020.395994186401, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.4213471, "msecs": 421.0, "relativeCreated": 9020.551204681396, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.4215071, "msecs": 421.0, "relativeCreated": 9020.711183547974, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488794.4217489, "msecs": 421.0, "relativeCreated": 9020.952939987183, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-64cd4c91-aad2-4550-a2fe-dc4d960fc916\", \"object\": \"chat.completion\", \"created\": 1749488794, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_fkyt\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"bio\\\":\\\"Loves coding in Python\\\",\\\"username\\\":\\\"techguy\\\",\\\"website\\\":null}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049471717, \"prompt_tokens\": 362, \"prompt_time\": 0.024831476, \"completion_tokens\": 26, \"completion_time\": 0.094545455, \"total_tokens\": 388, \"total_time\": 0.119376931}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgyptfv4t3q00krh85cvf\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.422026, "msecs": 422.0, "relativeCreated": 9021.229982376099, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488794.424735, "msecs": 424.0, "relativeCreated": 9023.93913269043, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488794.425028, "msecs": 425.0, "relativeCreated": 9024.232149124146, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.425181, "msecs": 425.0, "relativeCreated": 9024.38497543335, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00021358, completion_tokens_cost_usd_dollar: 2.054e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488794.425258, "msecs": 425.0, "relativeCreated": 9024.4619846344, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "response_cost: 0.00023412", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488794.425334, "msecs": 425.0, "relativeCreated": 9024.538040161133, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-75", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c93d0>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.4258811, "msecs": 425.0, "relativeCreated": 9025.08521080017, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-77", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488794.425945, "msecs": 425.0, "relativeCreated": 9025.14910697937, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-77", "asctime": "22:36:34"}], "longrepr": "settings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_optional_fields(settings: Any) -> None:\n        \"\"\"Test a schema with optional fields, ensuring that missing data is handled gracefully.\"\"\"\n    \n        class Profile(BaseModel):\n            \"\"\"A user profile with a mandatory username and optional bio and website fields.\"\"\"\n    \n            username: str = Field(..., description=\"Unique username in string format.\")\n            bio: str | None = Field(\n                None,\n                description=\"Short bio in string format (optional). For example: 'Love to hike in the Alps.'\",\n            )\n            website: str | None = Field(None, description=\"Website URL in string format (optional).\")\n    \n        text = \"Username: techguy. Bio: Loves coding in Python. (No website provided).\"\n    \n        only_json = OnlyJson(with_model=settings.with_model)\n        profile = await only_json.parse(content=text, schema=Profile)\n        assert isinstance(profile, Profile)\n        assert profile.username == \"techguy\"\n>       assert profile.bio == \"Loves coding in Python.\"\nE       AssertionError: assert 'Loves coding in Python' == 'Loves coding in Python.'\nE         \nE         - Loves coding in Python.\nE         ?                       -\nE         + Loves coding in Python\n\ntests/test_only_json.py:242: AssertionError"}, "teardown": {"duration": 0.00200899998890236, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00021358, completion_tokens_cost_usd_dollar: 2.054e-05\nDEBUG: response_cost: 0.00023412\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00023412\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00021358, completion_tokens_cost_usd_dollar: 2.054e-05\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00023412\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00023412\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.4325361, "msecs": 432.0, "relativeCreated": 9031.740188598633, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.432985, "msecs": 432.0, "relativeCreated": 9032.189130783081, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488794.4331, "msecs": 433.0, "relativeCreated": 9032.304048538208, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.433269, "msecs": 433.0, "relativeCreated": 9032.473087310791, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488794.433344, "msecs": 433.0, "relativeCreated": 9032.547950744629, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.4334238, "msecs": 433.0, "relativeCreated": 9032.627820968628, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00021358, completion_tokens_cost_usd_dollar: 2.054e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488794.433474, "msecs": 433.0, "relativeCreated": 9032.678127288818, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "response_cost: 0.00023412", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488794.433541, "msecs": 433.0, "relativeCreated": 9032.745122909546, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00023412", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488794.43358, "msecs": 433.0, "relativeCreated": 9032.78398513794, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.4336839, "msecs": 433.0, "relativeCreated": 9032.887935638428, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488794.4337401, "msecs": 433.0, "relativeCreated": 9032.944202423096, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-78", "asctime": "22:36:34"}]}}, {"nodeid": "tests/test_only_json.py::test_special_characters_and_lists", "lineno": 245, "outcome": "failed", "keywords": ["test_special_characters_and_lists", "asyncio", "pytestmark", "test_only_json.py", "tests", "elevate", ""], "setup": {"duration": 0.000585292000323534, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488794.4349282, "msecs": 434.0, "relativeCreated": 9034.132242202759, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.010301832982804626, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_json.py", "lineno": 258, "message": ""}, {"path": "src/elevate/only_json.py", "lineno": 62, "message": "in parse"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], response_format={'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}})\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], response_format={'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}})\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], 'thinking': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.43646, "msecs": 436.0, "relativeCreated": 9035.664081573486, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.43655, "msecs": 436.0, "relativeCreated": 9035.753965377808, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], response_format={'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}})\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.436576, "msecs": 436.0, "relativeCreated": 9035.77995300293, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.436606, "msecs": 436.0, "relativeCreated": 9035.809993743896, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.43671, "msecs": 436.0, "relativeCreated": 9035.913944244385, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.436738, "msecs": 436.0, "relativeCreated": 9035.942077636719, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488794.4367628, "msecs": 436.0, "relativeCreated": 9035.966873168945, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488794.437291, "msecs": 437.0, "relativeCreated": 9036.494970321655, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': {'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488794.437438, "msecs": 437.0, "relativeCreated": 9036.642074584961, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488794.437511, "msecs": 437.0, "relativeCreated": 9036.715030670166, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.437691, "msecs": 437.0, "relativeCreated": 9036.895036697388, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.4378529, "msecs": 437.0, "relativeCreated": 9037.056922912598, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'user', 'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'description': 'A grocery list containing multiple items in a JSON array of strings.', 'properties': {'items': {'description': 'A list of grocery items; each item is a string.', 'items': {'type': 'string'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', 'type': 'object'}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488794.4380422, "msecs": 438.0, "relativeCreated": 9037.246227264404, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.438842, "msecs": 438.0, "relativeCreated": 9038.04612159729, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.439056, "msecs": 439.0, "relativeCreated": 9038.259983062744, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.439081, "msecs": 439.0, "relativeCreated": 9038.28501701355, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.439171, "msecs": 439.0, "relativeCreated": 9038.37513923645, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.439193, "msecs": 439.0, "relativeCreated": 9038.397073745728, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.4392571, "msecs": 439.0, "relativeCreated": 9038.461208343506, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.43928, "msecs": 439.0, "relativeCreated": 9038.4840965271, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.439379, "msecs": 439.0, "relativeCreated": 9038.583040237427, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488794.4394639, "msecs": 439.0, "relativeCreated": 9038.667917251587, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488794.439799, "msecs": 439.0, "relativeCreated": 9039.003133773804, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488794.442336, "msecs": 442.0, "relativeCreated": 9041.540145874023, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-80", "asctime": "22:36:34"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-b84a3011-ae3d-4a0a-bc50-842ea3b2e743', created=1749488794, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c9c10>, stream = False\ndata = {'messages': [{'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\", 'role...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ... strings.', 'properties': {'items': {...}}, 'required': ['items'], 'title': 'GroceryList', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\", 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = None, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None\nresponse_format = {'json_schema': {'name': 'GroceryList', 'schema': {'description': 'A grocery list containing multiple items in a JSON ...ng'}, 'title': 'Items', 'type': 'array'}}, 'required': ['items'], 'title': 'GroceryList', ...}}, 'type': 'json_schema'}\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'b2b1d7cb-3252-4e4d-9011-f909124fa6f8', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c9c10>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-b84a3011-ae3d-4a0a-bc50-842ea3b2e743', created=1749488794, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c9c10>, stream = False\ndata = {'messages': [{'content': \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\", 'role...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ... strings.', 'properties': {'items': {...}}, 'required': ['items'], 'title': 'GroceryList', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_special_characters_and_lists(settings: Any) -> None:\n        \"\"\"Test parsing a list of items that includes special characters or bullets.\"\"\"\n    \n        class GroceryList(BaseModel):\n            \"\"\"A grocery list containing multiple items in a JSON array of strings.\"\"\"\n    \n            items: list[str] = Field(..., description=\"A list of grocery items; each item is a string.\")\n    \n        text = \"Today's grocery list:\\n\u2022 Apples\\n\u2022 2% Milk\\n\u2022 Honey\\n\u2022 Eggs (a dozen)\\nEnd of list.\"\n    \n        only_json = OnlyJson(with_model=settings.with_model)\n>       grocery_list = await only_json.parse(content=text, schema=GroceryList)\n\ntests/test_only_json.py:258: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_json.py:62: in parse\n    resp = await acompletion(model=self.model_id, messages=messages, response_format=json_schema)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'b2b1d7cb-3252-4e4d-9011-f909124fa6f8', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c9c10>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.00032095800270326436, "outcome": "passed"}}, {"nodeid": "tests/test_only_judge_llms.py::test_summary_evaluation", "lineno": 30, "outcome": "passed", "keywords": ["test_summary_evaluation", "asyncio", "pytestmark", "test_only_judge_llms.py", "tests", "elevate", ""], "setup": {"duration": 0.00022879199241288006, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488794.600733, "msecs": 600.0, "relativeCreated": 9199.937105178833, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.27102058299351484, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], response_format=<class 'tests.test_only_judge_llms.test_summary_evaluation.<locals>.SummaryCriteria'>)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_summary_evaluation.<locals>.SummaryCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'SummaryCriteria', 'strict': True}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b2960>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b2810>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'988'), (b'x-ratelimit-remaining-tokens', b'7649'), (b'x-ratelimit-reset-requests', b'17m16.407s'), (b'x-ratelimit-reset-tokens', b'21.754s'), (b'x-request-id', b'req_01jxatgz35ehra0f81x9zwadhs'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23866dd55563e-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-588214d1-7d72-48ac-b26f-8edade78a461\", \"object\": \"chat.completion\", \"created\": 1749488794, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_jfsd\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"coherence\\\":4,\\\"factual_consistency\\\":5,\\\"fluency\\\":4}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.051844857999999994, \"prompt_tokens\": 603, \"prompt_time\": 0.073977502, \"completion_tokens\": 26, \"completion_time\": 0.094545455, \"total_tokens\": 629, \"total_time\": 0.168522957}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgz35ehra0f81x9zwadhs\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00035577, completion_tokens_cost_usd_dollar: 2.054e-05\nDEBUG: response_cost: 0.00037631\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b3c20>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], response_format=<class 'tests.test_only_judge_llms.test_summary_evaluation.<locals>.SummaryCriteria'>)\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_summary_evaluation.<locals>.SummaryCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], 'thinking': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'SummaryCriteria', 'strict': True}}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-588214d1-7d72-48ac-b26f-8edade78a461\", \"object\": \"chat.completion\", \"created\": 1749488794, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_jfsd\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"coherence\\\":4,\\\"factual_consistency\\\":5,\\\"fluency\\\":4}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.051844857999999994, \"prompt_tokens\": 603, \"prompt_time\": 0.073977502, \"completion_tokens\": 26, \"completion_time\": 0.094545455, \"total_tokens\": 629, \"total_time\": 0.168522957}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgz35ehra0f81x9zwadhs\"}}\n\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00035577, completion_tokens_cost_usd_dollar: 2.054e-05\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00037631\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b3c20>>\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.601197, "msecs": 601.0, "relativeCreated": 9200.401067733765, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.6012402, "msecs": 601.0, "relativeCreated": 9200.444221496582, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], response_format=<class 'tests.test_only_judge_llms.test_summary_evaluation.<locals>.SummaryCriteria'>)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.6012611, "msecs": 601.0, "relativeCreated": 9200.465202331543, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.601289, "msecs": 601.0, "relativeCreated": 9200.493097305298, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.601356, "msecs": 601.0, "relativeCreated": 9200.560092926025, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.601378, "msecs": 601.0, "relativeCreated": 9200.582027435303, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488794.601397, "msecs": 601.0, "relativeCreated": 9200.60110092163, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488794.6017358, "msecs": 601.0, "relativeCreated": 9200.939893722534, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_summary_evaluation.<locals>.SummaryCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488794.601793, "msecs": 601.0, "relativeCreated": 9200.997114181519, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'SummaryCriteria', 'strict': True}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488794.6018329, "msecs": 601.0, "relativeCreated": 9201.036930084229, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.60187, "msecs": 601.0, "relativeCreated": 9201.074123382568, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.601902, "msecs": 601.0, "relativeCreated": 9201.106071472168, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'The Q3 report indicates a 12% increase in revenue driven by new market strategies, cost-saving initiatives, and improved operational efficiency, though challenges remain in emerging markets.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'coherence': {'description': 'Coherence of the summary (1-5)', 'title': 'Coherence', 'type': 'integer'}, 'fluency': {'description': 'Fluency of the language (1-5)', 'title': 'Fluency', 'type': 'integer'}, 'factual_consistency': {'description': 'Factual consistency (1-5)', 'title': 'Factual Consistency', 'type': 'integer'}}, 'required': ['coherence', 'fluency', 'factual_consistency'], 'title': 'SummaryCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488794.6019552, "msecs": 601.0, "relativeCreated": 9201.159238815308, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.602286, "msecs": 602.0, "relativeCreated": 9201.4901638031, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b2960>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.610218, "msecs": 610.0, "relativeCreated": 9209.42211151123, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.610249, "msecs": 610.0, "relativeCreated": 9209.453105926514, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b2810>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.6235142, "msecs": 623.0, "relativeCreated": 9222.718238830566, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.6236138, "msecs": 623.0, "relativeCreated": 9222.81789779663, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.6237152, "msecs": 623.0, "relativeCreated": 9222.919225692749, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.623737, "msecs": 623.0, "relativeCreated": 9222.941160202026, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.623806, "msecs": 623.0, "relativeCreated": 9223.010063171387, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.623821, "msecs": 623.0, "relativeCreated": 9223.02508354187, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:34 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'988'), (b'x-ratelimit-remaining-tokens', b'7649'), (b'x-ratelimit-reset-requests', b'17m16.407s'), (b'x-ratelimit-reset-tokens', b'21.754s'), (b'x-request-id', b'req_01jxatgz35ehra0f81x9zwadhs'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23866dd55563e-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.86543, "msecs": 865.0, "relativeCreated": 9464.63418006897, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488794.865932, "msecs": 865.0, "relativeCreated": 9465.136051177979, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.8662689, "msecs": 866.0, "relativeCreated": 9465.472936630249, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.866747, "msecs": 866.0, "relativeCreated": 9465.950965881348, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.866949, "msecs": 866.0, "relativeCreated": 9466.153144836426, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.867209, "msecs": 867.0, "relativeCreated": 9466.413021087646, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488794.867611, "msecs": 867.0, "relativeCreated": 9466.814994812012, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-588214d1-7d72-48ac-b26f-8edade78a461\", \"object\": \"chat.completion\", \"created\": 1749488794, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_jfsd\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"coherence\\\":4,\\\"factual_consistency\\\":5,\\\"fluency\\\":4}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.051844857999999994, \"prompt_tokens\": 603, \"prompt_time\": 0.073977502, \"completion_tokens\": 26, \"completion_time\": 0.094545455, \"total_tokens\": 629, \"total_time\": 0.168522957}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgz35ehra0f81x9zwadhs\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.867939, "msecs": 867.0, "relativeCreated": 9467.143058776855, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488794.870444, "msecs": 870.0, "relativeCreated": 9469.648122787476, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488794.870841, "msecs": 870.0, "relativeCreated": 9470.04508972168, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.871017, "msecs": 871.0, "relativeCreated": 9470.221042633057, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00035577, completion_tokens_cost_usd_dollar: 2.054e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488794.871103, "msecs": 871.0, "relativeCreated": 9470.307111740112, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "response_cost: 0.00037631", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488794.871191, "msecs": 871.0, "relativeCreated": 9470.3950881958, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-83", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b3c20>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.871335, "msecs": 871.0, "relativeCreated": 9470.539093017578, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-85", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488794.871393, "msecs": 871.0, "relativeCreated": 9470.5970287323, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-85", "asctime": "22:36:34"}]}, "teardown": {"duration": 0.0020174589881207794, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00035577, completion_tokens_cost_usd_dollar: 2.054e-05\nDEBUG: response_cost: 0.00037631\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00037631\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00035577, completion_tokens_cost_usd_dollar: 2.054e-05\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00037631\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00037631\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.8724082, "msecs": 872.0, "relativeCreated": 9471.612215042114, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.8728168, "msecs": 872.0, "relativeCreated": 9472.020864486694, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488794.872946, "msecs": 872.0, "relativeCreated": 9472.150087356567, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.8731468, "msecs": 873.0, "relativeCreated": 9472.35083580017, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488794.873232, "msecs": 873.0, "relativeCreated": 9472.43595123291, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.8733199, "msecs": 873.0, "relativeCreated": 9472.523927688599, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00035577, completion_tokens_cost_usd_dollar: 2.054e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488794.873371, "msecs": 873.0, "relativeCreated": 9472.574949264526, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "response_cost: 0.00037631", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488794.87343, "msecs": 873.0, "relativeCreated": 9472.634077072144, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00037631", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488794.8734689, "msecs": 873.0, "relativeCreated": 9472.672939300537, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488794.8735812, "msecs": 873.0, "relativeCreated": 9472.785234451294, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488794.873642, "msecs": 873.0, "relativeCreated": 9472.846031188965, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-86", "asctime": "22:36:34"}]}}, {"nodeid": "tests/test_only_judge_llms.py::test_conversational_evaluation", "lineno": 52, "outcome": "failed", "keywords": ["test_conversational_evaluation", "asyncio", "pytestmark", "test_only_judge_llms.py", "tests", "elevate", ""], "setup": {"duration": 0.0006322499830275774, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488794.874955, "msecs": 874.0, "relativeCreated": 9474.159002304077, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.013171125028748065, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_judge_llms.py", "lineno": 66, "message": ""}, {"path": "src/elevate/only_judge_llms.py", "lineno": 100, "message": "in evaluate"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], response_format=<class 'tests.test_only_judge_llms.test_conversational_evaluation.<locals>.ConversationCriteria'>)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_conversational_evaluation.<locals>.ConversationCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'ConversationCriteria', 'strict': True}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], response_format=<class 'tests.test_only_judge_llms.test_conversational_evaluation.<locals>.ConversationCriteria'>)\u001b[0m\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:34 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_conversational_evaluation.<locals>.ConversationCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], 'thinking': None}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'ConversationCriteria', 'strict': True}}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:34 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.8763678, "msecs": 876.0, "relativeCreated": 9475.571870803833, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.876478, "msecs": 876.0, "relativeCreated": 9475.682020187378, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], response_format=<class 'tests.test_only_judge_llms.test_conversational_evaluation.<locals>.ConversationCriteria'>)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.87652, "msecs": 876.0, "relativeCreated": 9475.7239818573, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.876599, "msecs": 876.0, "relativeCreated": 9475.803136825562, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.876778, "msecs": 876.0, "relativeCreated": 9475.981950759888, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.876836, "msecs": 876.0, "relativeCreated": 9476.040124893188, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488794.876883, "msecs": 876.0, "relativeCreated": 9476.087093353271, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488794.878342, "msecs": 878.0, "relativeCreated": 9477.545976638794, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_conversational_evaluation.<locals>.ConversationCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488794.878518, "msecs": 878.0, "relativeCreated": 9477.72216796875, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'ConversationCriteria', 'strict': True}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488794.8786612, "msecs": 878.0, "relativeCreated": 9477.865219116211, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488794.8787572, "msecs": 878.0, "relativeCreated": 9477.961301803589, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488794.878869, "msecs": 878.0, "relativeCreated": 9478.073120117188, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. It's a beautiful day, though. How have you been?\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'relevance': {'description': 'How relevant is the reply? (1-5)', 'title': 'Relevance', 'type': 'integer'}, 'helpfulness': {'description': 'How helpful is the response? (1-5)', 'title': 'Helpfulness', 'type': 'integer'}, 'conciseness': {'description': 'How concise is the answer? (1-5)', 'title': 'Conciseness', 'type': 'integer'}}, 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488794.879039, "msecs": 879.0, "relativeCreated": 9478.243112564087, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:34"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.880094, "msecs": 880.0, "relativeCreated": 9479.298114776611, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.8805702, "msecs": 880.0, "relativeCreated": 9479.774236679077, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.880626, "msecs": 880.0, "relativeCreated": 9479.830026626587, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.880836, "msecs": 880.0, "relativeCreated": 9480.040073394775, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.88088, "msecs": 880.0, "relativeCreated": 9480.08418083191, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.8810182, "msecs": 881.0, "relativeCreated": 9480.222225189209, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.881068, "msecs": 881.0, "relativeCreated": 9480.272054672241, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488794.8812108, "msecs": 881.0, "relativeCreated": 9480.414867401123, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488794.8812969, "msecs": 881.0, "relativeCreated": 9480.500936508179, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488794.8816361, "msecs": 881.0, "relativeCreated": 9480.84020614624, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488794.884525, "msecs": 884.0, "relativeCreated": 9483.729124069214, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-88", "asctime": "22:36:34"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion mi...ave any details about the product launch right now. It's a beautiful day, though. How have you been?\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-584db42c-bbc5-4bbc-931c-8d87bf297bdd', created=1749488794, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b3d40>, stream = False\ndata = {'messages': [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ... 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion mi...ave any details about the product launch right now. It's a beautiful day, though. How have you been?\", 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = None, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None\nresponse_format = <class 'tests.test_only_judge_llms.test_conversational_evaluation.<locals>.ConversationCriteria'>\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '8d05716b-db07-47a2-87af-fdeac9c0b53e', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b3d40>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion mi...ave any details about the product launch right now. It's a beautiful day, though. How have you been?\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-584db42c-bbc5-4bbc-931c-8d87bf297bdd', created=1749488794, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b3d40>, stream = False\ndata = {'messages': [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': ... 'required': ['relevance', 'helpfulness', 'conciseness'], 'title': 'ConversationCriteria', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_conversational_evaluation(settings: Any) -> None:\n        class ConversationCriteria(BaseModel):\n            relevance: int = Field(..., description=\"How relevant is the reply? (1-5)\")\n            helpfulness: int = Field(..., description=\"How helpful is the response? (1-5)\")\n            conciseness: int = Field(..., description=\"How concise is the answer? (1-5)\")\n    \n        # This reply doesn't provide useful info about a product launch, so we expect lower scores for relevance/helpfulness.\n        sample_text = (\n            \"Thanks for reaching out. Honestly, I don't have any details about the product launch right now. \"\n            \"It's a beautiful day, though. How have you been?\"\n        )\n        judge = OnlyJudgeLLMs(with_model=settings.with_model)\n>       result: Any = await judge.evaluate(sample_text, ConversationCriteria)\n\ntests/test_only_judge_llms.py:66: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_judge_llms.py:100: in evaluate\n    response = await acompletion(\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '8d05716b-db07-47a2-87af-fdeac9c0b53e', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b3d40>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.00032579200342297554, "outcome": "passed"}}, {"nodeid": "tests/test_only_judge_llms.py::test_creative_writing_evaluation", "lineno": 77, "outcome": "passed", "keywords": ["test_creative_writing_evaluation", "asyncio", "pytestmark", "test_only_judge_llms.py", "tests", "elevate", ""], "setup": {"duration": 0.0002058749960269779, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.047081, "msecs": 47.0, "relativeCreated": 9646.285057067871, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.3100823749846313, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], response_format=<class 'tests.test_only_judge_llms.test_creative_writing_evaluation.<locals>.CreativeCriteria'>)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_creative_writing_evaluation.<locals>.CreativeCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'CreativeCriteria', 'strict': True}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57f5f0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57f4a0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'987'), (b'x-ratelimit-remaining-tokens', b'7103'), (b'x-ratelimit-reset-requests', b'18m42.753999999s'), (b'x-ratelimit-reset-tokens', b'24.484s'), (b'x-request-id', b'req_01jxatgzh4fv4v0qtj56bt5k3k'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23869baab3ab1-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-fefa3273-6d30-444a-901d-1a695041d17b\", \"object\": \"chat.completion\", \"created\": 1749488795, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_3ezy\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"creativity\\\":4,\\\"imagery\\\":5,\\\"narrative_flow\\\":3}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.05005078600000001, \"prompt_tokens\": 613, \"prompt_time\": 0.048504395, \"completion_tokens\": 26, \"completion_time\": 0.101594492, \"total_tokens\": 639, \"total_time\": 0.150098887}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgzh4fv4v0qtj56bt5k3k\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036166999999999997, completion_tokens_cost_usd_dollar: 2.054e-05\nDEBUG: response_cost: 0.00038220999999999996\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57f110>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], response_format=<class 'tests.test_only_judge_llms.test_creative_writing_evaluation.<locals>.CreativeCriteria'>)\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_creative_writing_evaluation.<locals>.CreativeCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], 'thinking': None}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'CreativeCriteria', 'strict': True}}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-fefa3273-6d30-444a-901d-1a695041d17b\", \"object\": \"chat.completion\", \"created\": 1749488795, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_3ezy\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"creativity\\\":4,\\\"imagery\\\":5,\\\"narrative_flow\\\":3}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.05005078600000001, \"prompt_tokens\": 613, \"prompt_time\": 0.048504395, \"completion_tokens\": 26, \"completion_time\": 0.101594492, \"total_tokens\": 639, \"total_time\": 0.150098887}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgzh4fv4v0qtj56bt5k3k\"}}\n\n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036166999999999997, completion_tokens_cost_usd_dollar: 2.054e-05\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00038220999999999996\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57f110>>\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.047539, "msecs": 47.0, "relativeCreated": 9646.743059158325, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.047586, "msecs": 47.0, "relativeCreated": 9646.790027618408, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], response_format=<class 'tests.test_only_judge_llms.test_creative_writing_evaluation.<locals>.CreativeCriteria'>)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.047606, "msecs": 47.0, "relativeCreated": 9646.810054779053, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.047626, "msecs": 47.0, "relativeCreated": 9646.830081939697, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.047689, "msecs": 47.0, "relativeCreated": 9646.89302444458, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.0477078, "msecs": 47.0, "relativeCreated": 9646.91185951233, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488795.047725, "msecs": 47.0, "relativeCreated": 9646.929025650024, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488795.048047, "msecs": 48.0, "relativeCreated": 9647.25112915039, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_creative_writing_evaluation.<locals>.CreativeCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488795.0481129, "msecs": 48.0, "relativeCreated": 9647.316932678223, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'CreativeCriteria', 'strict': True}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488795.048171, "msecs": 48.0, "relativeCreated": 9647.375106811523, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.0482109, "msecs": 48.0, "relativeCreated": 9647.414922714233, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.048244, "msecs": 48.0, "relativeCreated": 9647.448062896729, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'Amidst the twilight of a fading day, the vibrant hues of an impressionist sky dissolve into whispers of lost dreams. The city pulses with poetic intensity, each corner revealing a story etched in light and shadow.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'creativity': {'description': 'How creative is the composition? (1-5)', 'title': 'Creativity', 'type': 'integer'}, 'narrative_flow': {'description': 'Flow of the narrative (1-5)', 'title': 'Narrative Flow', 'type': 'integer'}, 'imagery': {'description': 'Quality of imagery evoked (1-5)', 'title': 'Imagery', 'type': 'integer'}}, 'required': ['creativity', 'narrative_flow', 'imagery'], 'title': 'CreativeCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488795.048298, "msecs": 48.0, "relativeCreated": 9647.501945495605, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.048573, "msecs": 48.0, "relativeCreated": 9647.777080535889, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57f5f0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.056631, "msecs": 56.0, "relativeCreated": 9655.835151672363, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.056661, "msecs": 56.0, "relativeCreated": 9655.864953994751, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57f4a0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.068716, "msecs": 68.0, "relativeCreated": 9667.920112609863, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.068801, "msecs": 68.0, "relativeCreated": 9668.004989624023, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.068904, "msecs": 68.0, "relativeCreated": 9668.107986450195, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.068925, "msecs": 68.0, "relativeCreated": 9668.128967285156, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.069, "msecs": 69.0, "relativeCreated": 9668.204069137573, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.0690198, "msecs": 69.0, "relativeCreated": 9668.223857879639, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'987'), (b'x-ratelimit-remaining-tokens', b'7103'), (b'x-ratelimit-reset-requests', b'18m42.753999999s'), (b'x-ratelimit-reset-tokens', b'24.484s'), (b'x-request-id', b'req_01jxatgzh4fv4v0qtj56bt5k3k'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d23869baab3ab1-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.3541079, "msecs": 354.0, "relativeCreated": 9953.311920166016, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488795.3543448, "msecs": 354.0, "relativeCreated": 9953.548908233643, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.35443, "msecs": 354.0, "relativeCreated": 9953.634023666382, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.354552, "msecs": 354.0, "relativeCreated": 9953.756093978882, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.3545978, "msecs": 354.0, "relativeCreated": 9953.80187034607, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.35466, "msecs": 354.0, "relativeCreated": 9953.864097595215, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488795.354817, "msecs": 354.0, "relativeCreated": 9954.020977020264, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-fefa3273-6d30-444a-901d-1a695041d17b\", \"object\": \"chat.completion\", \"created\": 1749488795, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_3ezy\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"creativity\\\":4,\\\"imagery\\\":5,\\\"narrative_flow\\\":3}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.05005078600000001, \"prompt_tokens\": 613, \"prompt_time\": 0.048504395, \"completion_tokens\": 26, \"completion_time\": 0.101594492, \"total_tokens\": 639, \"total_time\": 0.150098887}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxatgzh4fv4v0qtj56bt5k3k\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.355018, "msecs": 355.0, "relativeCreated": 9954.221963882446, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488795.3565598, "msecs": 356.0, "relativeCreated": 9955.763816833496, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488795.356703, "msecs": 356.0, "relativeCreated": 9955.907106399536, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.3567822, "msecs": 356.0, "relativeCreated": 9955.986261367798, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036166999999999997, completion_tokens_cost_usd_dollar: 2.054e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488795.356818, "msecs": 356.0, "relativeCreated": 9956.022024154663, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "response_cost: 0.00038220999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488795.356858, "msecs": 356.0, "relativeCreated": 9956.062078475952, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-91", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57f110>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.356933, "msecs": 356.0, "relativeCreated": 9956.13718032837, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-93", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488795.356979, "msecs": 356.0, "relativeCreated": 9956.182956695557, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-93", "asctime": "22:36:35"}]}, "teardown": {"duration": 0.0014400829968508333, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036166999999999997, completion_tokens_cost_usd_dollar: 2.054e-05\nDEBUG: response_cost: 0.00038220999999999996\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00038220999999999996\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036166999999999997, completion_tokens_cost_usd_dollar: 2.054e-05\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00038220999999999996\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00038220999999999996\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.3576891, "msecs": 357.0, "relativeCreated": 9956.8932056427, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.357973, "msecs": 357.0, "relativeCreated": 9957.17716217041, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488795.358071, "msecs": 358.0, "relativeCreated": 9957.27515220642, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.358214, "msecs": 358.0, "relativeCreated": 9957.417964935303, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488795.358287, "msecs": 358.0, "relativeCreated": 9957.491159439087, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.358357, "msecs": 358.0, "relativeCreated": 9957.561016082764, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036166999999999997, completion_tokens_cost_usd_dollar: 2.054e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488795.358411, "msecs": 358.0, "relativeCreated": 9957.61513710022, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "response_cost: 0.00038220999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488795.358474, "msecs": 358.0, "relativeCreated": 9957.678079605103, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00038220999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488795.3585129, "msecs": 358.0, "relativeCreated": 9957.716941833496, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.358635, "msecs": 358.0, "relativeCreated": 9957.839012145996, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488795.358685, "msecs": 358.0, "relativeCreated": 9957.889080047607, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-94", "asctime": "22:36:35"}]}}, {"nodeid": "tests/test_only_judge_llms.py::test_instructional_evaluation", "lineno": 101, "outcome": "failed", "keywords": ["test_instructional_evaluation", "asyncio", "pytestmark", "test_only_judge_llms.py", "tests", "elevate", ""], "setup": {"duration": 0.00037320901174098253, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.359293, "msecs": 359.0, "relativeCreated": 9958.497047424316, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.007919958006823435, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_judge_llms.py", "lineno": 114, "message": ""}, {"path": "src/elevate/only_judge_llms.py", "lineno": 100, "message": "in evaluate"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], response_format=<class 'tests.test_only_judge_llms.test_instructional_evaluation.<locals>.InstructionCriteria'>)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_instructional_evaluation.<locals>.InstructionCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'InstructionCriteria', 'strict': True}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], response_format=<class 'tests.test_only_judge_llms.test_instructional_evaluation.<locals>.InstructionCriteria'>)\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_instructional_evaluation.<locals>.InstructionCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], 'thinking': None}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'InstructionCriteria', 'strict': True}}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.3602078, "msecs": 360.0, "relativeCreated": 9959.41185951233, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.3602839, "msecs": 360.0, "relativeCreated": 9959.487915039062, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], response_format=<class 'tests.test_only_judge_llms.test_instructional_evaluation.<locals>.InstructionCriteria'>)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.360327, "msecs": 360.0, "relativeCreated": 9959.53106880188, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.3603761, "msecs": 360.0, "relativeCreated": 9959.580183029175, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.360493, "msecs": 360.0, "relativeCreated": 9959.697008132935, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.3605409, "msecs": 360.0, "relativeCreated": 9959.744930267334, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488795.3605828, "msecs": 360.0, "relativeCreated": 9959.786891937256, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488795.361353, "msecs": 361.0, "relativeCreated": 9960.556983947754, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_instructional_evaluation.<locals>.InstructionCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488795.361428, "msecs": 361.0, "relativeCreated": 9960.63208580017, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'InstructionCriteria', 'strict': True}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488795.361474, "msecs": 361.0, "relativeCreated": 9960.678100585938, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.361514, "msecs": 361.0, "relativeCreated": 9960.718154907227, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.361554, "msecs": 361.0, "relativeCreated": 9960.757970809937, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'clarity': {'description': 'Clarity of the instructions (1-5)', 'title': 'Clarity', 'type': 'integer'}, 'accuracy': {'description': 'Accuracy of the details provided (1-5)', 'title': 'Accuracy', 'type': 'integer'}, 'completeness': {'description': 'Completeness of the explanation (1-5)', 'title': 'Completeness', 'type': 'integer'}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488795.361622, "msecs": 361.0, "relativeCreated": 9960.82615852356, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.362012, "msecs": 362.0, "relativeCreated": 9961.21597290039, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.36225, "msecs": 362.0, "relativeCreated": 9961.454153060913, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.3622882, "msecs": 362.0, "relativeCreated": 9961.49230003357, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.36241, "msecs": 362.0, "relativeCreated": 9961.61413192749, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.362443, "msecs": 362.0, "relativeCreated": 9961.647033691406, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.3625212, "msecs": 362.0, "relativeCreated": 9961.725234985352, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.362559, "msecs": 362.0, "relativeCreated": 9961.763143539429, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.362655, "msecs": 362.0, "relativeCreated": 9961.858987808228, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488795.362716, "msecs": 362.0, "relativeCreated": 9961.920022964478, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488795.3629818, "msecs": 362.0, "relativeCreated": 9962.185859680176, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488795.364938, "msecs": 364.0, "relativeCreated": 9964.142084121704, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-96", "asctime": "22:36:35"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion mi...nstall the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-b6d720b4-31cc-4e53-917f-f7a399f6b372', created=1749488795, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b1220>, stream = False\ndata = {'messages': [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': .....}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion mi...nstall the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\", 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = None, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None\nresponse_format = <class 'tests.test_only_judge_llms.test_instructional_evaluation.<locals>.InstructionCriteria'>\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '7516fb8f-d2f9-49f5-b353-19774ea22b3e', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b1220>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion mi...nstall the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\", 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-b6d720b4-31cc-4e53-917f-f7a399f6b372', created=1749488795, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b1220>, stream = False\ndata = {'messages': [{'content': 'You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each...-3.3-70b-versatile', 'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, ...}\nbase_model = None\noptional_params = {'stream': False, 'tool_choice': {'function': {'name': 'json_tool_call'}, 'type': 'function'}, 'tools': [{'function': .....}}, 'required': ['clarity', 'accuracy', 'completeness'], 'title': 'InstructionCriteria', ...}}, 'type': 'function'}]}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = True\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_instructional_evaluation(settings: Any) -> None:\n        class InstructionCriteria(BaseModel):\n            clarity: int = Field(..., description=\"Clarity of the instructions (1-5)\")\n            accuracy: int = Field(..., description=\"Accuracy of the details provided (1-5)\")\n            completeness: int = Field(..., description=\"Completeness of the explanation (1-5)\")\n    \n        # This is intentionally vague and incomplete for installation instructions.\n        sample_text = (\n            \"To install the package, first you open your terminal. Then do something with pip. I'm not entirely sure.\"\n        )\n        judge = OnlyJudgeLLMs(with_model=settings.with_model)\n>       result: Any = await judge.evaluate(sample_text, InstructionCriteria)\n\ntests/test_only_judge_llms.py:114: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_judge_llms.py:100: in evaluate\n    response = await acompletion(\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '7516fb8f-d2f9-49f5-b353-19774ea22b3e', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b1220>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003191670111846179, "outcome": "passed"}}, {"nodeid": "tests/test_only_judge_llms.py::test_poetic_evaluation", "lineno": 125, "outcome": "passed", "keywords": ["test_poetic_evaluation", "asyncio", "pytestmark", "test_only_judge_llms.py", "tests", "elevate", ""], "setup": {"duration": 0.00020412501180544496, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.520763, "msecs": 520.0, "relativeCreated": 10119.966983795166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.3529586670047138, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], response_format=<class 'tests.test_only_judge_llms.test_poetic_evaluation.<locals>.PoeticCriteria'>)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_poetic_evaluation.<locals>.PoeticCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'PoeticCriteria', 'strict': True}}}\nDEBUG: Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12810b170>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b5eb0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'985'), (b'x-ratelimit-remaining-tokens', b'5962'), (b'x-ratelimit-reset-requests', b'21m35.801s'), (b'x-ratelimit-reset-tokens', b'30.186s'), (b'x-request-id', b'req_01jxath00behrrjdhvxh82kk35'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2386cac7e6efe-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-db4085f5-f65e-4072-8275-b50a5f4b8c57\", \"object\": \"chat.completion\", \"created\": 1749488795, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_jpzt\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"elegance\\\":5,\\\"metaphorical_depth\\\":5,\\\"rhythm\\\":5}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049300540999999996, \"prompt_tokens\": 611, \"prompt_time\": 0.0623618, \"completion_tokens\": 27, \"completion_time\": 0.098181818, \"total_tokens\": 638, \"total_time\": 0.160543618}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath00behrrjdhvxh82kk35\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036049, completion_tokens_cost_usd_dollar: 2.1329999999999997e-05\nDEBUG: response_cost: 0.00038182\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b7f20>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], response_format=<class 'tests.test_only_judge_llms.test_poetic_evaluation.<locals>.PoeticCriteria'>)\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_poetic_evaluation.<locals>.PoeticCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], 'thinking': None}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'PoeticCriteria', 'strict': True}}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-db4085f5-f65e-4072-8275-b50a5f4b8c57\", \"object\": \"chat.completion\", \"created\": 1749488795, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_jpzt\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"elegance\\\":5,\\\"metaphorical_depth\\\":5,\\\"rhythm\\\":5}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049300540999999996, \"prompt_tokens\": 611, \"prompt_time\": 0.0623618, \"completion_tokens\": 27, \"completion_time\": 0.098181818, \"total_tokens\": 638, \"total_time\": 0.160543618}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath00behrrjdhvxh82kk35\"}}\n\n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036049, completion_tokens_cost_usd_dollar: 2.1329999999999997e-05\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00038182\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b7f20>>\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.521215, "msecs": 521.0, "relativeCreated": 10120.419025421143, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.521261, "msecs": 521.0, "relativeCreated": 10120.46504020691, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], response_format=<class 'tests.test_only_judge_llms.test_poetic_evaluation.<locals>.PoeticCriteria'>)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.5212889, "msecs": 521.0, "relativeCreated": 10120.492935180664, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.521313, "msecs": 521.0, "relativeCreated": 10120.517015457153, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.521379, "msecs": 521.0, "relativeCreated": 10120.583057403564, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.521399, "msecs": 521.0, "relativeCreated": 10120.603084564209, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488795.521416, "msecs": 521.0, "relativeCreated": 10120.620012283325, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488795.5217438, "msecs": 521.0, "relativeCreated": 10120.94783782959, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': <class 'tests.test_only_judge_llms.test_poetic_evaluation.<locals>.PoeticCriteria'>, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488795.521803, "msecs": 521.0, "relativeCreated": 10121.006965637207, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'response_format': {'type': 'json_schema', 'json_schema': {'schema': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}, 'name': 'PoeticCriteria', 'strict': True}}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488795.521845, "msecs": 521.0, "relativeCreated": 10121.049165725708, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.5218801, "msecs": 521.0, "relativeCreated": 10121.084213256836, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'json_mode': True, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.5219111, "msecs": 521.0, "relativeCreated": 10121.11520767212, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"You are an expert evaluator of LLM outputs. You have been given multiple criteria, and each criterion might use a different method of assessment (e.g., a numerical scale, a boolean check, a pass/fail judgment, or something else entirely).\\n\\nYour task is to:\\n1. Identify the type of rating/assessment required for each criterion as indicated by the schema.\\n2. Plan how you will judge each criterion based on the provided text.\\n3. Carefully analyze the text to assess how well it meets each criterion.\\n4. Assign the correct rating or answer for each criterion (e.g., if it's a numeric scale, choose a value within that range; if it's a boolean check, choose the appropriate true/false or pass/fail).\\n5. Provide a brief factual justification for each rating or assessment, using direct references or observations from the text.\\n\\nImportant:\\n- If a numeric scale is provided, use the full range realistically. Do not default to the highest or lowest score unless it is justified.\\n- Return only a valid JSON object that exactly matches the schema\u2014no extra commentary or text outside the JSON.\\n- Do not reveal your internal chain-of-thought; simply provide the final ratings and justifications.\\n\"}, {'role': 'user', 'content': 'In the gentle embrace of dusk, silver threads of moonlight weave ancient tales; the cadence of nature whispers secrets where dreams and reality converge into a poetic reverie.'}], 'tools': [{'type': 'function', 'function': {'name': 'json_tool_call', 'parameters': {'properties': {'elegance': {'description': 'Elegance of the phrasing (1-5)', 'title': 'Elegance', 'type': 'integer'}, 'metaphorical_depth': {'description': 'Depth of metaphorical expression (1-5)', 'title': 'Metaphorical Depth', 'type': 'integer'}, 'rhythm': {'description': 'Rhythmic quality of the verse (1-5)', 'title': 'Rhythm', 'type': 'integer'}}, 'required': ['elegance', 'metaphorical_depth', 'rhythm'], 'title': 'PoeticCriteria', 'type': 'object', 'additionalProperties': False}}}], 'tool_choice': {'type': 'function', 'function': {'name': 'json_tool_call'}}, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488795.521965, "msecs": 521.0, "relativeCreated": 10121.169090270996, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.5222552, "msecs": 522.0, "relativeCreated": 10121.459245681763, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12810b170>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.5296981, "msecs": 529.0, "relativeCreated": 10128.902196884155, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.529731, "msecs": 529.0, "relativeCreated": 10128.935098648071, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b5eb0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.553824, "msecs": 553.0, "relativeCreated": 10153.028011322021, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.553926, "msecs": 553.0, "relativeCreated": 10153.130054473877, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.5540302, "msecs": 554.0, "relativeCreated": 10153.234243392944, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.554048, "msecs": 554.0, "relativeCreated": 10153.252124786377, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.5541172, "msecs": 554.0, "relativeCreated": 10153.321266174316, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.554136, "msecs": 554.0, "relativeCreated": 10153.340101242065, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'985'), (b'x-ratelimit-remaining-tokens', b'5962'), (b'x-ratelimit-reset-requests', b'21m35.801s'), (b'x-ratelimit-reset-tokens', b'30.186s'), (b'x-request-id', b'req_01jxath00behrrjdhvxh82kk35'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2386cac7e6efe-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.86785, "msecs": 867.0, "relativeCreated": 10467.05412864685, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488795.868376, "msecs": 868.0, "relativeCreated": 10467.580080032349, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.868563, "msecs": 868.0, "relativeCreated": 10467.767000198364, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.868845, "msecs": 868.0, "relativeCreated": 10468.049049377441, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.8689542, "msecs": 868.0, "relativeCreated": 10468.15824508667, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.86908, "msecs": 869.0, "relativeCreated": 10468.284130096436, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488795.8692899, "msecs": 869.0, "relativeCreated": 10468.493938446045, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-db4085f5-f65e-4072-8275-b50a5f4b8c57\", \"object\": \"chat.completion\", \"created\": 1749488795, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"tool_calls\": [{\"id\": \"call_jpzt\", \"type\": \"function\", \"function\": {\"name\": \"json_tool_call\", \"arguments\": \"{\\\"elegance\\\":5,\\\"metaphorical_depth\\\":5,\\\"rhythm\\\":5}\"}}]}, \"logprobs\": null, \"finish_reason\": \"tool_calls\"}], \"usage\": {\"queue_time\": 0.049300540999999996, \"prompt_tokens\": 611, \"prompt_time\": 0.0623618, \"completion_tokens\": 27, \"completion_time\": 0.098181818, \"total_tokens\": 638, \"total_time\": 0.160543618}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath00behrrjdhvxh82kk35\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.869591, "msecs": 869.0, "relativeCreated": 10468.79506111145, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488795.87241, "msecs": 872.0, "relativeCreated": 10471.614122390747, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488795.8727858, "msecs": 872.0, "relativeCreated": 10471.989870071411, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.872975, "msecs": 872.0, "relativeCreated": 10472.179174423218, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036049, completion_tokens_cost_usd_dollar: 2.1329999999999997e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488795.873065, "msecs": 873.0, "relativeCreated": 10472.269058227539, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "response_cost: 0.00038182", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488795.873169, "msecs": 873.0, "relativeCreated": 10472.373008728027, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-99", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5b7f20>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.873323, "msecs": 873.0, "relativeCreated": 10472.527027130127, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-101", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488795.8733828, "msecs": 873.0, "relativeCreated": 10472.586870193481, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-101", "asctime": "22:36:35"}]}, "teardown": {"duration": 0.0018250829889439046, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036049, completion_tokens_cost_usd_dollar: 2.1329999999999997e-05\nDEBUG: response_cost: 0.00038182\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00038182\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036049, completion_tokens_cost_usd_dollar: 2.1329999999999997e-05\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00038182\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00038182\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.874335, "msecs": 874.0, "relativeCreated": 10473.539113998413, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.87462, "msecs": 874.0, "relativeCreated": 10473.82402420044, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488795.874717, "msecs": 874.0, "relativeCreated": 10473.921060562134, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.874885, "msecs": 874.0, "relativeCreated": 10474.0891456604, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488795.8749518, "msecs": 874.0, "relativeCreated": 10474.155902862549, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.8750238, "msecs": 875.0, "relativeCreated": 10474.227905273438, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00036049, completion_tokens_cost_usd_dollar: 2.1329999999999997e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488795.8750741, "msecs": 875.0, "relativeCreated": 10474.278211593628, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "response_cost: 0.00038182", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488795.8751318, "msecs": 875.0, "relativeCreated": 10474.33590888977, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00038182", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488795.8751712, "msecs": 875.0, "relativeCreated": 10474.375247955322, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488795.875283, "msecs": 875.0, "relativeCreated": 10474.48706626892, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488795.875341, "msecs": 875.0, "relativeCreated": 10474.545001983643, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-102", "asctime": "22:36:35"}]}}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_html_conversion", "lineno": 36, "outcome": "passed", "keywords": ["test_hastily_copied_html_conversion", "asyncio", "pytestmark", "test_only_markdown.py", "tests", "elevate", ""], "setup": {"duration": 0.0006626659887842834, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.876891, "msecs": 876.0, "relativeCreated": 10476.094961166382, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.0006509579834528267, "outcome": "passed", "stdout": "DEBUG: HTML Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x10f55e7a0>\n", "log": [{"name": "root", "msg": "HTML Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x10f55e7a0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_markdown.py", "filename": "test_only_markdown.py", "module": "test_only_markdown", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 52, "funcName": "test_hastily_copied_html_conversion", "created": 1749488795.8774972, "msecs": 877.0, "relativeCreated": 10476.701259613037, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-104"}]}, "teardown": {"duration": 0.00043091701809316874, "outcome": "passed"}}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_word_doc_conversion", "lineno": 54, "outcome": "passed", "keywords": ["test_hastily_copied_word_doc_conversion", "asyncio", "pytestmark", "test_only_markdown.py", "tests", "elevate", ""], "setup": {"duration": 0.0005792080191895366, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.8820748, "msecs": 882.0, "relativeCreated": 10481.278896331787, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.00033579199225641787, "outcome": "passed", "stdout": "DEBUG: Word Doc Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815e180>\n", "log": [{"name": "root", "msg": "Word Doc Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815e180>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_markdown.py", "filename": "test_only_markdown.py", "module": "test_only_markdown", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 70, "funcName": "test_hastily_copied_word_doc_conversion", "created": 1749488795.882638, "msecs": 882.0, "relativeCreated": 10481.842041015625, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-106"}]}, "teardown": {"duration": 0.00021491700317710638, "outcome": "passed"}}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_db_output_conversion", "lineno": 72, "outcome": "passed", "keywords": ["test_hastily_copied_db_output_conversion", "asyncio", "pytestmark", "test_only_markdown.py", "tests", "elevate", ""], "setup": {"duration": 0.00037487500230781734, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.883391, "msecs": 883.0, "relativeCreated": 10482.594966888428, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.00033266699756495655, "outcome": "passed", "stdout": "DEBUG: DB Output Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815dc40>\n", "log": [{"name": "root", "msg": "DB Output Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815dc40>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_markdown.py", "filename": "test_only_markdown.py", "module": "test_only_markdown", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 79, "funcName": "test_hastily_copied_db_output_conversion", "created": 1749488795.88381, "msecs": 883.0, "relativeCreated": 10483.014106750488, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-108"}]}, "teardown": {"duration": 0.0002079170080833137, "outcome": "passed"}}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_blog_post_conversion", "lineno": 81, "outcome": "passed", "keywords": ["test_hastily_copied_blog_post_conversion", "asyncio", "pytestmark", "test_only_markdown.py", "tests", "elevate", ""], "setup": {"duration": 0.00028670800384134054, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.8845398, "msecs": 884.0, "relativeCreated": 10483.743906021118, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.0002801249793265015, "outcome": "passed", "stdout": "DEBUG: Blog Post Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815db60>\n", "log": [{"name": "root", "msg": "Blog Post Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815db60>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_markdown.py", "filename": "test_only_markdown.py", "module": "test_only_markdown", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 94, "funcName": "test_hastily_copied_blog_post_conversion", "created": 1749488795.884866, "msecs": 884.0, "relativeCreated": 10484.07006263733, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-110"}]}, "teardown": {"duration": 0.00017437501810491085, "outcome": "passed"}}, {"nodeid": "tests/test_only_markdown.py::test_hastily_copied_complex_unformatted_conversion", "lineno": 96, "outcome": "passed", "keywords": ["test_hastily_copied_complex_unformatted_conversion", "asyncio", "pytestmark", "test_only_markdown.py", "tests", "elevate", ""], "setup": {"duration": 0.00031437500729225576, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.8855062, "msecs": 885.0, "relativeCreated": 10484.710216522217, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.00039570798981003463, "outcome": "passed", "stdout": "DEBUG: Complex Unformatted Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815cac0>\n", "log": [{"name": "root", "msg": "Complex Unformatted Copy-Paste Conversion:\n<coroutine object OnlyMarkdown.convert_to_markdown at 0x12815cac0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_markdown.py", "filename": "test_only_markdown.py", "module": "test_only_markdown", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 114, "funcName": "test_hastily_copied_complex_unformatted_conversion", "created": 1749488795.8858619, "msecs": 885.0, "relativeCreated": 10485.065937042236, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-112"}]}, "teardown": {"duration": 0.00033195799915120006, "outcome": "passed"}}, {"nodeid": "tests/test_only_python.py::test_generate_code_simple_function_generation", "lineno": 37, "outcome": "failed", "keywords": ["test_generate_code_simple_function_generation", "asyncio", "pytestmark", "test_only_python.py", "tests", "elevate", ""], "setup": {"duration": 0.00027462499565444887, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488795.886994, "msecs": 886.0, "relativeCreated": 10486.19794845581, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.009986334014683962, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_python.py", "lineno": 50, "message": ""}, {"path": "src/elevate/only_python.py", "lineno": 180, "message": "in generate_code"}, {"path": "src/elevate/only_python.py", "lineno": 56, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n******************** Generating Python Code ********************\n\nDEBUG: Generating the python code...\nDEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:35 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:35 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "root", "msg": "\n******************** Generating Python Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488795.887347, "msecs": 887.0, "relativeCreated": 10486.55104637146, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "root", "msg": "Generating the python code...", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 179, "funcName": "generate_code", "created": 1749488795.88742, "msecs": 887.0, "relativeCreated": 10486.624002456665, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.887521, "msecs": 887.0, "relativeCreated": 10486.725091934204, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.8875961, "msecs": 887.0, "relativeCreated": 10486.800193786621, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.8876412, "msecs": 887.0, "relativeCreated": 10486.845254898071, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.88769, "msecs": 887.0, "relativeCreated": 10486.894130706787, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.88786, "msecs": 887.0, "relativeCreated": 10487.064123153687, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.887916, "msecs": 887.0, "relativeCreated": 10487.120151519775, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488795.887964, "msecs": 887.0, "relativeCreated": 10487.168073654175, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488795.8887901, "msecs": 888.0, "relativeCreated": 10487.994194030762, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488795.8889852, "msecs": 888.0, "relativeCreated": 10488.189220428467, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488795.889177, "msecs": 889.0, "relativeCreated": 10488.381147384644, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488795.889342, "msecs": 889.0, "relativeCreated": 10488.546133041382, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488795.889518, "msecs": 889.0, "relativeCreated": 10488.722085952759, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488795.889814, "msecs": 889.0, "relativeCreated": 10489.017963409424, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:35"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.89077, "msecs": 890.0, "relativeCreated": 10489.974021911621, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.8910668, "msecs": 891.0, "relativeCreated": 10490.270853042603, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.89111, "msecs": 891.0, "relativeCreated": 10490.31400680542, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.891319, "msecs": 891.0, "relativeCreated": 10490.523099899292, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.891359, "msecs": 891.0, "relativeCreated": 10490.563154220581, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.891467, "msecs": 891.0, "relativeCreated": 10490.671157836914, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.8915179, "msecs": 891.0, "relativeCreated": 10490.721940994263, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488795.891847, "msecs": 891.0, "relativeCreated": 10491.050958633423, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488795.8919349, "msecs": 891.0, "relativeCreated": 10491.138935089111, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488795.892181, "msecs": 892.0, "relativeCreated": 10491.384983062744, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488795.8943958, "msecs": 894.0, "relativeCreated": 10493.599891662598, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-114", "asctime": "22:36:35"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr... that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-2e99c78d-823e-4801-b380-bfe53dcb07d8', created=1749488795, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128109be0>, stream = False\ndata = {'messages': [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on t...tFormat>str</OutputFormat>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr... that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'f73dd2b4-ec16-4ac3-a716-9ed6ffcc1807', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128109be0>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr... that adds two numbers and execute that function.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-2e99c78d-823e-4801-b380-bfe53dcb07d8', created=1749488795, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128109be0>, stream = False\ndata = {'messages': [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on t...tFormat>str</OutputFormat>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_generate_code_simple_function_generation(settings: Any) -> None:\n        \"\"\"\n        Test the generate_code method of the OnlyPython class with a simple function generation task.\n    \n        The task involves creating a function that adds two numbers and then executing that function.\n        The generated code is then printed to the console for verification.\n        \"\"\"\n        input_message = \"\"\"\n      Create a function that adds two numbers and execute that function.\n      \"\"\"\n        only_python = OnlyPython(with_model=settings.with_model)\n>       output = await only_python.generate_code(input_message, \"\", False, False)\n\ntests/test_only_python.py:50: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_python.py:180: in generate_code\n    code = await self.make_llm_call(system_prompt, message)\nsrc/elevate/only_python.py:56: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'f73dd2b4-ec16-4ac3-a716-9ed6ffcc1807', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128109be0>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003357499954290688, "outcome": "passed"}}, {"nodeid": "tests/test_only_python.py::test_api_call", "lineno": 53, "outcome": "failed", "keywords": ["test_api_call", "asyncio", "pytestmark", "test_only_python.py", "tests", "elevate", ""], "setup": {"duration": 0.00019749999046325684, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488796.0761302, "msecs": 76.0, "relativeCreated": 10675.334215164185, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 1.1302403330046218, "outcome": "failed", "crash": {"path": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py", "lineno": 356, "message": "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)"}, "traceback": [{"path": "tests/test_only_python.py", "lineno": 77, "message": ""}, {"path": "src/elevate/only_python.py", "lineno": 223, "message": "in generate_code"}, {"path": "../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/__init__.py", "lineno": 346, "message": "in loads"}, {"path": "../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py", "lineno": 338, "message": "in decode"}, {"path": "../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py", "lineno": 356, "message": "JSONDecodeError"}], "stdout": "DEBUG: \n******************** Generating Python Code ********************\n\nDEBUG: Generating the python code...\nDEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b0440>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57cad0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'984'), (b'x-ratelimit-remaining-tokens', b'4864'), (b'x-ratelimit-reset-requests', b'23m1.854999999s'), (b'x-ratelimit-reset-tokens', b'35.679s'), (b'x-request-id', b'req_01jxath0hcfpkrhaqbhqjn8jqz'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238701eda0029-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-444f288a-31e9-43b1-8e21-12f80084568c\", \"object\": \"chat.completion\", \"created\": 1749488796, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"<PipInstalls>\\npip install requests json\\n</PipInstalls>\\n<Imports>\\nimport requests\\nimport json\\n</Imports>\\n<CodeCompletion>\\ndef fetch_product_data(product_id: int, api_url: str = \\\"https://jsonplaceholder.typicode.com/posts/\\\") -> dict:\\n    # Construct the full API URL with the product ID\\n    full_api_url = f\\\"{api_url}{product_id}\\\"\\n    \\n    try:\\n        # Send a GET request to the API\\n        response = requests.get(full_api_url, timeout=5)\\n        \\n        # Check if the request was successful\\n        response.raise_for_status()\\n        \\n        # Parse the JSON response\\n        data = response.json()\\n        \\n        # Return the parsed data\\n        return data\\n    \\n    except requests.RequestException as e:\\n        # Handle any request exceptions\\n        print(f\\\"Error fetching data: {e}\\\")\\n        return None\\n\\n# Fetch data for product ID 2\\nproduct_id = 2\\nproduct_data = fetch_product_data(product_id)\\n\\n# Print the product data in JSON format\\nif product_data is not None:\\n    output = json.dumps(product_data, indent=2)\\n    print(output)\\n</CodeCompletion>\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050491194, \"prompt_tokens\": 859, \"prompt_time\": 0.066617206, \"completion_tokens\": 244, \"completion_time\": 0.887272727, \"total_tokens\": 1103, \"total_time\": 0.953889933}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath0hcfpkrhaqbhqjn8jqz\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00050681, completion_tokens_cost_usd_dollar: 0.00019276\nDEBUG: response_cost: 0.00069957\nDEBUG: \nGenerated Code + Installs:\n\n<PipInstalls>\npip install requests json\n</PipInstalls>\n<Imports>\nimport requests\nimport json\n</Imports>\n<CodeCompletion>\ndef fetch_product_data(product_id: int, api_url: str = \"https://jsonplaceholder.typicode.com/posts/\") -> dict:\n    # Construct the full API URL with the product ID\n    full_api_url = f\"{api_url}{product_id}\"\n    \n    try:\n        # Send a GET request to the API\n        response = requests.get(full_api_url, timeout=5)\n        \n        # Check if the request was successful\n        response.raise_for_status()\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Return the parsed data\n        return data\n    \n    except requests.RequestException as e:\n        # Handle any request exceptions\n        print(f\"Error fetching data: {e}\")\n        return None\n\n# Fetch data for product ID 2\nproduct_id = 2\nproduct_data = fetch_product_data(product_id)\n\n# Print the product data in JSON format\nif product_data is not None:\n    output = json.dumps(product_data, indent=2)\n    print(output)\n</CodeCompletion>\nDEBUG: \n******************** End of Generated Code ********************\n\nDEBUG: \n******************** Generated Code ********************\n\nDEBUG: ----------------------------------------\nDEBUG: Full code with imports and pip installs:\n\nDEBUG: ----------------------------------------\nDEBUG: pip install requests json\nDEBUG: ----------------------------------------\nDEBUG: # -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated imports\n\nimport requests\nimport json\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Original code.\n\n\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated Completion\n\ndef fetch_product_data(product_id: int, api_url: str = \"https://jsonplaceholder.typicode.com/posts/\") -> dict:\n    # Construct the full API URL with the product ID\n    full_api_url = f\"{api_url}{product_id}\"\n    \n    try:\n        # Send a GET request to the API\n        response = requests.get(full_api_url, timeout=5)\n        \n        # Check if the request was successful\n        response.raise_for_status()\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Return the parsed data\n        return data\n    \n    except requests.RequestException as e:\n        # Handle any request exceptions\n        print(f\"Error fetching data: {e}\")\n        return None\n\n# Fetch data for product ID 2\nproduct_id = 2\nproduct_data = fetch_product_data(product_id)\n\n# Print the product data in JSON format\nif product_data is not None:\n    output = json.dumps(product_data, indent=2)\n    print(output)\nDEBUG: ----------------------------------------\nDEBUG: \n******************** Executing Generated Code ********************\n\nERROR: Error during code execution\nTraceback (most recent call last):\n  File \" Developer/OpenSource/elevate/src/elevate/only_python.py\", line 84, in execute_code_using_e2b_sandbox\n    sandbox = await AsyncSandbox.create(envs=dict(os.environ))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/main.py\", line 208, in create\n    response = await SandboxApi._create_sandbox(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/sandbox_api.py\", line 235, in _create_sandbox\n    async with AsyncApiClient(config) as api_client:\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/api/__init__.py\", line 70, in __init__\n    raise AuthenticationException(\ne2b.exceptions.AuthenticationException: API key is required, please visit the Team tab at https://e2b.dev/dashboard to get your API key. You can either set the environment variable `E2B_API_KEY` or you can pass it directly to the sandbox like Sandbox(api_key=\"e2b_...\")\nDEBUG: \nOutput of Execution:\n\nDEBUG: Error during code execution.\nDEBUG: \n******************** End of Execution ********************\n\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57e810>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:36 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'thinking': None}\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:36 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-444f288a-31e9-43b1-8e21-12f80084568c\", \"object\": \"chat.completion\", \"created\": 1749488796, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"<PipInstalls>\\npip install requests json\\n</PipInstalls>\\n<Imports>\\nimport requests\\nimport json\\n</Imports>\\n<CodeCompletion>\\ndef fetch_product_data(product_id: int, api_url: str = \\\"https://jsonplaceholder.typicode.com/posts/\\\") -> dict:\\n    # Construct the full API URL with the product ID\\n    full_api_url = f\\\"{api_url}{product_id}\\\"\\n    \\n    try:\\n        # Send a GET request to the API\\n        response = requests.get(full_api_url, timeout=5)\\n        \\n        # Check if the request was successful\\n        response.raise_for_status()\\n        \\n        # Parse the JSON response\\n        data = response.json()\\n        \\n        # Return the parsed data\\n        return data\\n    \\n    except requests.RequestException as e:\\n        # Handle any request exceptions\\n        print(f\\\"Error fetching data: {e}\\\")\\n        return None\\n\\n# Fetch data for product ID 2\\nproduct_id = 2\\nproduct_data = fetch_product_data(product_id)\\n\\n# Print the product data in JSON format\\nif product_data is not None:\\n    output = json.dumps(product_data, indent=2)\\n    print(output)\\n</CodeCompletion>\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050491194, \"prompt_tokens\": 859, \"prompt_time\": 0.066617206, \"completion_tokens\": 244, \"completion_time\": 0.887272727, \"total_tokens\": 1103, \"total_time\": 0.953889933}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath0hcfpkrhaqbhqjn8jqz\"}}\n\n\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00050681, completion_tokens_cost_usd_dollar: 0.00019276\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00069957\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57e810>>\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "root", "msg": "\n******************** Generating Python Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488796.076334, "msecs": 76.0, "relativeCreated": 10675.538063049316, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "Generating the python code...", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 179, "funcName": "generate_code", "created": 1749488796.076362, "msecs": 76.0, "relativeCreated": 10675.565958023071, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488796.076397, "msecs": 76.0, "relativeCreated": 10675.6010055542, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488796.076424, "msecs": 76.0, "relativeCreated": 10675.627946853638, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488796.07644, "msecs": 76.0, "relativeCreated": 10675.644159317017, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488796.076457, "msecs": 76.0, "relativeCreated": 10675.661087036133, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488796.0765288, "msecs": 76.0, "relativeCreated": 10675.732851028442, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488796.07655, "msecs": 76.0, "relativeCreated": 10675.754070281982, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488796.07657, "msecs": 76.0, "relativeCreated": 10675.774097442627, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488796.076791, "msecs": 76.0, "relativeCreated": 10675.995111465454, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488796.076857, "msecs": 76.0, "relativeCreated": 10676.061153411865, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488796.0768929, "msecs": 76.0, "relativeCreated": 10676.09691619873, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488796.076921, "msecs": 76.0, "relativeCreated": 10676.125049591064, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488796.076949, "msecs": 76.0, "relativeCreated": 10676.15294456482, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:36"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Fetch data of product id 2 from the given API.\\n  API : https://jsonplaceholder.typicode.com/posts/2\\n  Sample response from the API:\\n  {\\n    \"userId\": 1,\\n    \"id\": 2,\\n    \"title\": \"qui est esse\",\\n    \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\\n  }\\n  \\n  </Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488796.077003, "msecs": 77.0, "relativeCreated": 10676.207065582275, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:36"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.077278, "msecs": 77.0, "relativeCreated": 10676.48196220398, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5b0440>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.085216, "msecs": 85.0, "relativeCreated": 10684.420108795166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.085242, "msecs": 85.0, "relativeCreated": 10684.446096420288, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57cad0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.0957088, "msecs": 95.0, "relativeCreated": 10694.912910461426, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.0958111, "msecs": 95.0, "relativeCreated": 10695.01519203186, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.095916, "msecs": 95.0, "relativeCreated": 10695.120096206665, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.095934, "msecs": 95.0, "relativeCreated": 10695.137977600098, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.096003, "msecs": 96.0, "relativeCreated": 10695.207118988037, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488796.096017, "msecs": 96.0, "relativeCreated": 10695.220947265625, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:37 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'984'), (b'x-ratelimit-remaining-tokens', b'4864'), (b'x-ratelimit-reset-requests', b'23m1.854999999s'), (b'x-ratelimit-reset-tokens', b'35.679s'), (b'x-request-id', b'req_01jxath0hcfpkrhaqbhqjn8jqz'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238701eda0029-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.1989279, "msecs": 198.0, "relativeCreated": 11798.131942749023, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488797.199432, "msecs": 199.0, "relativeCreated": 11798.635959625244, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.1996338, "msecs": 199.0, "relativeCreated": 11798.837900161743, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.1999302, "msecs": 199.0, "relativeCreated": 11799.134254455566, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.2000608, "msecs": 200.0, "relativeCreated": 11799.264907836914, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.200195, "msecs": 200.0, "relativeCreated": 11799.399137496948, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488797.2004328, "msecs": 200.0, "relativeCreated": 11799.636840820312, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-444f288a-31e9-43b1-8e21-12f80084568c\", \"object\": \"chat.completion\", \"created\": 1749488796, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"<PipInstalls>\\npip install requests json\\n</PipInstalls>\\n<Imports>\\nimport requests\\nimport json\\n</Imports>\\n<CodeCompletion>\\ndef fetch_product_data(product_id: int, api_url: str = \\\"https://jsonplaceholder.typicode.com/posts/\\\") -> dict:\\n    # Construct the full API URL with the product ID\\n    full_api_url = f\\\"{api_url}{product_id}\\\"\\n    \\n    try:\\n        # Send a GET request to the API\\n        response = requests.get(full_api_url, timeout=5)\\n        \\n        # Check if the request was successful\\n        response.raise_for_status()\\n        \\n        # Parse the JSON response\\n        data = response.json()\\n        \\n        # Return the parsed data\\n        return data\\n    \\n    except requests.RequestException as e:\\n        # Handle any request exceptions\\n        print(f\\\"Error fetching data: {e}\\\")\\n        return None\\n\\n# Fetch data for product ID 2\\nproduct_id = 2\\nproduct_data = fetch_product_data(product_id)\\n\\n# Print the product data in JSON format\\nif product_data is not None:\\n    output = json.dumps(product_data, indent=2)\\n    print(output)\\n</CodeCompletion>\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050491194, \"prompt_tokens\": 859, \"prompt_time\": 0.066617206, \"completion_tokens\": 244, \"completion_time\": 0.887272727, \"total_tokens\": 1103, \"total_time\": 0.953889933}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath0hcfpkrhaqbhqjn8jqz\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.200723, "msecs": 200.0, "relativeCreated": 11799.92699623108, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488797.201167, "msecs": 201.0, "relativeCreated": 11800.371170043945, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488797.201468, "msecs": 201.0, "relativeCreated": 11800.672054290771, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488797.201621, "msecs": 201.0, "relativeCreated": 11800.825119018555, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00050681, completion_tokens_cost_usd_dollar: 0.00019276", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488797.201699, "msecs": 201.0, "relativeCreated": 11800.90308189392, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "response_cost: 0.00069957", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488797.201772, "msecs": 201.0, "relativeCreated": 11800.976037979126, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117", "asctime": "22:36:37"}, {"name": "root", "msg": "\nGenerated Code + Installs:\n\n<PipInstalls>\npip install requests json\n</PipInstalls>\n<Imports>\nimport requests\nimport json\n</Imports>\n<CodeCompletion>\ndef fetch_product_data(product_id: int, api_url: str = \"https://jsonplaceholder.typicode.com/posts/\") -> dict:\n    # Construct the full API URL with the product ID\n    full_api_url = f\"{api_url}{product_id}\"\n    \n    try:\n        # Send a GET request to the API\n        response = requests.get(full_api_url, timeout=5)\n        \n        # Check if the request was successful\n        response.raise_for_status()\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Return the parsed data\n        return data\n    \n    except requests.RequestException as e:\n        # Handle any request exceptions\n        print(f\"Error fetching data: {e}\")\n        return None\n\n# Fetch data for product ID 2\nproduct_id = 2\nproduct_data = fetch_product_data(product_id)\n\n# Print the product data in JSON format\nif product_data is not None:\n    output = json.dumps(product_data, indent=2)\n    print(output)\n</CodeCompletion>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 181, "funcName": "generate_code", "created": 1749488797.202152, "msecs": 202.0, "relativeCreated": 11801.356077194214, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "\n******************** End of Generated Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488797.20222, "msecs": 202.0, "relativeCreated": 11801.424026489258, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "\n******************** Generated Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488797.2022521, "msecs": 202.0, "relativeCreated": 11801.456212997437, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 207, "funcName": "generate_code", "created": 1749488797.2022889, "msecs": 202.0, "relativeCreated": 11801.492929458618, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "Full code with imports and pip installs:\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 208, "funcName": "generate_code", "created": 1749488797.2023199, "msecs": 202.0, "relativeCreated": 11801.523923873901, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 209, "funcName": "generate_code", "created": 1749488797.202348, "msecs": 202.0, "relativeCreated": 11801.552057266235, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "pip install requests json", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "generate_code", "created": 1749488797.202379, "msecs": 202.0, "relativeCreated": 11801.583051681519, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 211, "funcName": "generate_code", "created": 1749488797.202413, "msecs": 202.0, "relativeCreated": 11801.61714553833, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated imports\n\nimport requests\nimport json\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Original code.\n\n\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated Completion\n\ndef fetch_product_data(product_id: int, api_url: str = \"https://jsonplaceholder.typicode.com/posts/\") -> dict:\n    # Construct the full API URL with the product ID\n    full_api_url = f\"{api_url}{product_id}\"\n    \n    try:\n        # Send a GET request to the API\n        response = requests.get(full_api_url, timeout=5)\n        \n        # Check if the request was successful\n        response.raise_for_status()\n        \n        # Parse the JSON response\n        data = response.json()\n        \n        # Return the parsed data\n        return data\n    \n    except requests.RequestException as e:\n        # Handle any request exceptions\n        print(f\"Error fetching data: {e}\")\n        return None\n\n# Fetch data for product ID 2\nproduct_id = 2\nproduct_data = fetch_product_data(product_id)\n\n# Print the product data in JSON format\nif product_data is not None:\n    output = json.dumps(product_data, indent=2)\n    print(output)", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 212, "funcName": "generate_code", "created": 1749488797.20245, "msecs": 202.0, "relativeCreated": 11801.65410041809, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 213, "funcName": "generate_code", "created": 1749488797.202488, "msecs": 202.0, "relativeCreated": 11801.692008972168, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "\n******************** Executing Generated Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488797.2025468, "msecs": 202.0, "relativeCreated": 11801.750898361206, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "Error during code execution", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": "Traceback (most recent call last):\n  File \" Developer/OpenSource/elevate/src/elevate/only_python.py\", line 84, in execute_code_using_e2b_sandbox\n    sandbox = await AsyncSandbox.create(envs=dict(os.environ))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/main.py\", line 208, in create\n    response = await SandboxApi._create_sandbox(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/sandbox_api.py\", line 235, in _create_sandbox\n    async with AsyncApiClient(config) as api_client:\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/api/__init__.py\", line 70, in __init__\n    raise AuthenticationException(\ne2b.exceptions.AuthenticationException: API key is required, please visit the Team tab at https://e2b.dev/dashboard to get your API key. You can either set the environment variable `E2B_API_KEY` or you can pass it directly to the sandbox like Sandbox(api_key=\"e2b_...\")", "stack_info": null, "lineno": 116, "funcName": "execute_code_using_e2b_sandbox", "created": 1749488797.2027788, "msecs": 202.0, "relativeCreated": 11801.982879638672, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "\nOutput of Execution:\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 218, "funcName": "generate_code", "created": 1749488797.2056031, "msecs": 205.0, "relativeCreated": 11804.807186126709, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "Error during code execution.", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 219, "funcName": "generate_code", "created": 1749488797.205664, "msecs": 205.0, "relativeCreated": 11804.86798286438, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "root", "msg": "\n******************** End of Execution ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488797.205706, "msecs": 205.0, "relativeCreated": 11804.909944534302, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-117"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57e810>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.205835, "msecs": 205.0, "relativeCreated": 11805.039167404175, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-119", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488797.205927, "msecs": 205.0, "relativeCreated": 11805.130958557129, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-119", "asctime": "22:36:37"}], "longrepr": "settings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_api_call(settings: Any) -> None:\n        \"\"\"\n        Test the generate_code method's ability to generate code that fetches data from an API.\n    \n        The function uses a public API endpoint from JSONPlaceholder to fetch data for product id 2.\n        The generated code is then printed to the console.\n        \"\"\"\n        sample_api_response = \"\"\"\n      {\n        \"userId\": 1,\n        \"id\": 2,\n        \"title\": \"qui est esse\",\n        \"body\": \"est rerum tempore vitae\\\\nsequi sint nihil reprehenderit dolor beatae ea dolores neque\\\\nfugiat blanditiis voluptate porro vel nihil molestiae ut reiciendis\\\\nqui aperiam non debitis possimus qui neque nisi nulla\"\n      }\n      \"\"\"\n        api = \"https://jsonplaceholder.typicode.com/posts/2\"\n        input_message = f\"\"\"\n      Fetch data of product id 2 from the given API.\n      API : {api}\n      Sample response from the API:{sample_api_response}\n      \"\"\"\n        only_python = OnlyPython(with_model=settings.with_model)\n>       output = await only_python.generate_code(input_message, \"\", True, False)\n\ntests/test_only_python.py:77: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_python.py:223: in generate_code\n    generated_code_output = json.loads(generated_code_output)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py:338: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x1054d30b0>, s = 'Error during code execution.', idx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py:356: JSONDecodeError"}, "teardown": {"duration": 0.001560417003929615, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00050681, completion_tokens_cost_usd_dollar: 0.00019276\nDEBUG: response_cost: 0.00069957\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00069957\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:37 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00050681, completion_tokens_cost_usd_dollar: 0.00019276\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00069957\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00069957\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.221598, "msecs": 221.0, "relativeCreated": 11820.801973342896, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488797.221962, "msecs": 221.0, "relativeCreated": 11821.166038513184, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488797.222059, "msecs": 222.0, "relativeCreated": 11821.263074874878, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.222201, "msecs": 222.0, "relativeCreated": 11821.405172348022, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488797.222273, "msecs": 222.0, "relativeCreated": 11821.477174758911, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488797.222351, "msecs": 222.0, "relativeCreated": 11821.555137634277, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00050681, completion_tokens_cost_usd_dollar: 0.00019276", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488797.22241, "msecs": 222.0, "relativeCreated": 11821.614027023315, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "response_cost: 0.00069957", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488797.222473, "msecs": 222.0, "relativeCreated": 11821.676969528198, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00069957", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488797.222511, "msecs": 222.0, "relativeCreated": 11821.715116500854, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488797.222629, "msecs": 222.0, "relativeCreated": 11821.83313369751, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488797.222665, "msecs": 222.0, "relativeCreated": 11821.869134902954, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-120", "asctime": "22:36:37"}]}}, {"nodeid": "tests/test_only_python.py::test_internet_connection", "lineno": 82, "outcome": "failed", "keywords": ["test_internet_connection", "asyncio", "pytestmark", "test_only_python.py", "tests", "elevate", ""], "setup": {"duration": 0.00026170798810198903, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488797.2232761, "msecs": 223.0, "relativeCreated": 11822.480201721191, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.008003041002666578, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_python.py", "lineno": 95, "message": ""}, {"path": "src/elevate/only_python.py", "lineno": 180, "message": "in generate_code"}, {"path": "src/elevate/only_python.py", "lineno": 56, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n******************** Generating Python Code ********************\n\nDEBUG: Generating the python code...\nDEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "root", "msg": "\n******************** Generating Python Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488797.2235289, "msecs": 223.0, "relativeCreated": 11822.732925415039, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "root", "msg": "Generating the python code...", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 179, "funcName": "generate_code", "created": 1749488797.223584, "msecs": 223.0, "relativeCreated": 11822.788000106812, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.223666, "msecs": 223.0, "relativeCreated": 11822.870016098022, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.223712, "msecs": 223.0, "relativeCreated": 11822.916030883789, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.2237482, "msecs": 223.0, "relativeCreated": 11822.952270507812, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.2237902, "msecs": 223.0, "relativeCreated": 11822.994232177734, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488797.2239318, "msecs": 223.0, "relativeCreated": 11823.13585281372, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.22398, "msecs": 223.0, "relativeCreated": 11823.1840133667, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488797.224021, "msecs": 224.0, "relativeCreated": 11823.225021362305, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488797.22445, "msecs": 224.0, "relativeCreated": 11823.654174804688, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488797.224562, "msecs": 224.0, "relativeCreated": 11823.765993118286, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488797.2246149, "msecs": 224.0, "relativeCreated": 11823.818922042847, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.224669, "msecs": 224.0, "relativeCreated": 11823.873043060303, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488797.224726, "msecs": 224.0, "relativeCreated": 11823.930025100708, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488797.224836, "msecs": 224.0, "relativeCreated": 11824.040174484253, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.225413, "msecs": 225.0, "relativeCreated": 11824.617147445679, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.22561, "msecs": 225.0, "relativeCreated": 11824.814081192017, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.225633, "msecs": 225.0, "relativeCreated": 11824.83696937561, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.225719, "msecs": 225.0, "relativeCreated": 11824.923038482666, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.225738, "msecs": 225.0, "relativeCreated": 11824.942111968994, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.225792, "msecs": 225.0, "relativeCreated": 11824.995994567871, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.2258172, "msecs": 225.0, "relativeCreated": 11825.021266937256, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.225899, "msecs": 225.0, "relativeCreated": 11825.103044509888, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488797.225964, "msecs": 225.0, "relativeCreated": 11825.168132781982, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488797.226221, "msecs": 226.0, "relativeCreated": 11825.425148010254, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488797.228698, "msecs": 228.0, "relativeCreated": 11827.90207862854, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-122", "asctime": "22:36:37"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr... Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-b5867d77-3d2a-4438-8dc3-9c147c05e2b1', created=1749488797, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57f0b0>, stream = False\ndata = {'messages': [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on t...tFormat>str</OutputFormat>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr... Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '8f6c0d8d-5521-4a29-be02-918649ae98de', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57f0b0>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr... Create a function to send request to google.com.\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-b5867d77-3d2a-4438-8dc3-9c147c05e2b1', created=1749488797, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57f0b0>, stream = False\ndata = {'messages': [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on t...tFormat>str</OutputFormat>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_internet_connection(settings: Any) -> None:\n        \"\"\"\n        Test the generate_code method's ability to generate code that makes an internet connection.\n    \n        The function instructs the generate_code method to create a function that sends a request to google.com.\n        The generated code is then printed to the console.\n        \"\"\"\n        input_message = \"\"\"\n      Create a function to send request to google.com.\n      \"\"\"\n        only_python = OnlyPython(with_model=settings.with_model)\n>       output = await only_python.generate_code(input_message, \"\", False, False)\n\ntests/test_only_python.py:95: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_python.py:180: in generate_code\n    code = await self.make_llm_call(system_prompt, message)\nsrc/elevate/only_python.py:56: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '8f6c0d8d-5521-4a29-be02-918649ae98de', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57f0b0>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003287919971626252, "outcome": "passed"}}, {"nodeid": "tests/test_only_python.py::test_data_structure_code", "lineno": 100, "outcome": "failed", "keywords": ["test_data_structure_code", "asyncio", "pytestmark", "test_only_python.py", "tests", "elevate", ""], "setup": {"duration": 0.00020204202155582607, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488797.386903, "msecs": 386.0, "relativeCreated": 11986.107110977173, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 1.7267489999940153, "outcome": "failed", "crash": {"path": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py", "lineno": 356, "message": "json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)"}, "traceback": [{"path": "tests/test_only_python.py", "lineno": 117, "message": ""}, {"path": "src/elevate/only_python.py", "lineno": 223, "message": "in generate_code"}, {"path": "../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/__init__.py", "lineno": 346, "message": "in loads"}, {"path": "../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py", "lineno": 338, "message": "in decode"}, {"path": "../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py", "lineno": 356, "message": "JSONDecodeError"}], "stdout": "DEBUG: \n******************** Generating Python Code ********************\n\nDEBUG: Generating the python code...\nDEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f54f4a0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e3d4bf0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'983'), (b'x-ratelimit-remaining-tokens', b'4035'), (b'x-ratelimit-reset-requests', b'24m27.479s'), (b'x-ratelimit-reset-tokens', b'39.824s'), (b'x-request-id', b'req_01jxath1tpfpkrajg02fp6s8sm'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238785dc48540-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-fc5cc7cf-ce2b-4d68-97ad-fd4eb50bb0ca\", \"object\": \"chat.completion\", \"created\": 1749488797, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"<PipInstalls>\\npip install json\\n</PipInstalls>\\n<Imports>\\nimport json\\n</Imports>\\n<CodeCompletion>\\nclass Node:\\n    # Initialize a Node with a value and a pointer to the next Node\\n    def __init__(self, val):\\n        self.val = val\\n        self.next = None\\n\\ndef delete_middle(head):\\n    # Handle edge cases\\n    if not head:\\n        return head\\n    if not head.next:\\n        return None\\n\\n    # Initialize two pointers, slow and fast\\n    slow = head\\n    fast = head\\n\\n    # Move the fast pointer two steps at a time, and the slow pointer one step at a time\\n    prev_slow = None\\n    while fast.next and fast.next.next:\\n        prev_slow = slow\\n        slow = slow.next\\n        fast = fast.next.next\\n\\n    # If the list has an even number of nodes, move the slow pointer one step forward\\n    if fast.next:\\n        prev_slow = slow\\n        slow = slow.next\\n\\n    # Remove the middle node\\n    if prev_slow:\\n        prev_slow.next = slow.next\\n    else:\\n        head = slow.next\\n\\n    return head\\n\\ndef construct_linked_list(arr):\\n    # Create a new Node for each value in the array\\n    if not arr:\\n        return None\\n    head = Node(arr[0])\\n    current = head\\n    for val in arr[1:]:\\n        current.next = Node(val)\\n        current = current.next\\n    return head\\n\\ndef print_linked_list(head):\\n    # Traverse the linked list and append each value to a list\\n    result = []\\n    current = head\\n    while current:\\n        result.append(current.val)\\n        current = current.next\\n    return json.dumps(result)\\n\\n# Construct a linked list from the given array\\nhead = construct_linked_list([1, 2, 3, 4, 5])\\n\\n# Delete the middle node\\nhead = delete_middle(head)\\n\\n# Print the resulting linked list\\noutput = print_linked_list(head)\\nprint(output)\\n</CodeCompletion>\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050727919, \"prompt_tokens\": 830, \"prompt_time\": 0.062003841, \"completion_tokens\": 425, \"completion_time\": 1.5454545450000001, \"total_tokens\": 1255, \"total_time\": 1.607458386}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath1tpfpkrajg02fp6s8sm\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0004896999999999999, completion_tokens_cost_usd_dollar: 0.00033575\nDEBUG: response_cost: 0.0008254499999999999\nDEBUG: \nGenerated Code + Installs:\n\n<PipInstalls>\npip install json\n</PipInstalls>\n<Imports>\nimport json\n</Imports>\n<CodeCompletion>\nclass Node:\n    # Initialize a Node with a value and a pointer to the next Node\n    def __init__(self, val):\n        self.val = val\n        self.next = None\n\ndef delete_middle(head):\n    # Handle edge cases\n    if not head:\n        return head\n    if not head.next:\n        return None\n\n    # Initialize two pointers, slow and fast\n    slow = head\n    fast = head\n\n    # Move the fast pointer two steps at a time, and the slow pointer one step at a time\n    prev_slow = None\n    while fast.next and fast.next.next:\n        prev_slow = slow\n        slow = slow.next\n        fast = fast.next.next\n\n    # If the list has an even number of nodes, move the slow pointer one step forward\n    if fast.next:\n        prev_slow = slow\n        slow = slow.next\n\n    # Remove the middle node\n    if prev_slow:\n        prev_slow.next = slow.next\n    else:\n        head = slow.next\n\n    return head\n\ndef construct_linked_list(arr):\n    # Create a new Node for each value in the array\n    if not arr:\n        return None\n    head = Node(arr[0])\n    current = head\n    for val in arr[1:]:\n        current.next = Node(val)\n        current = current.next\n    return head\n\ndef print_linked_list(head):\n    # Traverse the linked list and append each value to a list\n    result = []\n    current = head\n    while current:\n        result.append(current.val)\n        current = current.next\n    return json.dumps(result)\n\n# Construct a linked list from the given array\nhead = construct_linked_list([1, 2, 3, 4, 5])\n\n# Delete the middle node\nhead = delete_middle(head)\n\n# Print the resulting linked list\noutput = print_linked_list(head)\nprint(output)\n</CodeCompletion>\nDEBUG: \n******************** End of Generated Code ********************\n\nDEBUG: \n******************** Generated Code ********************\n\nDEBUG: ----------------------------------------\nDEBUG: Full code with imports and pip installs:\n\nDEBUG: ----------------------------------------\nDEBUG: pip install json\nDEBUG: ----------------------------------------\nDEBUG: # -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated imports\n\nimport json\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Original code.\n\n\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated Completion\n\nclass Node:\n    # Initialize a Node with a value and a pointer to the next Node\n    def __init__(self, val):\n        self.val = val\n        self.next = None\n\ndef delete_middle(head):\n    # Handle edge cases\n    if not head:\n        return head\n    if not head.next:\n        return None\n\n    # Initialize two pointers, slow and fast\n    slow = head\n    fast = head\n\n    # Move the fast pointer two steps at a time, and the slow pointer one step at a time\n    prev_slow = None\n    while fast.next and fast.next.next:\n        prev_slow = slow\n        slow = slow.next\n        fast = fast.next.next\n\n    # If the list has an even number of nodes, move the slow pointer one step forward\n    if fast.next:\n        prev_slow = slow\n        slow = slow.next\n\n    # Remove the middle node\n    if prev_slow:\n        prev_slow.next = slow.next\n    else:\n        head = slow.next\n\n    return head\n\ndef construct_linked_list(arr):\n    # Create a new Node for each value in the array\n    if not arr:\n        return None\n    head = Node(arr[0])\n    current = head\n    for val in arr[1:]:\n        current.next = Node(val)\n        current = current.next\n    return head\n\ndef print_linked_list(head):\n    # Traverse the linked list and append each value to a list\n    result = []\n    current = head\n    while current:\n        result.append(current.val)\n        current = current.next\n    return json.dumps(result)\n\n# Construct a linked list from the given array\nhead = construct_linked_list([1, 2, 3, 4, 5])\n\n# Delete the middle node\nhead = delete_middle(head)\n\n# Print the resulting linked list\noutput = print_linked_list(head)\nprint(output)\nDEBUG: ----------------------------------------\nDEBUG: \n******************** Executing Generated Code ********************\n\nERROR: Error during code execution\nTraceback (most recent call last):\n  File \" Developer/OpenSource/elevate/src/elevate/only_python.py\", line 84, in execute_code_using_e2b_sandbox\n    sandbox = await AsyncSandbox.create(envs=dict(os.environ))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/main.py\", line 208, in create\n    response = await SandboxApi._create_sandbox(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/sandbox_api.py\", line 235, in _create_sandbox\n    async with AsyncApiClient(config) as api_client:\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/api/__init__.py\", line 70, in __init__\n    raise AuthenticationException(\ne2b.exceptions.AuthenticationException: API key is required, please visit the Team tab at https://e2b.dev/dashboard to get your API key. You can either set the environment variable `E2B_API_KEY` or you can pass it directly to the sandbox like Sandbox(api_key=\"e2b_...\")\nDEBUG: \nOutput of Execution:\n\nDEBUG: Error during code execution.\nDEBUG: \n******************** End of Execution ********************\n\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f538860>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:37 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'thinking': None}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:37 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-fc5cc7cf-ce2b-4d68-97ad-fd4eb50bb0ca\", \"object\": \"chat.completion\", \"created\": 1749488797, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"<PipInstalls>\\npip install json\\n</PipInstalls>\\n<Imports>\\nimport json\\n</Imports>\\n<CodeCompletion>\\nclass Node:\\n    # Initialize a Node with a value and a pointer to the next Node\\n    def __init__(self, val):\\n        self.val = val\\n        self.next = None\\n\\ndef delete_middle(head):\\n    # Handle edge cases\\n    if not head:\\n        return head\\n    if not head.next:\\n        return None\\n\\n    # Initialize two pointers, slow and fast\\n    slow = head\\n    fast = head\\n\\n    # Move the fast pointer two steps at a time, and the slow pointer one step at a time\\n    prev_slow = None\\n    while fast.next and fast.next.next:\\n        prev_slow = slow\\n        slow = slow.next\\n        fast = fast.next.next\\n\\n    # If the list has an even number of nodes, move the slow pointer one step forward\\n    if fast.next:\\n        prev_slow = slow\\n        slow = slow.next\\n\\n    # Remove the middle node\\n    if prev_slow:\\n        prev_slow.next = slow.next\\n    else:\\n        head = slow.next\\n\\n    return head\\n\\ndef construct_linked_list(arr):\\n    # Create a new Node for each value in the array\\n    if not arr:\\n        return None\\n    head = Node(arr[0])\\n    current = head\\n    for val in arr[1:]:\\n        current.next = Node(val)\\n        current = current.next\\n    return head\\n\\ndef print_linked_list(head):\\n    # Traverse the linked list and append each value to a list\\n    result = []\\n    current = head\\n    while current:\\n        result.append(current.val)\\n        current = current.next\\n    return json.dumps(result)\\n\\n# Construct a linked list from the given array\\nhead = construct_linked_list([1, 2, 3, 4, 5])\\n\\n# Delete the middle node\\nhead = delete_middle(head)\\n\\n# Print the resulting linked list\\noutput = print_linked_list(head)\\nprint(output)\\n</CodeCompletion>\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050727919, \"prompt_tokens\": 830, \"prompt_time\": 0.062003841, \"completion_tokens\": 425, \"completion_time\": 1.5454545450000001, \"total_tokens\": 1255, \"total_time\": 1.607458386}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath1tpfpkrajg02fp6s8sm\"}}\n\n\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0004896999999999999, completion_tokens_cost_usd_dollar: 0.00033575\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.0008254499999999999\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f538860>>\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "root", "msg": "\n******************** Generating Python Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488797.387112, "msecs": 387.0, "relativeCreated": 11986.315965652466, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "Generating the python code...", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 179, "funcName": "generate_code", "created": 1749488797.3871381, "msecs": 387.0, "relativeCreated": 11986.342191696167, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.3871732, "msecs": 387.0, "relativeCreated": 11986.377239227295, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.3872042, "msecs": 387.0, "relativeCreated": 11986.408233642578, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.38722, "msecs": 387.0, "relativeCreated": 11986.423969268799, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.387237, "msecs": 387.0, "relativeCreated": 11986.441135406494, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488797.387298, "msecs": 387.0, "relativeCreated": 11986.502170562744, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.387321, "msecs": 387.0, "relativeCreated": 11986.525058746338, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488797.38734, "msecs": 387.0, "relativeCreated": 11986.544132232666, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488797.387578, "msecs": 387.0, "relativeCreated": 11986.78207397461, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488797.3876228, "msecs": 387.0, "relativeCreated": 11986.82689666748, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488797.387677, "msecs": 387.0, "relativeCreated": 11986.881017684937, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488797.387711, "msecs": 387.0, "relativeCreated": 11986.915111541748, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488797.387739, "msecs": 387.0, "relativeCreated": 11986.943006515503, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>Write Python code that:\\n\\n1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\\n2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\\n3. Constructs a linked list from the json array `[1, 2, 3, 4\\n5. Prints the resulting linked list values as a json array.\\n</Prompt>\\n\\n\\n<OutputFormat>json</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488797.387796, "msecs": 387.0, "relativeCreated": 11986.999988555908, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:37"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.388101, "msecs": 388.0, "relativeCreated": 11987.305164337158, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f54f4a0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.4014468, "msecs": 401.0, "relativeCreated": 12000.650882720947, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.4014862, "msecs": 401.0, "relativeCreated": 12000.690221786499, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10e3d4bf0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.414624, "msecs": 414.0, "relativeCreated": 12013.828039169312, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.4147232, "msecs": 414.0, "relativeCreated": 12013.927221298218, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.41483, "msecs": 414.0, "relativeCreated": 12014.034032821655, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.414849, "msecs": 414.0, "relativeCreated": 12014.053106307983, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.414919, "msecs": 414.0, "relativeCreated": 12014.12296295166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488797.414934, "msecs": 414.0, "relativeCreated": 12014.137983322144, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:39 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'983'), (b'x-ratelimit-remaining-tokens', b'4035'), (b'x-ratelimit-reset-requests', b'24m27.479s'), (b'x-ratelimit-reset-tokens', b'39.824s'), (b'x-request-id', b'req_01jxath1tpfpkrajg02fp6s8sm'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238785dc48540-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.108247, "msecs": 108.0, "relativeCreated": 13707.451105117798, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488799.108802, "msecs": 108.0, "relativeCreated": 13708.006143569946, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.109024, "msecs": 109.0, "relativeCreated": 13708.22811126709, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.1093419, "msecs": 109.0, "relativeCreated": 13708.545923233032, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.1094558, "msecs": 109.0, "relativeCreated": 13708.659887313843, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.1095989, "msecs": 109.0, "relativeCreated": 13708.802938461304, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488799.109895, "msecs": 109.0, "relativeCreated": 13709.099054336548, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-fc5cc7cf-ce2b-4d68-97ad-fd4eb50bb0ca\", \"object\": \"chat.completion\", \"created\": 1749488797, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"<PipInstalls>\\npip install json\\n</PipInstalls>\\n<Imports>\\nimport json\\n</Imports>\\n<CodeCompletion>\\nclass Node:\\n    # Initialize a Node with a value and a pointer to the next Node\\n    def __init__(self, val):\\n        self.val = val\\n        self.next = None\\n\\ndef delete_middle(head):\\n    # Handle edge cases\\n    if not head:\\n        return head\\n    if not head.next:\\n        return None\\n\\n    # Initialize two pointers, slow and fast\\n    slow = head\\n    fast = head\\n\\n    # Move the fast pointer two steps at a time, and the slow pointer one step at a time\\n    prev_slow = None\\n    while fast.next and fast.next.next:\\n        prev_slow = slow\\n        slow = slow.next\\n        fast = fast.next.next\\n\\n    # If the list has an even number of nodes, move the slow pointer one step forward\\n    if fast.next:\\n        prev_slow = slow\\n        slow = slow.next\\n\\n    # Remove the middle node\\n    if prev_slow:\\n        prev_slow.next = slow.next\\n    else:\\n        head = slow.next\\n\\n    return head\\n\\ndef construct_linked_list(arr):\\n    # Create a new Node for each value in the array\\n    if not arr:\\n        return None\\n    head = Node(arr[0])\\n    current = head\\n    for val in arr[1:]:\\n        current.next = Node(val)\\n        current = current.next\\n    return head\\n\\ndef print_linked_list(head):\\n    # Traverse the linked list and append each value to a list\\n    result = []\\n    current = head\\n    while current:\\n        result.append(current.val)\\n        current = current.next\\n    return json.dumps(result)\\n\\n# Construct a linked list from the given array\\nhead = construct_linked_list([1, 2, 3, 4, 5])\\n\\n# Delete the middle node\\nhead = delete_middle(head)\\n\\n# Print the resulting linked list\\noutput = print_linked_list(head)\\nprint(output)\\n</CodeCompletion>\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050727919, \"prompt_tokens\": 830, \"prompt_time\": 0.062003841, \"completion_tokens\": 425, \"completion_time\": 1.5454545450000001, \"total_tokens\": 1255, \"total_time\": 1.607458386}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath1tpfpkrajg02fp6s8sm\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.110189, "msecs": 110.0, "relativeCreated": 13709.39302444458, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488799.110673, "msecs": 110.0, "relativeCreated": 13709.877014160156, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488799.110989, "msecs": 110.0, "relativeCreated": 13710.193157196045, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488799.111187, "msecs": 111.0, "relativeCreated": 13710.3910446167, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0004896999999999999, completion_tokens_cost_usd_dollar: 0.00033575", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488799.11131, "msecs": 111.0, "relativeCreated": 13710.514068603516, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "response_cost: 0.0008254499999999999", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488799.111413, "msecs": 111.0, "relativeCreated": 13710.617065429688, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125", "asctime": "22:36:39"}, {"name": "root", "msg": "\nGenerated Code + Installs:\n\n<PipInstalls>\npip install json\n</PipInstalls>\n<Imports>\nimport json\n</Imports>\n<CodeCompletion>\nclass Node:\n    # Initialize a Node with a value and a pointer to the next Node\n    def __init__(self, val):\n        self.val = val\n        self.next = None\n\ndef delete_middle(head):\n    # Handle edge cases\n    if not head:\n        return head\n    if not head.next:\n        return None\n\n    # Initialize two pointers, slow and fast\n    slow = head\n    fast = head\n\n    # Move the fast pointer two steps at a time, and the slow pointer one step at a time\n    prev_slow = None\n    while fast.next and fast.next.next:\n        prev_slow = slow\n        slow = slow.next\n        fast = fast.next.next\n\n    # If the list has an even number of nodes, move the slow pointer one step forward\n    if fast.next:\n        prev_slow = slow\n        slow = slow.next\n\n    # Remove the middle node\n    if prev_slow:\n        prev_slow.next = slow.next\n    else:\n        head = slow.next\n\n    return head\n\ndef construct_linked_list(arr):\n    # Create a new Node for each value in the array\n    if not arr:\n        return None\n    head = Node(arr[0])\n    current = head\n    for val in arr[1:]:\n        current.next = Node(val)\n        current = current.next\n    return head\n\ndef print_linked_list(head):\n    # Traverse the linked list and append each value to a list\n    result = []\n    current = head\n    while current:\n        result.append(current.val)\n        current = current.next\n    return json.dumps(result)\n\n# Construct a linked list from the given array\nhead = construct_linked_list([1, 2, 3, 4, 5])\n\n# Delete the middle node\nhead = delete_middle(head)\n\n# Print the resulting linked list\noutput = print_linked_list(head)\nprint(output)\n</CodeCompletion>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 181, "funcName": "generate_code", "created": 1749488799.111555, "msecs": 111.0, "relativeCreated": 13710.759162902832, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "\n******************** End of Generated Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488799.111681, "msecs": 111.0, "relativeCreated": 13710.885047912598, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "\n******************** Generated Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488799.1117382, "msecs": 111.0, "relativeCreated": 13710.942268371582, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 207, "funcName": "generate_code", "created": 1749488799.111799, "msecs": 111.0, "relativeCreated": 13711.003065109253, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "Full code with imports and pip installs:\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 208, "funcName": "generate_code", "created": 1749488799.111845, "msecs": 111.0, "relativeCreated": 13711.04907989502, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 209, "funcName": "generate_code", "created": 1749488799.111881, "msecs": 111.0, "relativeCreated": 13711.085081100464, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "pip install json", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "generate_code", "created": 1749488799.111917, "msecs": 111.0, "relativeCreated": 13711.121082305908, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 211, "funcName": "generate_code", "created": 1749488799.111948, "msecs": 111.0, "relativeCreated": 13711.152076721191, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated imports\n\nimport json\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Original code.\n\n\n\n# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -# -\n\n# Generated Completion\n\nclass Node:\n    # Initialize a Node with a value and a pointer to the next Node\n    def __init__(self, val):\n        self.val = val\n        self.next = None\n\ndef delete_middle(head):\n    # Handle edge cases\n    if not head:\n        return head\n    if not head.next:\n        return None\n\n    # Initialize two pointers, slow and fast\n    slow = head\n    fast = head\n\n    # Move the fast pointer two steps at a time, and the slow pointer one step at a time\n    prev_slow = None\n    while fast.next and fast.next.next:\n        prev_slow = slow\n        slow = slow.next\n        fast = fast.next.next\n\n    # If the list has an even number of nodes, move the slow pointer one step forward\n    if fast.next:\n        prev_slow = slow\n        slow = slow.next\n\n    # Remove the middle node\n    if prev_slow:\n        prev_slow.next = slow.next\n    else:\n        head = slow.next\n\n    return head\n\ndef construct_linked_list(arr):\n    # Create a new Node for each value in the array\n    if not arr:\n        return None\n    head = Node(arr[0])\n    current = head\n    for val in arr[1:]:\n        current.next = Node(val)\n        current = current.next\n    return head\n\ndef print_linked_list(head):\n    # Traverse the linked list and append each value to a list\n    result = []\n    current = head\n    while current:\n        result.append(current.val)\n        current = current.next\n    return json.dumps(result)\n\n# Construct a linked list from the given array\nhead = construct_linked_list([1, 2, 3, 4, 5])\n\n# Delete the middle node\nhead = delete_middle(head)\n\n# Print the resulting linked list\noutput = print_linked_list(head)\nprint(output)", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 212, "funcName": "generate_code", "created": 1749488799.112003, "msecs": 112.0, "relativeCreated": 13711.207151412964, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "----------------------------------------", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 213, "funcName": "generate_code", "created": 1749488799.112054, "msecs": 112.0, "relativeCreated": 13711.258172988892, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "\n******************** Executing Generated Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488799.112085, "msecs": 112.0, "relativeCreated": 13711.289167404175, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "Error during code execution", "args": null, "levelname": "ERROR", "levelno": 40, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": "Traceback (most recent call last):\n  File \" Developer/OpenSource/elevate/src/elevate/only_python.py\", line 84, in execute_code_using_e2b_sandbox\n    sandbox = await AsyncSandbox.create(envs=dict(os.environ))\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/main.py\", line 208, in create\n    response = await SandboxApi._create_sandbox(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/sandbox_async/sandbox_api.py\", line 235, in _create_sandbox\n    async with AsyncApiClient(config) as api_client:\n               ^^^^^^^^^^^^^^^^^^^^^^\n  File \" Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/e2b/api/__init__.py\", line 70, in __init__\n    raise AuthenticationException(\ne2b.exceptions.AuthenticationException: API key is required, please visit the Team tab at https://e2b.dev/dashboard to get your API key. You can either set the environment variable `E2B_API_KEY` or you can pass it directly to the sandbox like Sandbox(api_key=\"e2b_...\")", "stack_info": null, "lineno": 116, "funcName": "execute_code_using_e2b_sandbox", "created": 1749488799.112301, "msecs": 112.0, "relativeCreated": 13711.50517463684, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "\nOutput of Execution:\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 218, "funcName": "generate_code", "created": 1749488799.1129322, "msecs": 112.0, "relativeCreated": 13712.136268615723, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "Error during code execution.", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 219, "funcName": "generate_code", "created": 1749488799.112982, "msecs": 112.0, "relativeCreated": 13712.186098098755, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "root", "msg": "\n******************** End of Execution ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488799.1130261, "msecs": 113.0, "relativeCreated": 13712.230205535889, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-125"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f538860>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.113156, "msecs": 113.0, "relativeCreated": 13712.360143661499, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-127", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488799.113251, "msecs": 113.0, "relativeCreated": 13712.455034255981, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-127", "asctime": "22:36:39"}], "longrepr": "settings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_data_structure_code(settings: Any) -> None:\n        \"\"\"\n        Test the generate_code method's ability to generate code that manipulates a data structure (linked list).\n    \n        The function instructs the generate_code method to create code that deletes the middle node of a linked list.\n        The generated code is then printed to the console.\n        \"\"\"\n        input_message = \"\"\"Write Python code that:\n    \n    1. Defines a Node class for a singly linked list, with attributes `val` and `next`.\n    2. Implements a function `delete_middle(head)` which removes the middle node of a linked list (if the list has even length, remove the second of the two middles).\n    3. Constructs a linked list from the json array `[1, 2, 3, 4\n    5. Prints the resulting linked list values as a json array.\n    \"\"\"\n        only_python = OnlyPython(with_model=settings.with_model)\n>       output = await only_python.generate_code(input_message, \"\", True, False)\n\ntests/test_only_python.py:117: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_python.py:223: in generate_code\n    generated_code_output = json.loads(generated_code_output)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/__init__.py:346: in loads\n    return _default_decoder.decode(s)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py:338: in decode\n    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <json.decoder.JSONDecoder object at 0x1054d30b0>, s = 'Error during code execution.', idx = 0\n\n    def raw_decode(self, s, idx=0):\n        \"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\n        a JSON document) and return a 2-tuple of the Python\n        representation and the index in ``s`` where the document ended.\n    \n        This can be used to decode a JSON document from a string that may\n        have extraneous data at the end.\n    \n        \"\"\"\n        try:\n            obj, end = self.scan_once(s, idx)\n        except StopIteration as err:\n>           raise JSONDecodeError(\"Expecting value\", s, err.value) from None\nE           json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/json/decoder.py:356: JSONDecodeError"}, "teardown": {"duration": 0.0013101249933242798, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0004896999999999999, completion_tokens_cost_usd_dollar: 0.00033575\nDEBUG: response_cost: 0.0008254499999999999\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.0008254499999999999\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:39 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0004896999999999999, completion_tokens_cost_usd_dollar: 0.00033575\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.0008254499999999999\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.0008254499999999999\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.129347, "msecs": 129.0, "relativeCreated": 13728.551149368286, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488799.129604, "msecs": 129.0, "relativeCreated": 13728.808164596558, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488799.1296582, "msecs": 129.0, "relativeCreated": 13728.862285614014, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.129746, "msecs": 129.0, "relativeCreated": 13728.950023651123, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488799.129788, "msecs": 129.0, "relativeCreated": 13728.991985321045, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488799.129832, "msecs": 129.0, "relativeCreated": 13729.036092758179, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0004896999999999999, completion_tokens_cost_usd_dollar: 0.00033575", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488799.129863, "msecs": 129.0, "relativeCreated": 13729.067087173462, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "response_cost: 0.0008254499999999999", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488799.129903, "msecs": 129.0, "relativeCreated": 13729.107141494751, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.0008254499999999999", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488799.129924, "msecs": 129.0, "relativeCreated": 13729.128122329712, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488799.129983, "msecs": 129.0, "relativeCreated": 13729.18701171875, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488799.130018, "msecs": 130.0, "relativeCreated": 13729.222059249878, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-128", "asctime": "22:36:39"}]}}, {"nodeid": "tests/test_only_python.py::test_data_visualization", "lineno": 122, "outcome": "failed", "keywords": ["test_data_visualization", "asyncio", "pytestmark", "test_only_python.py", "tests", "elevate", ""], "setup": {"duration": 0.0004612079937942326, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488799.130965, "msecs": 130.0, "relativeCreated": 13730.16905784607, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.008411124988924712, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_python.py", "lineno": 137, "message": ""}, {"path": "src/elevate/only_python.py", "lineno": 180, "message": "in generate_code"}, {"path": "src/elevate/only_python.py", "lineno": 56, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n******************** Generating Python Code ********************\n\nDEBUG: Generating the python code...\nDEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:39 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "root", "msg": "\n******************** Generating Python Code ********************\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 145, "funcName": "print_section_header", "created": 1749488799.131412, "msecs": 131.0, "relativeCreated": 13730.616092681885, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "root", "msg": "Generating the python code...", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/src/elevate/only_python.py", "filename": "only_python.py", "module": "only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 179, "funcName": "generate_code", "created": 1749488799.131465, "msecs": 131.0, "relativeCreated": 13730.669021606445, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.131537, "msecs": 131.0, "relativeCreated": 13730.741024017334, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.1315851, "msecs": 131.0, "relativeCreated": 13730.789184570312, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.1316218, "msecs": 131.0, "relativeCreated": 13730.825901031494, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.131661, "msecs": 131.0, "relativeCreated": 13730.865001678467, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488799.131807, "msecs": 131.0, "relativeCreated": 13731.011152267456, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.1318521, "msecs": 131.0, "relativeCreated": 13731.056213378906, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488799.131891, "msecs": 131.0, "relativeCreated": 13731.0950756073, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488799.1322749, "msecs": 132.0, "relativeCreated": 13731.478929519653, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488799.132352, "msecs": 132.0, "relativeCreated": 13731.556177139282, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488799.132395, "msecs": 132.0, "relativeCreated": 13731.59909248352, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.132437, "msecs": 132.0, "relativeCreated": 13731.641054153442, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488799.1324742, "msecs": 132.0, "relativeCreated": 13731.678247451782, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s prompt.\\nIf provided, use the frameworks mentioned in <Framework> block (assume that it is installed).\\nIf provided, refer to the code given in <Code> block and generate the python code using it if needed.\\nWhile generating this code, DO NOT include the code in <Code> block in your output.\\n\\nSo, write both codes in a single python file.\\nDO NOT add \"if __name__ == \\'__main__\\':\" code snippet in generated code.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n```\\n<Prompt>User prompt specifying what Python code should be generated.</Prompt>\\n<Framework>The desired framework to use (e.g., Flask, Django, TensorFlow).\\nIf no framework is specified, use standard Python libraries.</Framework>\\n<Code> Existing code which you may use (DO NOT INCLUDE THIS IN THE OUTPUT). </Code>\\n<OutputFormat>The output of code should be printed in this format</OutputFormat>\\n```\\n\\n**INSTRUCTIONS**\\n\\n1.  **Understand the User\\'s Intent:** Carefully analyze the user\\'s prompt to understand the desired functionality and purpose of the code.\\n2.  **Generate Python Code:** Write Python code that fulfills the user\\'s intent. Ensure the code is syntactically correct, well-structured, and follows Python best practices.\\n3.  **Adhere to the Specified Framework:** If a framework is specified, use it to structure the code and implement the desired functionality. If no framework is specified, use standard Python libraries.\\n4.  **Produce Readable and Functional Code:** Write code that is easy to read, understand, and maintain. Use meaningful variable names, clear comments, and proper indentation. Ensure the code is functional and produces the expected output.\\n5.  **Include Comments:** Add comments to explain the code\\'s logic, purpose, and functionality. This will help users understand and modify the code if needed.\\n6.  **Handle Errors:** Implement error handling to gracefully handle unexpected inputs or situations.\\n7.  **Other libraries:** Feel free to use any other libraries that don\\'t need installs (e.g. datetime, json, etc.)\\n\\n\\n**OUTPUT**\\nReturn ONLY an XML containing two fields:\\n1. **PipInstalls**: Any pip installs needed\\n2. **Imports**: Any imports needed for the code to run.\\n3. **CodeCompletion**: Your additional generated Python code in your response to follow the existing code.\\n    Include comments to explain the code.\\n    Do not include any additional formatting or explanations.\\n    Your code should not print anything except the output.\\n\\nFor example (when output format is json):\\n```\\n<PipInstalls>\\npip install requests\\n</PipInstalls>\\n<Imports>\\nimport requests\\n</Imports>\\n<CodeCompletion>\\ndef get_current_ip(service_url: str = \"https://api.ipify.org?format=json\") -> str | None:\\n    try:\\n        response = requests.get(service_url, timeout=5)\\n        response.raise_for_status()\\n        data = response.json()\\n        return data.get(\"ip\")\\n    except requests.RequestException as e:\\n        print(f\"Error fetching IP: {e}\")\\n        return None\\n\\noutput = {\"current_ip\": get_current_ip()}\\nprint(json.dumps(output, indent=2))\\n</CodeCompletion>\\n```\\n'}, {'role': 'user', 'content': '\\n<Prompt>\\n  Plot below numbers\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488799.1325598, "msecs": 132.0, "relativeCreated": 13731.76383972168, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.133228, "msecs": 133.0, "relativeCreated": 13732.432126998901, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.1334882, "msecs": 133.0, "relativeCreated": 13732.692241668701, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.133527, "msecs": 133.0, "relativeCreated": 13732.731103897095, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.1336598, "msecs": 133.0, "relativeCreated": 13732.863903045654, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.133703, "msecs": 133.0, "relativeCreated": 13732.907056808472, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.1338, "msecs": 133.0, "relativeCreated": 13733.004093170166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.133843, "msecs": 133.0, "relativeCreated": 13733.047008514404, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.13395, "msecs": 133.0, "relativeCreated": 13733.15405845642, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488799.134026, "msecs": 134.0, "relativeCreated": 13733.230113983154, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488799.1343, "msecs": 134.0, "relativeCreated": 13733.504056930542, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488799.136725, "msecs": 136.0, "relativeCreated": 13735.929012298584, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-130", "asctime": "22:36:39"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr...\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-80a04c13-e9a3-478e-b0b2-6a18f380b721', created=1749488799, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54fd70>, stream = False\ndata = {'messages': [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on t...tFormat>str</OutputFormat>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr...\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'dedb4a62-c8f0-4613-a6ba-d4589dae0e57', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54fd70>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on the user\\'s pr...\\n  x value: [1,2,3,4,5]\\n  y value: [2,4,6,8,10]\\n  </Prompt>\\n\\n\\n<OutputFormat>str</OutputFormat>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-80a04c13-e9a3-478e-b0b2-6a18f380b721', created=1749488799, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54fd70>, stream = False\ndata = {'messages': [{'content': '\\nYou are an experienced Python programmer. Your task is to generate Python code based on t...tFormat>str</OutputFormat>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_data_visualization(settings: Any) -> None:\n        \"\"\"\n        Test the generate_code method's ability to generate code that performs data visualization.\n    \n        The function instructs the generate_code method to create code that plots a given set of x and y values.\n        The generated code is then printed to the console.\n        \"\"\"\n        input_message = \"\"\"\n      Plot below numbers\n      x value: [1,2,3,4,5]\n      y value: [2,4,6,8,10]\n      \"\"\"\n        only_python = OnlyPython(with_model=settings.with_model)\n>       output = await only_python.generate_code(input_message, \"\", False, True)\n\ntests/test_only_python.py:137: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_python.py:180: in generate_code\n    code = await self.make_llm_call(system_prompt, message)\nsrc/elevate/only_python.py:56: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'dedb4a62-c8f0-4613-a6ba-d4589dae0e57', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54fd70>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003175000019837171, "outcome": "passed"}}, {"nodeid": "tests/test_only_python.py::test_only_email_code_generation", "lineno": 142, "outcome": "passed", "keywords": ["test_only_email_code_generation", "asyncio", "pytestmark", "test_only_python.py", "tests", "elevate", ""], "setup": {"duration": 0.00019404198974370956, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488799.292384, "msecs": 292.0, "relativeCreated": 13891.587972640991, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.8798877080262173, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f14a480>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c96a0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'982'), (b'x-ratelimit-remaining-tokens', b'3925'), (b'x-ratelimit-reset-requests', b'25m53.309999999s'), (b'x-ratelimit-reset-tokens', b'40.374s'), (b'x-request-id', b'req_01jxath3nsfpkr9zs8pc13y2pt'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238843cdccde3-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-c6d5d34c-d5d2-4409-91fa-a86c30b913df\", \"object\": \"chat.completion\", \"created\": 1749488799, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Happy Wedding Anniversary to a Wonderful Couple\\n\\nDear John and Jane,\\n\\nAs you celebrate another year of love, laughter, and adventure together, I wanted to take a moment to express my warmest congratulations and best wishes on your wedding anniversary. Your bond is a true inspiration to all those around you, and I feel so grateful to have witnessed your journey together.\\n\\nOver the years, I've had the pleasure of seeing your love grow stronger with each passing day, and it's a joy to see the happiness you bring to each other's lives. Your relationship is a beautiful reminder that marriage is a journey, not a destination, and that the love, trust, and commitment you share are the keys to a lifelong partnership.\\n\\nHere's to many more years of happiness, joy, and making memories together! May your love continue to be the guiding force in your lives, and may you always find joy in each other's company.\\n\\nWith love and best wishes,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050112711, \"prompt_tokens\": 143, \"prompt_time\": 0.009606479, \"completion_tokens\": 197, \"completion_time\": 0.716363636, \"total_tokens\": 340, \"total_time\": 0.725970115}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath3nsfpkr9zs8pc13y2pt\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.437e-05, completion_tokens_cost_usd_dollar: 0.00015563\nDEBUG: response_cost: 0.00023999999999999998\nDEBUG: Subject: Happy Wedding Anniversary to a Wonderful Couple\n\nDear John and Jane,\n\nAs you celebrate another year of love, laughter, and adventure together, I wanted to take a moment to express my warmest congratulations and best wishes on your wedding anniversary. Your bond is a true inspiration to all those around you, and I feel so grateful to have witnessed your journey together.\n\nOver the years, I've had the pleasure of seeing your love grow stronger with each passing day, and it's a joy to see the happiness you bring to each other's lives. Your relationship is a beautiful reminder that marriage is a journey, not a destination, and that the love, trust, and commitment you share are the keys to a lifelong partnership.\n\nHere's to many more years of happiness, joy, and making memories together! May your love continue to be the guiding force in your lives, and may you always find joy in each other's company.\n\nWith love and best wishes,\n[Your Name]\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54d490>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:39 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], 'thinking': None}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:39 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-c6d5d34c-d5d2-4409-91fa-a86c30b913df\", \"object\": \"chat.completion\", \"created\": 1749488799, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Happy Wedding Anniversary to a Wonderful Couple\\n\\nDear John and Jane,\\n\\nAs you celebrate another year of love, laughter, and adventure together, I wanted to take a moment to express my warmest congratulations and best wishes on your wedding anniversary. Your bond is a true inspiration to all those around you, and I feel so grateful to have witnessed your journey together.\\n\\nOver the years, I've had the pleasure of seeing your love grow stronger with each passing day, and it's a joy to see the happiness you bring to each other's lives. Your relationship is a beautiful reminder that marriage is a journey, not a destination, and that the love, trust, and commitment you share are the keys to a lifelong partnership.\\n\\nHere's to many more years of happiness, joy, and making memories together! May your love continue to be the guiding force in your lives, and may you always find joy in each other's company.\\n\\nWith love and best wishes,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050112711, \"prompt_tokens\": 143, \"prompt_time\": 0.009606479, \"completion_tokens\": 197, \"completion_time\": 0.716363636, \"total_tokens\": 340, \"total_time\": 0.725970115}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath3nsfpkr9zs8pc13y2pt\"}}\n\n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.437e-05, completion_tokens_cost_usd_dollar: 0.00015563\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00023999999999999998\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54d490>>\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.292588, "msecs": 292.0, "relativeCreated": 13891.792058944702, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.2926269, "msecs": 292.0, "relativeCreated": 13891.830921173096, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.292646, "msecs": 292.0, "relativeCreated": 13891.849994659424, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.292662, "msecs": 292.0, "relativeCreated": 13891.865968704224, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488799.292722, "msecs": 292.0, "relativeCreated": 13891.926050186157, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.292743, "msecs": 292.0, "relativeCreated": 13891.947031021118, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488799.2927601, "msecs": 292.0, "relativeCreated": 13891.964197158813, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488799.292972, "msecs": 292.0, "relativeCreated": 13892.176151275635, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488799.293032, "msecs": 293.0, "relativeCreated": 13892.23599433899, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488799.293073, "msecs": 293.0, "relativeCreated": 13892.277002334595, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488799.293101, "msecs": 293.0, "relativeCreated": 13892.305135726929, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488799.293139, "msecs": 293.0, "relativeCreated": 13892.343044281006, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert in crafting engaging and thoughtful personal emails. Your goal is to write a warm and friendly email that is tailored to the recipient and the specific context provided.  You must only output the complete email, including a subject line, salutation, body, and closing. Do not include any conversational elements or introductory phrases beyond the email itself.\\n\\n*OUTPUT:*\\nRespond *only* with the rephrased message, adhering to the specified instructions.\\n'}, {'role': 'user', 'content': '\\n    A wedding anuversary message to John and Jane.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488799.293186, "msecs": 293.0, "relativeCreated": 13892.390012741089, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:39"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.293449, "msecs": 293.0, "relativeCreated": 13892.652988433838, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f14a480>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.301737, "msecs": 301.0, "relativeCreated": 13900.941133499146, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.301766, "msecs": 301.0, "relativeCreated": 13900.969982147217, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5c96a0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.315685, "msecs": 315.0, "relativeCreated": 13914.889097213745, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.3157802, "msecs": 315.0, "relativeCreated": 13914.984226226807, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.315885, "msecs": 315.0, "relativeCreated": 13915.089130401611, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.3159082, "msecs": 315.0, "relativeCreated": 13915.112257003784, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.315978, "msecs": 315.0, "relativeCreated": 13915.182113647461, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488799.316, "msecs": 315.0, "relativeCreated": 13915.204048156738, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'982'), (b'x-ratelimit-remaining-tokens', b'3925'), (b'x-ratelimit-reset-requests', b'25m53.309999999s'), (b'x-ratelimit-reset-tokens', b'40.374s'), (b'x-request-id', b'req_01jxath3nsfpkr9zs8pc13y2pt'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238843cdccde3-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.168597, "msecs": 168.0, "relativeCreated": 14767.80104637146, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488800.1693099, "msecs": 169.0, "relativeCreated": 14768.513917922974, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.1695292, "msecs": 169.0, "relativeCreated": 14768.733263015747, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.169894, "msecs": 169.0, "relativeCreated": 14769.098043441772, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.1700218, "msecs": 170.0, "relativeCreated": 14769.22583580017, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.1701581, "msecs": 170.0, "relativeCreated": 14769.362211227417, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488800.170392, "msecs": 170.0, "relativeCreated": 14769.596099853516, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-c6d5d34c-d5d2-4409-91fa-a86c30b913df\", \"object\": \"chat.completion\", \"created\": 1749488799, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"Subject: Happy Wedding Anniversary to a Wonderful Couple\\n\\nDear John and Jane,\\n\\nAs you celebrate another year of love, laughter, and adventure together, I wanted to take a moment to express my warmest congratulations and best wishes on your wedding anniversary. Your bond is a true inspiration to all those around you, and I feel so grateful to have witnessed your journey together.\\n\\nOver the years, I've had the pleasure of seeing your love grow stronger with each passing day, and it's a joy to see the happiness you bring to each other's lives. Your relationship is a beautiful reminder that marriage is a journey, not a destination, and that the love, trust, and commitment you share are the keys to a lifelong partnership.\\n\\nHere's to many more years of happiness, joy, and making memories together! May your love continue to be the guiding force in your lives, and may you always find joy in each other's company.\\n\\nWith love and best wishes,\\n[Your Name]\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.050112711, \"prompt_tokens\": 143, \"prompt_time\": 0.009606479, \"completion_tokens\": 197, \"completion_time\": 0.716363636, \"total_tokens\": 340, \"total_time\": 0.725970115}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath3nsfpkr9zs8pc13y2pt\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.170656, "msecs": 170.0, "relativeCreated": 14769.860029220581, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488800.171073, "msecs": 171.0, "relativeCreated": 14770.27702331543, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488800.171341, "msecs": 171.0, "relativeCreated": 14770.54500579834, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.171488, "msecs": 171.0, "relativeCreated": 14770.692110061646, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.437e-05, completion_tokens_cost_usd_dollar: 0.00015563", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488800.171562, "msecs": 171.0, "relativeCreated": 14770.766019821167, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "response_cost: 0.00023999999999999998", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488800.171635, "msecs": 171.0, "relativeCreated": 14770.838975906372, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133", "asctime": "22:36:40"}, {"name": "root", "msg": "Subject: Happy Wedding Anniversary to a Wonderful Couple\n\nDear John and Jane,\n\nAs you celebrate another year of love, laughter, and adventure together, I wanted to take a moment to express my warmest congratulations and best wishes on your wedding anniversary. Your bond is a true inspiration to all those around you, and I feel so grateful to have witnessed your journey together.\n\nOver the years, I've had the pleasure of seeing your love grow stronger with each passing day, and it's a joy to see the happiness you bring to each other's lives. Your relationship is a beautiful reminder that marriage is a journey, not a destination, and that the love, trust, and commitment you share are the keys to a lifelong partnership.\n\nHere's to many more years of happiness, joy, and making memories together! May your love continue to be the guiding force in your lives, and may you always find joy in each other's company.\n\nWith love and best wishes,\n[Your Name]", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_python.py", "filename": "test_only_python.py", "module": "test_only_python", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 156, "funcName": "test_only_email_code_generation", "created": 1749488800.171725, "msecs": 171.0, "relativeCreated": 14770.929098129272, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-133"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54d490>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.171853, "msecs": 171.0, "relativeCreated": 14771.05712890625, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-135", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488800.171967, "msecs": 171.0, "relativeCreated": 14771.17109298706, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-135", "asctime": "22:36:40"}]}, "teardown": {"duration": 0.0022338330163620412, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.437e-05, completion_tokens_cost_usd_dollar: 0.00015563\nDEBUG: response_cost: 0.00023999999999999998\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00023999999999999998\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.437e-05, completion_tokens_cost_usd_dollar: 0.00015563\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00023999999999999998\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00023999999999999998\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.1729128, "msecs": 172.0, "relativeCreated": 14772.116899490356, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.173336, "msecs": 173.0, "relativeCreated": 14772.540092468262, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488800.173463, "msecs": 173.0, "relativeCreated": 14772.667169570923, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.1736782, "msecs": 173.0, "relativeCreated": 14772.882223129272, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488800.173766, "msecs": 173.0, "relativeCreated": 14772.969961166382, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.1738582, "msecs": 173.0, "relativeCreated": 14773.062229156494, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 8.437e-05, completion_tokens_cost_usd_dollar: 0.00015563", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488800.173929, "msecs": 173.0, "relativeCreated": 14773.133039474487, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "response_cost: 0.00023999999999999998", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488800.1740139, "msecs": 174.0, "relativeCreated": 14773.217916488647, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00023999999999999998", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488800.17408, "msecs": 174.0, "relativeCreated": 14773.283958435059, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.174228, "msecs": 174.0, "relativeCreated": 14773.43201637268, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488800.1743412, "msecs": 174.0, "relativeCreated": 14773.545265197754, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-136", "asctime": "22:36:40"}]}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_formal", "lineno": 36, "outcome": "failed", "keywords": ["test_rephase_text_formal", "asyncio", "pytestmark", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.0006306249997578561, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488800.1757948, "msecs": 175.0, "relativeCreated": 14774.998903274536, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.01244845800101757, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_rephrase.py", "lineno": 44, "message": ""}, {"path": "src/elevate/only_rephrase.py", "lineno": 97, "message": "in rephrase_text"}, {"path": "src/elevate/only_rephrase.py", "lineno": 41, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.176397, "msecs": 176.0, "relativeCreated": 14775.601148605347, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.176512, "msecs": 176.0, "relativeCreated": 14775.716066360474, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.176569, "msecs": 176.0, "relativeCreated": 14775.773048400879, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.176627, "msecs": 176.0, "relativeCreated": 14775.8309841156, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.176851, "msecs": 176.0, "relativeCreated": 14776.055097579956, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.176926, "msecs": 176.0, "relativeCreated": 14776.129961013794, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488800.176995, "msecs": 176.0, "relativeCreated": 14776.199102401733, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488800.177652, "msecs": 177.0, "relativeCreated": 14776.855945587158, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488800.177817, "msecs": 177.0, "relativeCreated": 14777.021169662476, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488800.177923, "msecs": 177.0, "relativeCreated": 14777.127027511597, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.178026, "msecs": 178.0, "relativeCreated": 14777.230024337769, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.178106, "msecs": 178.0, "relativeCreated": 14777.310132980347, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488800.178276, "msecs": 178.0, "relativeCreated": 14777.480125427246, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.179619, "msecs": 179.0, "relativeCreated": 14778.823137283325, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.180115, "msecs": 180.0, "relativeCreated": 14779.319047927856, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.1801941, "msecs": 180.0, "relativeCreated": 14779.398202896118, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.180421, "msecs": 180.0, "relativeCreated": 14779.625177383423, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.180466, "msecs": 180.0, "relativeCreated": 14779.670000076294, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.180599, "msecs": 180.0, "relativeCreated": 14779.803037643433, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.180649, "msecs": 180.0, "relativeCreated": 14779.853105545044, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.18081, "msecs": 180.0, "relativeCreated": 14780.014038085938, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488800.180906, "msecs": 180.0, "relativeCreated": 14780.110120773315, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488800.181319, "msecs": 181.0, "relativeCreated": 14780.52306175232, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488800.184681, "msecs": 184.0, "relativeCreated": 14783.88500213623, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-138", "asctime": "22:36:40"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-0e966e04-4245-4ef7-aacb-50d5b7449da5', created=1749488800, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5be6c0>, stream = False\ndata = {'messages': [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is...<Length> lengthy </Length>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = '', model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'df3a3fa2-c610-4eb3-a1ad-9b9595bf9147', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5be6c0>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> formal </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-0e966e04-4245-4ef7-aacb-50d5b7449da5', created=1749488800, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5be6c0>, stream = False\ndata = {'messages': [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is...<Length> lengthy </Length>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_rephase_text_formal(settings: Any) -> None:\n        \"\"\"Test the rephrase_text method with a formal style.\"\"\"\n        input_message = \"\"\"\n        I need 2 days of sick leave.\n        \"\"\"\n        only_rephrase = OnlyRephrase(with_model=settings.with_model)\n>       rephrased_text = await only_rephrase.rephrase_text(input_message, \"formal\", \"lengthy\")\n\ntests/test_only_rephrase.py:44: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_rephrase.py:97: in rephrase_text\n    return await self.make_llm_call(system_prompt, message)\nsrc/elevate/only_rephrase.py:41: in make_llm_call\n    response = await acompletion(api_key=\"\", model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'df3a3fa2-c610-4eb3-a1ad-9b9595bf9147', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10e5be6c0>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.000326832989230752, "outcome": "passed"}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_informal", "lineno": 47, "outcome": "passed", "keywords": ["test_rephase_text_informal", "asyncio", "pytestmark", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.00021229198318906128, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488800.348165, "msecs": 348.0, "relativeCreated": 14947.36909866333, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.22002783298376016, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12819c860>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5835f0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'980'), (b'x-ratelimit-remaining-tokens', b'3320'), (b'x-ratelimit-reset-requests', b'28m47.806s'), (b'x-ratelimit-reset-tokens', b'43.397s'), (b'x-request-id', b'req_01jxath4prehsv93tw3jwfe989'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2388adb96492a-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-3f68c7fa-78f4-44aa-a097-bd02327a46f5\", \"object\": \"chat.completion\", \"created\": 1749488800, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I'm out for 2 days, need some sick leave.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049786275, \"prompt_tokens\": 563, \"prompt_time\": 0.046095035, \"completion_tokens\": 14, \"completion_time\": 0.075695248, \"total_tokens\": 577, \"total_time\": 0.121790283}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath4prehsv93tw3jwfe989\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033216999999999996, completion_tokens_cost_usd_dollar: 1.1059999999999999e-05\nDEBUG: response_cost: 0.00034323\nDEBUG: Informal Rephrase:\nModelResponse(id='chatcmpl-3f68c7fa-78f4-44aa-a097-bd02327a46f5', created=1749488800, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_3f3b593e33', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm out for 2 days, need some sick leave.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=14, prompt_tokens=563, total_tokens=577, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.049786275, prompt_time=0.046095035, completion_time=0.075695248, total_time=0.121790283), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath4prehsv93tw3jwfe989'})\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5824b0>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], 'thinking': None}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-3f68c7fa-78f4-44aa-a097-bd02327a46f5\", \"object\": \"chat.completion\", \"created\": 1749488800, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I'm out for 2 days, need some sick leave.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049786275, \"prompt_tokens\": 563, \"prompt_time\": 0.046095035, \"completion_tokens\": 14, \"completion_time\": 0.075695248, \"total_tokens\": 577, \"total_time\": 0.121790283}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath4prehsv93tw3jwfe989\"}}\n\n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033216999999999996, completion_tokens_cost_usd_dollar: 1.1059999999999999e-05\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00034323\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5824b0>>\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.3484192, "msecs": 348.0, "relativeCreated": 14947.623252868652, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.34847, "msecs": 348.0, "relativeCreated": 14947.674036026001, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.348497, "msecs": 348.0, "relativeCreated": 14947.70097732544, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.348521, "msecs": 348.0, "relativeCreated": 14947.725057601929, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.348595, "msecs": 348.0, "relativeCreated": 14947.79896736145, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.348623, "msecs": 348.0, "relativeCreated": 14947.827100753784, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488800.3486452, "msecs": 348.0, "relativeCreated": 14947.84927368164, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488800.3488798, "msecs": 348.0, "relativeCreated": 14948.083877563477, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488800.348932, "msecs": 348.0, "relativeCreated": 14948.1360912323, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488800.348979, "msecs": 348.0, "relativeCreated": 14948.183059692383, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.3490062, "msecs": 349.0, "relativeCreated": 14948.2102394104, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.349031, "msecs": 349.0, "relativeCreated": 14948.235034942627, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I need 2 days of sick leave.\\n    </Message>\\n\\n<Tone> informal </Tone>\\n\\n<Length> short </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488800.3490849, "msecs": 349.0, "relativeCreated": 14948.288917541504, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.349371, "msecs": 349.0, "relativeCreated": 14948.575019836426, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12819c860>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.356734, "msecs": 356.0, "relativeCreated": 14955.93810081482, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.3567948, "msecs": 356.0, "relativeCreated": 14955.99889755249, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f5835f0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.369993, "msecs": 369.0, "relativeCreated": 14969.197034835815, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.370104, "msecs": 370.0, "relativeCreated": 14969.308137893677, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.370207, "msecs": 370.0, "relativeCreated": 14969.411134719849, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.3702312, "msecs": 370.0, "relativeCreated": 14969.435214996338, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.3702981, "msecs": 370.0, "relativeCreated": 14969.502210617065, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.370312, "msecs": 370.0, "relativeCreated": 14969.516038894653, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:40 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'980'), (b'x-ratelimit-remaining-tokens', b'3320'), (b'x-ratelimit-reset-requests', b'28m47.806s'), (b'x-ratelimit-reset-tokens', b'43.397s'), (b'x-request-id', b'req_01jxath4prehsv93tw3jwfe989'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2388adb96492a-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.564827, "msecs": 564.0, "relativeCreated": 15164.031028747559, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488800.565397, "msecs": 565.0, "relativeCreated": 15164.60108757019, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.5656052, "msecs": 565.0, "relativeCreated": 15164.809226989746, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.565879, "msecs": 565.0, "relativeCreated": 15165.083169937134, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.565993, "msecs": 565.0, "relativeCreated": 15165.197134017944, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.566114, "msecs": 566.0, "relativeCreated": 15165.318012237549, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488800.566355, "msecs": 566.0, "relativeCreated": 15165.55905342102, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-3f68c7fa-78f4-44aa-a097-bd02327a46f5\", \"object\": \"chat.completion\", \"created\": 1749488800, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I'm out for 2 days, need some sick leave.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049786275, \"prompt_tokens\": 563, \"prompt_time\": 0.046095035, \"completion_tokens\": 14, \"completion_time\": 0.075695248, \"total_tokens\": 577, \"total_time\": 0.121790283}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath4prehsv93tw3jwfe989\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.566637, "msecs": 566.0, "relativeCreated": 15165.841102600098, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488800.567059, "msecs": 567.0, "relativeCreated": 15166.263103485107, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488800.5673118, "msecs": 567.0, "relativeCreated": 15166.515827178955, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.567447, "msecs": 567.0, "relativeCreated": 15166.651010513306, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033216999999999996, completion_tokens_cost_usd_dollar: 1.1059999999999999e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488800.567512, "msecs": 567.0, "relativeCreated": 15166.7160987854, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "response_cost: 0.00034323", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488800.567616, "msecs": 567.0, "relativeCreated": 15166.820049285889, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141", "asctime": "22:36:40"}, {"name": "root", "msg": "Informal Rephrase:\nModelResponse(id='chatcmpl-3f68c7fa-78f4-44aa-a097-bd02327a46f5', created=1749488800, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_3f3b593e33', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'm out for 2 days, need some sick leave.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=14, prompt_tokens=563, total_tokens=577, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.049786275, prompt_time=0.046095035, completion_time=0.075695248, total_time=0.121790283), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath4prehsv93tw3jwfe989'})", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_rephrase.py", "filename": "test_only_rephrase.py", "module": "test_only_rephrase", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 56, "funcName": "test_rephase_text_informal", "created": 1749488800.5677729, "msecs": 567.0, "relativeCreated": 15166.976928710938, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-141"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5824b0>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.5678499, "msecs": 567.0, "relativeCreated": 15167.053937911987, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-143", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488800.567897, "msecs": 567.0, "relativeCreated": 15167.10114479065, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-143", "asctime": "22:36:40"}]}, "teardown": {"duration": 0.001958625012775883, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033216999999999996, completion_tokens_cost_usd_dollar: 1.1059999999999999e-05\nDEBUG: response_cost: 0.00034323\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00034323\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033216999999999996, completion_tokens_cost_usd_dollar: 1.1059999999999999e-05\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00034323\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00034323\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.568827, "msecs": 568.0, "relativeCreated": 15168.030977249146, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.569173, "msecs": 569.0, "relativeCreated": 15168.377161026001, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488800.5692768, "msecs": 569.0, "relativeCreated": 15168.48087310791, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.569437, "msecs": 569.0, "relativeCreated": 15168.641090393066, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488800.569508, "msecs": 569.0, "relativeCreated": 15168.712139129639, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.5695932, "msecs": 569.0, "relativeCreated": 15168.797254562378, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033216999999999996, completion_tokens_cost_usd_dollar: 1.1059999999999999e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488800.569648, "msecs": 569.0, "relativeCreated": 15168.852090835571, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "response_cost: 0.00034323", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488800.5697112, "msecs": 569.0, "relativeCreated": 15168.915271759033, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00034323", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488800.569758, "msecs": 569.0, "relativeCreated": 15168.962001800537, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488800.5698931, "msecs": 569.0, "relativeCreated": 15169.097185134888, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488800.569974, "msecs": 569.0, "relativeCreated": 15169.178009033203, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-144", "asctime": "22:36:40"}]}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_urgent", "lineno": 58, "outcome": "failed", "keywords": ["test_rephase_text_urgent", "asyncio", "pytestmark", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.000635249976767227, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488800.571334, "msecs": 571.0, "relativeCreated": 15170.537948608398, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.011248916998738423, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_rephrase.py", "lineno": 66, "message": ""}, {"path": "src/elevate/only_rephrase.py", "lineno": 97, "message": "in rephrase_text"}, {"path": "src/elevate/only_rephrase.py", "lineno": 41, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.571913, "msecs": 571.0, "relativeCreated": 15171.117067337036, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.572006, "msecs": 572.0, "relativeCreated": 15171.210050582886, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.572048, "msecs": 572.0, "relativeCreated": 15171.252012252808, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.5720918, "msecs": 572.0, "relativeCreated": 15171.295881271362, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.572288, "msecs": 572.0, "relativeCreated": 15171.492099761963, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.5723379, "msecs": 572.0, "relativeCreated": 15171.541929244995, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488800.572384, "msecs": 572.0, "relativeCreated": 15171.58818244934, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488800.573123, "msecs": 573.0, "relativeCreated": 15172.327041625977, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488800.573259, "msecs": 573.0, "relativeCreated": 15172.463178634644, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488800.573354, "msecs": 573.0, "relativeCreated": 15172.558069229126, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.573439, "msecs": 573.0, "relativeCreated": 15172.642946243286, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.573519, "msecs": 573.0, "relativeCreated": 15172.723054885864, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    This is a serious problem that needs to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488800.5736918, "msecs": 573.0, "relativeCreated": 15172.895908355713, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.574681, "msecs": 574.0, "relativeCreated": 15173.885107040405, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.57512, "msecs": 575.0, "relativeCreated": 15174.324035644531, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.5751958, "msecs": 575.0, "relativeCreated": 15174.399852752686, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.575427, "msecs": 575.0, "relativeCreated": 15174.631118774414, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.575472, "msecs": 575.0, "relativeCreated": 15174.676179885864, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.5755892, "msecs": 575.0, "relativeCreated": 15174.793243408203, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.575637, "msecs": 575.0, "relativeCreated": 15174.841165542603, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.575785, "msecs": 575.0, "relativeCreated": 15174.988985061646, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488800.575885, "msecs": 575.0, "relativeCreated": 15175.089120864868, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488800.576253, "msecs": 576.0, "relativeCreated": 15175.457000732422, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488800.579531, "msecs": 579.0, "relativeCreated": 15178.73501777649, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-146", "asctime": "22:36:40"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-2ee44a19-4e33-4f33-ad01-7a982b0260ec', created=1749488800, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f581e50>, stream = False\ndata = {'messages': [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is...<Length> lengthy </Length>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = '', model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'acbf1362-54d7-4565-8ca7-58f3bfa86e8e', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f581e50>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... to be addressed immediately.\\n    </Message>\\n\\n<Tone> urgent </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-2ee44a19-4e33-4f33-ad01-7a982b0260ec', created=1749488800, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f581e50>, stream = False\ndata = {'messages': [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is...<Length> lengthy </Length>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_rephase_text_urgent(settings: Any) -> None:\n        \"\"\"Test the rephrase_text method with an urgent style.\"\"\"\n        input_message = \"\"\"\n        This is a serious problem that needs to be addressed immediately.\n        \"\"\"\n        only_rephrase = OnlyRephrase(with_model=settings.with_model)\n>       rephrased_text = await only_rephrase.rephrase_text(input_message, \"urgent\", \"lengthy\")\n\ntests/test_only_rephrase.py:66: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_rephrase.py:97: in rephrase_text\n    return await self.make_llm_call(system_prompt, message)\nsrc/elevate/only_rephrase.py:41: in make_llm_call\n    response = await acompletion(api_key=\"\", model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'acbf1362-54d7-4565-8ca7-58f3bfa86e8e', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f581e50>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.00032812499557621777, "outcome": "passed"}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_enthusiastic", "lineno": 69, "outcome": "passed", "keywords": ["test_rephase_text_enthusiastic", "asyncio", "pytestmark", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.00021208301768638194, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488800.740572, "msecs": 740.0, "relativeCreated": 15339.776039123535, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.32751787500455976, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x128141c10>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x128141af0>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'979'), (b'x-ratelimit-remaining-tokens', b'2818'), (b'x-ratelimit-reset-requests', b'30m14.007s'), (b'x-ratelimit-reset-tokens', b'45.906s'), (b'x-request-id', b'req_01jxath531fpm9yveqzxtz1b7p'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2388d4d104720-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-fafdb837-19f2-4dcb-a719-1fe355028e99\", \"object\": \"chat.completion\", \"created\": 1749488800, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I am absolutely thrilled about this project, and I just can't wait to see it come to life and witness the incredible impact it's going to have.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05093178599999999, \"prompt_tokens\": 562, \"prompt_time\": 0.074658344, \"completion_tokens\": 32, \"completion_time\": 0.151697055, \"total_tokens\": 594, \"total_time\": 0.226355399}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath531fpm9yveqzxtz1b7p\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033158, completion_tokens_cost_usd_dollar: 2.528e-05\nDEBUG: response_cost: 0.00035685999999999997\nDEBUG: Enthusiastic Rephrase:\nModelResponse(id='chatcmpl-fafdb837-19f2-4dcb-a719-1fe355028e99', created=1749488800, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_3f3b593e33', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I am absolutely thrilled about this project, and I just can't wait to see it come to life and witness the incredible impact it's going to have.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=32, prompt_tokens=562, total_tokens=594, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.05093178599999999, prompt_time=0.074658344, completion_time=0.151697055, total_time=0.226355399), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath531fpm9yveqzxtz1b7p'})\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128142ff0>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:40 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], 'thinking': None}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-fafdb837-19f2-4dcb-a719-1fe355028e99\", \"object\": \"chat.completion\", \"created\": 1749488800, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I am absolutely thrilled about this project, and I just can't wait to see it come to life and witness the incredible impact it's going to have.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05093178599999999, \"prompt_tokens\": 562, \"prompt_time\": 0.074658344, \"completion_tokens\": 32, \"completion_time\": 0.151697055, \"total_tokens\": 594, \"total_time\": 0.226355399}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath531fpm9yveqzxtz1b7p\"}}\n\n\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033158, completion_tokens_cost_usd_dollar: 2.528e-05\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00035685999999999997\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128142ff0>>\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.7408068, "msecs": 740.0, "relativeCreated": 15340.01088142395, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.740848, "msecs": 740.0, "relativeCreated": 15340.052127838135, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.740868, "msecs": 740.0, "relativeCreated": 15340.07215499878, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.740886, "msecs": 740.0, "relativeCreated": 15340.090036392212, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.740948, "msecs": 740.0, "relativeCreated": 15340.152025222778, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.74097, "msecs": 740.0, "relativeCreated": 15340.173959732056, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488800.7409868, "msecs": 740.0, "relativeCreated": 15340.190887451172, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488800.741205, "msecs": 741.0, "relativeCreated": 15340.40904045105, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488800.741267, "msecs": 741.0, "relativeCreated": 15340.471029281616, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488800.741305, "msecs": 741.0, "relativeCreated": 15340.509176254272, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488800.741334, "msecs": 741.0, "relativeCreated": 15340.538024902344, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488800.741364, "msecs": 741.0, "relativeCreated": 15340.56806564331, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': \"\\n<Message>\\n    I'm so excited about this project!\\n    </Message>\\n\\n<Tone> enthusiastic </Tone>\\n\\n<Length> medium </Length>\"}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488800.741418, "msecs": 741.0, "relativeCreated": 15340.621948242188, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:40"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.7416902, "msecs": 741.0, "relativeCreated": 15340.894222259521, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x128141c10>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.749581, "msecs": 749.0, "relativeCreated": 15348.785161972046, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.7496161, "msecs": 749.0, "relativeCreated": 15348.820209503174, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x128141af0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.7602692, "msecs": 760.0, "relativeCreated": 15359.47322845459, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.76036, "msecs": 760.0, "relativeCreated": 15359.564065933228, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.760457, "msecs": 760.0, "relativeCreated": 15359.661102294922, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.760475, "msecs": 760.0, "relativeCreated": 15359.678983688354, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.7605429, "msecs": 760.0, "relativeCreated": 15359.746932983398, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488800.760559, "msecs": 760.0, "relativeCreated": 15359.763145446777, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:41 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'979'), (b'x-ratelimit-remaining-tokens', b'2818'), (b'x-ratelimit-reset-requests', b'30m14.007s'), (b'x-ratelimit-reset-tokens', b'45.906s'), (b'x-request-id', b'req_01jxath531fpm9yveqzxtz1b7p'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2388d4d104720-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.064532, "msecs": 64.0, "relativeCreated": 15663.73610496521, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488801.065266, "msecs": 65.0, "relativeCreated": 15664.469957351685, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.06548, "msecs": 65.0, "relativeCreated": 15664.684057235718, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.0657358, "msecs": 65.0, "relativeCreated": 15664.939880371094, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.065844, "msecs": 65.0, "relativeCreated": 15665.048122406006, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.065987, "msecs": 65.0, "relativeCreated": 15665.191173553467, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488801.066251, "msecs": 66.0, "relativeCreated": 15665.455102920532, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-fafdb837-19f2-4dcb-a719-1fe355028e99\", \"object\": \"chat.completion\", \"created\": 1749488800, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I am absolutely thrilled about this project, and I just can't wait to see it come to life and witness the incredible impact it's going to have.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05093178599999999, \"prompt_tokens\": 562, \"prompt_time\": 0.074658344, \"completion_tokens\": 32, \"completion_time\": 0.151697055, \"total_tokens\": 594, \"total_time\": 0.226355399}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath531fpm9yveqzxtz1b7p\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.06651, "msecs": 66.0, "relativeCreated": 15665.714025497437, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488801.066893, "msecs": 66.0, "relativeCreated": 15666.097164154053, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488801.067183, "msecs": 67.0, "relativeCreated": 15666.38708114624, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488801.067331, "msecs": 67.0, "relativeCreated": 15666.535139083862, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033158, completion_tokens_cost_usd_dollar: 2.528e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488801.067429, "msecs": 67.0, "relativeCreated": 15666.633129119873, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "response_cost: 0.00035685999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488801.067505, "msecs": 67.0, "relativeCreated": 15666.708946228027, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149", "asctime": "22:36:41"}, {"name": "root", "msg": "Enthusiastic Rephrase:\nModelResponse(id='chatcmpl-fafdb837-19f2-4dcb-a719-1fe355028e99', created=1749488800, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_3f3b593e33', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I am absolutely thrilled about this project, and I just can't wait to see it come to life and witness the incredible impact it's going to have.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=32, prompt_tokens=562, total_tokens=594, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.05093178599999999, prompt_time=0.074658344, completion_time=0.151697055, total_time=0.226355399), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath531fpm9yveqzxtz1b7p'})", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_rephrase.py", "filename": "test_only_rephrase.py", "module": "test_only_rephrase", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 78, "funcName": "test_rephase_text_enthusiastic", "created": 1749488801.067671, "msecs": 67.0, "relativeCreated": 15666.875123977661, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-149"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128142ff0>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.067759, "msecs": 67.0, "relativeCreated": 15666.96310043335, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-151", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488801.0678122, "msecs": 67.0, "relativeCreated": 15667.01626777649, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-151", "asctime": "22:36:41"}]}, "teardown": {"duration": 0.002431542001431808, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033158, completion_tokens_cost_usd_dollar: 2.528e-05\nDEBUG: response_cost: 0.00035685999999999997\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00035685999999999997\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:41 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033158, completion_tokens_cost_usd_dollar: 2.528e-05\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00035685999999999997\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00035685999999999997\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.0687518, "msecs": 68.0, "relativeCreated": 15667.955875396729, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488801.0691872, "msecs": 69.0, "relativeCreated": 15668.391227722168, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488801.0693681, "msecs": 69.0, "relativeCreated": 15668.572187423706, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.069587, "msecs": 69.0, "relativeCreated": 15668.791055679321, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488801.069699, "msecs": 69.0, "relativeCreated": 15668.903112411499, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488801.069825, "msecs": 69.0, "relativeCreated": 15669.028997421265, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00033158, completion_tokens_cost_usd_dollar: 2.528e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488801.069908, "msecs": 69.0, "relativeCreated": 15669.111967086792, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "response_cost: 0.00035685999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488801.0700002, "msecs": 70.0, "relativeCreated": 15669.204235076904, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00035685999999999997", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488801.070057, "msecs": 70.0, "relativeCreated": 15669.26097869873, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488801.0702279, "msecs": 70.0, "relativeCreated": 15669.431924819946, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488801.070314, "msecs": 70.0, "relativeCreated": 15669.517993927002, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-152", "asctime": "22:36:41"}]}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_informative", "lineno": 80, "outcome": "failed", "keywords": ["test_rephase_text_informative", "asyncio", "pytestmark", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.0006456249975599349, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488801.071656, "msecs": 71.0, "relativeCreated": 15670.860052108765, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.0111865829967428, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_rephrase.py", "lineno": 88, "message": ""}, {"path": "src/elevate/only_rephrase.py", "lineno": 97, "message": "in rephrase_text"}, {"path": "src/elevate/only_rephrase.py", "lineno": 41, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:41 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.0722501, "msecs": 72.0, "relativeCreated": 15671.454191207886, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.0723681, "msecs": 72.0, "relativeCreated": 15671.572208404541, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.072421, "msecs": 72.0, "relativeCreated": 15671.625137329102, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.07247, "msecs": 72.0, "relativeCreated": 15671.674013137817, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488801.0726552, "msecs": 72.0, "relativeCreated": 15671.85926437378, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.072715, "msecs": 72.0, "relativeCreated": 15671.919107437134, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488801.0727649, "msecs": 72.0, "relativeCreated": 15671.968936920166, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488801.0734668, "msecs": 73.0, "relativeCreated": 15672.670841217041, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488801.073571, "msecs": 73.0, "relativeCreated": 15672.775030136108, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488801.073646, "msecs": 73.0, "relativeCreated": 15672.850131988525, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.073715, "msecs": 73.0, "relativeCreated": 15672.919034957886, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488801.0737789, "msecs": 73.0, "relativeCreated": 15672.982931137085, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    The system utilizes a multi-threaded architecture to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488801.073929, "msecs": 73.0, "relativeCreated": 15673.133134841919, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.074795, "msecs": 74.0, "relativeCreated": 15673.999071121216, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.075179, "msecs": 75.0, "relativeCreated": 15674.383163452148, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.075234, "msecs": 75.0, "relativeCreated": 15674.437999725342, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.075499, "msecs": 75.0, "relativeCreated": 15674.703121185303, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.0755782, "msecs": 75.0, "relativeCreated": 15674.782276153564, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.075804, "msecs": 75.0, "relativeCreated": 15675.008058547974, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.075881, "msecs": 75.0, "relativeCreated": 15675.085067749023, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.07609, "msecs": 76.0, "relativeCreated": 15675.294160842896, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488801.07619, "msecs": 76.0, "relativeCreated": 15675.394058227539, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488801.0766358, "msecs": 76.0, "relativeCreated": 15675.839900970459, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488801.079582, "msecs": 79.0, "relativeCreated": 15678.786039352417, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-154", "asctime": "22:36:41"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-38439e7e-baaf-4663-94d4-472cd2bb380f', created=1749488801, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128142750>, stream = False\ndata = {'messages': [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is...<Length> lengthy </Length>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = '', model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '33aeb2c1-21dd-4cc4-83e5-cb6465543ab3', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128142750>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase ... to improve performance.\\n    </Message>\\n\\n<Tone> informative </Tone>\\n\\n<Length> lengthy </Length>', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-38439e7e-baaf-4663-94d4-472cd2bb380f', created=1749488801, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128142750>, stream = False\ndata = {'messages': [{'content': '\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is...<Length> lengthy </Length>', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_rephase_text_informative(settings: Any) -> None:\n        \"\"\"Test the rephrase_text method with an informative style.\"\"\"\n        input_message = \"\"\"\n        The system utilizes a multi-threaded architecture to improve performance.\n        \"\"\"\n        only_rephrase = OnlyRephrase(with_model=settings.with_model)\n>       rephrased_text = await only_rephrase.rephrase_text(input_message, \"informative\", \"lengthy\")\n\ntests/test_only_rephrase.py:88: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_rephrase.py:97: in rephrase_text\n    return await self.make_llm_call(system_prompt, message)\nsrc/elevate/only_rephrase.py:41: in make_llm_call\n    response = await acompletion(api_key=\"\", model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '33aeb2c1-21dd-4cc4-83e5-cb6465543ab3', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x128142750>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003139589971397072, "outcome": "passed"}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_apologetic", "lineno": 91, "outcome": "passed", "keywords": ["test_rephase_text_apologetic", "asyncio", "pytestmark", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.0002055829972960055, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488801.240584, "msecs": 240.0, "relativeCreated": 15839.78796005249, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 1.0827371250197757, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281fe690>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281fe540>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'978'), (b'x-ratelimit-remaining-tokens', b'1571'), (b'x-ratelimit-reset-requests', b'31m40.294999999s'), (b'x-ratelimit-reset-tokens', b'52.141s'), (b'x-request-id', b'req_01jxath5jwehsv93f81fmhc9hy'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238906dc24055-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-3b3d5523-d1cb-480b-9cde-61f8e2de845d\", \"object\": \"chat.completion\", \"created\": 1749488801, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I want to start by offering my sincerest apologies for the delay in my response, as I understand that it has likely caused a significant amount of inconvenience for you, and for that, I am truly sorry. I can only imagine how frustrating it must be to have to wait for a response, and I regret that I was not able to provide it to you in a more timely manner. Please know that I take full responsibility for the delay and assure you that it was not a result of a lack of importance or priority, but rather an unfortunate circumstance that was beyond my control. I want to assure you that I am committed to making it right and providing you with the attention and response you deserve. Once again, I offer my deepest apologies for the delay and any inconvenience it may have caused, and I hope that you will be able to accept my sincerest regret for the situation. I am dedicated to preventing such delays in the future and ensuring that our communication is prompt and efficient.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05003182500000001, \"prompt_tokens\": 579, \"prompt_time\": 0.042404225, \"completion_tokens\": 198, \"completion_time\": 0.834982249, \"total_tokens\": 777, \"total_time\": 0.877386474}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath5jwehsv93f81fmhc9hy\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00034161, completion_tokens_cost_usd_dollar: 0.00015642\nDEBUG: response_cost: 0.00049803\nDEBUG: Apologetic Rephrase:\nModelResponse(id='chatcmpl-3b3d5523-d1cb-480b-9cde-61f8e2de845d', created=1749488801, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_3f3b593e33', choices=[Choices(finish_reason='stop', index=0, message=Message(content='I want to start by offering my sincerest apologies for the delay in my response, as I understand that it has likely caused a significant amount of inconvenience for you, and for that, I am truly sorry. I can only imagine how frustrating it must be to have to wait for a response, and I regret that I was not able to provide it to you in a more timely manner. Please know that I take full responsibility for the delay and assure you that it was not a result of a lack of importance or priority, but rather an unfortunate circumstance that was beyond my control. I want to assure you that I am committed to making it right and providing you with the attention and response you deserve. Once again, I offer my deepest apologies for the delay and any inconvenience it may have caused, and I hope that you will be able to accept my sincerest regret for the situation. I am dedicated to preventing such delays in the future and ensuring that our communication is prompt and efficient.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=198, prompt_tokens=579, total_tokens=777, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.05003182500000001, prompt_time=0.042404225, completion_time=0.834982249, total_time=0.877386474), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath5jwehsv93f81fmhc9hy'})\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fe120>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:41 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:41 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-3b3d5523-d1cb-480b-9cde-61f8e2de845d\", \"object\": \"chat.completion\", \"created\": 1749488801, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I want to start by offering my sincerest apologies for the delay in my response, as I understand that it has likely caused a significant amount of inconvenience for you, and for that, I am truly sorry. I can only imagine how frustrating it must be to have to wait for a response, and I regret that I was not able to provide it to you in a more timely manner. Please know that I take full responsibility for the delay and assure you that it was not a result of a lack of importance or priority, but rather an unfortunate circumstance that was beyond my control. I want to assure you that I am committed to making it right and providing you with the attention and response you deserve. Once again, I offer my deepest apologies for the delay and any inconvenience it may have caused, and I hope that you will be able to accept my sincerest regret for the situation. I am dedicated to preventing such delays in the future and ensuring that our communication is prompt and efficient.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05003182500000001, \"prompt_tokens\": 579, \"prompt_time\": 0.042404225, \"completion_tokens\": 198, \"completion_time\": 0.834982249, \"total_tokens\": 777, \"total_time\": 0.877386474}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath5jwehsv93f81fmhc9hy\"}}\n\n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00034161, completion_tokens_cost_usd_dollar: 0.00015642\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00049803\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fe120>>\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.240808, "msecs": 240.0, "relativeCreated": 15840.012073516846, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.2408478, "msecs": 240.0, "relativeCreated": 15840.051889419556, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.2408671, "msecs": 240.0, "relativeCreated": 15840.071201324463, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.2408829, "msecs": 240.0, "relativeCreated": 15840.086936950684, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488801.2409458, "msecs": 240.0, "relativeCreated": 15840.149879455566, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.2409658, "msecs": 240.0, "relativeCreated": 15840.169906616211, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488801.240983, "msecs": 240.0, "relativeCreated": 15840.187072753906, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488801.241219, "msecs": 241.0, "relativeCreated": 15840.423107147217, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488801.241288, "msecs": 241.0, "relativeCreated": 15840.492010116577, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488801.241343, "msecs": 241.0, "relativeCreated": 15840.54708480835, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488801.241368, "msecs": 241.0, "relativeCreated": 15840.572118759155, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488801.2413929, "msecs": 241.0, "relativeCreated": 15840.596914291382, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\nYou are a highly skilled expert in English grammar, style, and tone. Your primary task is to rephrase user-provided text to ensure it is grammatically correct, stylistically appropriate, and aligned with the specified tone and length.\\n\\n**INPUT**\\n\\nYou will receive input in the following format:\\n\\n\\n<Message>The text to be rephrased.</Message>\\n<Tone>The desired tone of the rephrased text (e.g., formal, informal, professional, friendly, humorous).</Tone>\\n<Length>The desired length of the rephrased text (e.g., short, medium, long). Consider the original message's length when interpreting this. 'Short' should be shorter than the original, 'Long' should be longer, and 'Medium' should be roughly the same length.</Length>\\n\\n\\n**INSTRUCTIONS**\\n\\n1. **Grammatical Correctness:** Ensure the rephrased text is free of grammatical errors, including subject-verb agreement, tense consistency, correct punctuation, and proper sentence structure.\\n2. **Stylistic Appropriateness:** Adjust the vocabulary and sentence structure to match the specified tone. For example, a formal tone should use sophisticated language and avoid contractions, while an informal tone can use simpler language and contractions.\\n3. **Length Adjustment:** Modify the text to fit the specified length. If 'short' is specified, condense the text while preserving the core meaning. If 'long' is specified, elaborate on the text, providing more detail or examples. If 'medium' is specified, maintain a length similar to the original text.\\n4. **Maintain Meaning:** Ensure the rephrased text retains the original meaning of the user-provided text. Do not add or remove information unless necessary to meet the length requirements.\\n5. **Clarity and Coherence:** The rephrased text should be clear and easy to understand. Use appropriate transitions to ensure smooth flow between sentences.\\n6. **Handle Ambiguity:** If the user-provided text is ambiguous, make a reasonable interpretation and rephrase accordingly. If necessary, add a brief clarification to the rephrased text.\\n7. **Preserve Intent:** Understand the user's intent and ensure the rephrased text aligns with that intent. Consider the context of the message and the user's goals.\\n\\n\\n**OUTPUT**\\n\\nProvide the rephrased text as a single string. Do not include any additional formatting or explanations.\\n\\n\"}, {'role': 'user', 'content': '\\n<Message>\\n    I apologize for the delay in my response. I understand that this has caused inconvenience, and I am truly sorry.\\n    </Message>\\n\\n<Tone> apologetic </Tone>\\n\\n<Length> lengthy </Length>'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488801.241442, "msecs": 241.0, "relativeCreated": 15840.646028518677, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:41"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.241714, "msecs": 241.0, "relativeCreated": 15840.918064117432, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281fe690>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.250313, "msecs": 250.0, "relativeCreated": 15849.517107009888, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.2503428, "msecs": 250.0, "relativeCreated": 15849.546909332275, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281fe540>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.2609532, "msecs": 260.0, "relativeCreated": 15860.157251358032, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.2610419, "msecs": 261.0, "relativeCreated": 15860.245943069458, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.261139, "msecs": 261.0, "relativeCreated": 15860.342979431152, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.261158, "msecs": 261.0, "relativeCreated": 15860.36205291748, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.261224, "msecs": 261.0, "relativeCreated": 15860.428094863892, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488801.261239, "msecs": 261.0, "relativeCreated": 15860.443115234375, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'978'), (b'x-ratelimit-remaining-tokens', b'1571'), (b'x-ratelimit-reset-requests', b'31m40.294999999s'), (b'x-ratelimit-reset-tokens', b'52.141s'), (b'x-request-id', b'req_01jxath5jwehsv93f81fmhc9hy'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238906dc24055-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.318882, "msecs": 318.0, "relativeCreated": 16918.086051940918, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488802.3196511, "msecs": 319.0, "relativeCreated": 16918.8551902771, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.32004, "msecs": 320.0, "relativeCreated": 16919.244050979614, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.32063, "msecs": 320.0, "relativeCreated": 16919.83413696289, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.320844, "msecs": 320.0, "relativeCreated": 16920.047998428345, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.3211148, "msecs": 321.0, "relativeCreated": 16920.318841934204, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488802.321426, "msecs": 321.0, "relativeCreated": 16920.62997817993, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-3b3d5523-d1cb-480b-9cde-61f8e2de845d\", \"object\": \"chat.completion\", \"created\": 1749488801, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"I want to start by offering my sincerest apologies for the delay in my response, as I understand that it has likely caused a significant amount of inconvenience for you, and for that, I am truly sorry. I can only imagine how frustrating it must be to have to wait for a response, and I regret that I was not able to provide it to you in a more timely manner. Please know that I take full responsibility for the delay and assure you that it was not a result of a lack of importance or priority, but rather an unfortunate circumstance that was beyond my control. I want to assure you that I am committed to making it right and providing you with the attention and response you deserve. Once again, I offer my deepest apologies for the delay and any inconvenience it may have caused, and I hope that you will be able to accept my sincerest regret for the situation. I am dedicated to preventing such delays in the future and ensuring that our communication is prompt and efficient.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05003182500000001, \"prompt_tokens\": 579, \"prompt_time\": 0.042404225, \"completion_tokens\": 198, \"completion_time\": 0.834982249, \"total_tokens\": 777, \"total_time\": 0.877386474}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_3f3b593e33\", \"x_groq\": {\"id\": \"req_01jxath5jwehsv93f81fmhc9hy\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.32169, "msecs": 321.0, "relativeCreated": 16920.894145965576, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488802.322109, "msecs": 322.0, "relativeCreated": 16921.313047409058, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488802.322413, "msecs": 322.0, "relativeCreated": 16921.617031097412, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.3225641, "msecs": 322.0, "relativeCreated": 16921.768188476562, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00034161, completion_tokens_cost_usd_dollar: 0.00015642", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488802.322657, "msecs": 322.0, "relativeCreated": 16921.861171722412, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "response_cost: 0.00049803", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488802.322731, "msecs": 322.0, "relativeCreated": 16921.935081481934, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157", "asctime": "22:36:42"}, {"name": "root", "msg": "Apologetic Rephrase:\nModelResponse(id='chatcmpl-3b3d5523-d1cb-480b-9cde-61f8e2de845d', created=1749488801, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_3f3b593e33', choices=[Choices(finish_reason='stop', index=0, message=Message(content='I want to start by offering my sincerest apologies for the delay in my response, as I understand that it has likely caused a significant amount of inconvenience for you, and for that, I am truly sorry. I can only imagine how frustrating it must be to have to wait for a response, and I regret that I was not able to provide it to you in a more timely manner. Please know that I take full responsibility for the delay and assure you that it was not a result of a lack of importance or priority, but rather an unfortunate circumstance that was beyond my control. I want to assure you that I am committed to making it right and providing you with the attention and response you deserve. Once again, I offer my deepest apologies for the delay and any inconvenience it may have caused, and I hope that you will be able to accept my sincerest regret for the situation. I am dedicated to preventing such delays in the future and ensuring that our communication is prompt and efficient.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=198, prompt_tokens=579, total_tokens=777, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.05003182500000001, prompt_time=0.042404225, completion_time=0.834982249, total_time=0.877386474), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath5jwehsv93f81fmhc9hy'})", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_rephrase.py", "filename": "test_only_rephrase.py", "module": "test_only_rephrase", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 100, "funcName": "test_rephase_text_apologetic", "created": 1749488802.322878, "msecs": 322.0, "relativeCreated": 16922.08194732666, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-157"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fe120>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.322978, "msecs": 322.0, "relativeCreated": 16922.182083129883, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-159", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488802.3230348, "msecs": 323.0, "relativeCreated": 16922.23882675171, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-159", "asctime": "22:36:42"}]}, "teardown": {"duration": 0.0017603749874979258, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00034161, completion_tokens_cost_usd_dollar: 0.00015642\nDEBUG: response_cost: 0.00049803\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00049803\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00034161, completion_tokens_cost_usd_dollar: 0.00015642\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00049803\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00049803\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.323909, "msecs": 323.0, "relativeCreated": 16923.113107681274, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.324214, "msecs": 324.0, "relativeCreated": 16923.418045043945, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488802.3242948, "msecs": 324.0, "relativeCreated": 16923.49886894226, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.32446, "msecs": 324.0, "relativeCreated": 16923.664093017578, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488802.324524, "msecs": 324.0, "relativeCreated": 16923.727989196777, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.324598, "msecs": 324.0, "relativeCreated": 16923.802137374878, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00034161, completion_tokens_cost_usd_dollar: 0.00015642", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488802.324647, "msecs": 324.0, "relativeCreated": 16923.851013183594, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "response_cost: 0.00049803", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488802.324702, "msecs": 324.0, "relativeCreated": 16923.906087875366, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00049803", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488802.3247418, "msecs": 324.0, "relativeCreated": 16923.945903778076, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.324852, "msecs": 324.0, "relativeCreated": 16924.05605316162, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488802.324912, "msecs": 324.0, "relativeCreated": 16924.116134643555, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-160", "asctime": "22:36:42"}]}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_friendly", "lineno": 102, "outcome": "passed", "keywords": ["test_rephase_text_friendly", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.00021887500770390034, "outcome": "passed"}, "call": {"duration": 0.0002629999944474548, "outcome": "passed", "stdout": "DEBUG: Friendly Rephrase:\n<coroutine object OnlyRephrase.rephrase_text at 0x10f1415d0>\n", "log": [{"name": "root", "msg": "Friendly Rephrase:\n<coroutine object OnlyRephrase.rephrase_text at 0x10f1415d0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_rephrase.py", "filename": "test_only_rephrase.py", "module": "test_only_rephrase", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 110, "funcName": "test_rephase_text_friendly", "created": 1749488802.3262231, "msecs": 326.0, "relativeCreated": 16925.427198410034, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "teardown": {"duration": 0.00013945900718681514, "outcome": "passed"}}, {"nodeid": "tests/test_only_rephrase.py::test_rephase_text_technical", "lineno": 112, "outcome": "passed", "keywords": ["test_rephase_text_technical", "test_only_rephrase.py", "tests", "elevate", ""], "setup": {"duration": 0.00013945900718681514, "outcome": "passed"}, "call": {"duration": 0.0002552499936427921, "outcome": "passed", "stdout": "DEBUG: Technical Rephrase:\n<coroutine object OnlyRephrase.rephrase_text at 0x10f140400>\n", "log": [{"name": "root", "msg": "Technical Rephrase:\n<coroutine object OnlyRephrase.rephrase_text at 0x10f140400>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_rephrase.py", "filename": "test_only_rephrase.py", "module": "test_only_rephrase", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 120, "funcName": "test_rephase_text_technical", "created": 1749488802.3270369, "msecs": 327.0, "relativeCreated": 16926.240921020508, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "teardown": {"duration": 0.0001310829829890281, "outcome": "passed"}}, {"nodeid": "tests/test_only_shell.py::test_simple_shell_command", "lineno": 36, "outcome": "failed", "keywords": ["test_simple_shell_command", "asyncio", "pytestmark", "test_only_shell.py", "tests", "elevate", ""], "setup": {"duration": 0.0006888339994475245, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488802.3280299, "msecs": 328.0, "relativeCreated": 16927.233934402466, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.009602874983102083, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_shell.py", "lineno": 51, "message": ""}, {"path": "src/elevate/only_shell.py", "lineno": 56, "message": "in generate_shell_command"}, {"path": "src/elevate/only_shell.py", "lineno": 40, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], 'thinking': None}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.328757, "msecs": 328.0, "relativeCreated": 16927.961111068726, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.328994, "msecs": 328.0, "relativeCreated": 16928.198099136353, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.329074, "msecs": 329.0, "relativeCreated": 16928.27796936035, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.3291311, "msecs": 329.0, "relativeCreated": 16928.335189819336, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488802.329275, "msecs": 329.0, "relativeCreated": 16928.478956222534, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.3293939, "msecs": 329.0, "relativeCreated": 16928.597927093506, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488802.329479, "msecs": 329.0, "relativeCreated": 16928.683042526245, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488802.3299809, "msecs": 329.0, "relativeCreated": 16929.184913635254, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488802.3300831, "msecs": 330.0, "relativeCreated": 16929.28719520569, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488802.33015, "msecs": 330.0, "relativeCreated": 16929.353952407837, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.330273, "msecs": 330.0, "relativeCreated": 16929.476976394653, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488802.330364, "msecs": 330.0, "relativeCreated": 16929.56805229187, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and efficient shell commands.\\n\\n*   **Input:** User\\'s description of a task (e.g., \"List all files in the current directory\").\\n*   **Output:** A single, executable shell command that accomplishes the task.  Do not include any introductory text, explanations, or apologies.  Focus solely on the command itself.  Assume a Linux/Unix environment.  Prioritize common and portable commands.  If the task is ambiguous, ask the user for clarification.\\n'}, {'role': 'user', 'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488802.3305018, "msecs": 330.0, "relativeCreated": 16929.70585823059, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.3312361, "msecs": 331.0, "relativeCreated": 16930.440187454224, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.331527, "msecs": 331.0, "relativeCreated": 16930.731058120728, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.3315752, "msecs": 331.0, "relativeCreated": 16930.779218673706, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.331724, "msecs": 331.0, "relativeCreated": 16930.927991867065, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.3317661, "msecs": 331.0, "relativeCreated": 16930.970191955566, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.331866, "msecs": 331.0, "relativeCreated": 16931.07008934021, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.331917, "msecs": 331.0, "relativeCreated": 16931.121110916138, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.331993, "msecs": 331.0, "relativeCreated": 16931.19716644287, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488802.332038, "msecs": 332.0, "relativeCreated": 16931.241989135742, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488802.3322, "msecs": 332.0, "relativeCreated": 16931.40411376953, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488802.3348758, "msecs": 334.0, "relativeCreated": 16934.079885482788, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-162", "asctime": "22:36:42"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and ef...m'}, {'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-45d214c9-6ab2-4d39-b3b1-9ff37f79897b', created=1749488802, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281ffb90>, stream = False\ndata = {'messages': [{'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into p...ry.\\n    </UserPrompt>\\n\\n', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and ef...m'}, {'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = '', model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '58b0e9a0-ec86-4d80-82e4-bda1dd6df018', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281ffb90>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into precise and ef...m'}, {'content': '\\n<UserPrompt>\\n    Command to list all files in directory.\\n    </UserPrompt>\\n\\n', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-45d214c9-6ab2-4d39-b3b1-9ff37f79897b', created=1749488802, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281ffb90>, stream = False\ndata = {'messages': [{'content': '\\nYou are an expert shell command generator. Your task is to translate user requests into p...ry.\\n    </UserPrompt>\\n\\n', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_simple_shell_command(settings: Any) -> None:\n        \"\"\"\n        Tests the generation of a simple shell command from a natural language description using the `OnlyShell` class.\n    \n        This function creates an `OnlyShell` instance, provides it with a textual description\n        of the desired shell command (listing files in a directory), generates the corresponding\n        shell command, and prints the generated command to the console.  It serves as a basic\n        example of how to use the `OnlyShell` class to translate natural language into shell commands.\n        \"\"\"\n        input_message = \"\"\"\n        Command to list all files in directory.\n        \"\"\"\n        only_shell = OnlyShell(with_model=settings.with_model)\n>       shell_command = await only_shell.generate_shell_command(input_message)\n\ntests/test_only_shell.py:51: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_shell.py:56: in generate_shell_command\n    return await self.make_llm_call(system_prompt, message)\nsrc/elevate/only_shell.py:40: in make_llm_call\n    response = await acompletion(api_key=\"\", model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': '', 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '58b0e9a0-ec86-4d80-82e4-bda1dd6df018', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281ffb90>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.00032479100627824664, "outcome": "passed"}}, {"nodeid": "tests/test_only_summary.py::test_simple_text_summary", "lineno": 36, "outcome": "passed", "keywords": ["test_simple_text_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.00021916598780080676, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488802.492234, "msecs": 492.0, "relativeCreated": 17091.438055038452, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.3421109589980915, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281e5e50>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281e5d30>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'977'), (b'x-ratelimit-remaining-tokens', b'1197'), (b'x-ratelimit-reset-requests', b'33m5.937s'), (b'x-ratelimit-reset-tokens', b'54.012s'), (b'x-request-id', b'req_01jxath6tbfpmbnffvs85a0qr7'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238984b1ea9ec-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-99eb2edf-3eaa-40a8-a1f2-02e0e3a0a1b8\", \"object\": \"chat.completion\", \"created\": 1749488802, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* Effective communication is crucial in personal and professional settings.\\n* Clarity and brevity are key to conveying ideas successfully.\\n* A clear message significantly impacts how it is received.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05077549, \"prompt_tokens\": 478, \"prompt_time\": 0.040363529, \"completion_tokens\": 42, \"completion_time\": 0.152727273, \"total_tokens\": 520, \"total_time\": 0.193090802}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath6tbfpmbnffvs85a0qr7\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00028201999999999996, completion_tokens_cost_usd_dollar: 3.318e-05\nDEBUG: response_cost: 0.00031519999999999996\nDEBUG: Simple Text Summary (groq/llama-3.3-70b-versatile):\nModelResponse(id='chatcmpl-99eb2edf-3eaa-40a8-a1f2-02e0e3a0a1b8', created=1749488802, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ddfbb0da0', choices=[Choices(finish_reason='stop', index=0, message=Message(content='### TL;DR\\n* Effective communication is crucial in personal and professional settings.\\n* Clarity and brevity are key to conveying ideas successfully.\\n* A clear message significantly impacts how it is received.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=42, prompt_tokens=478, total_tokens=520, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.05077549, prompt_time=0.040363529, completion_time=0.152727273, total_time=0.193090802), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath6tbfpmbnffvs85a0qr7'})\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fd400>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], 'thinking': None}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-99eb2edf-3eaa-40a8-a1f2-02e0e3a0a1b8\", \"object\": \"chat.completion\", \"created\": 1749488802, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* Effective communication is crucial in personal and professional settings.\\n* Clarity and brevity are key to conveying ideas successfully.\\n* A clear message significantly impacts how it is received.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05077549, \"prompt_tokens\": 478, \"prompt_time\": 0.040363529, \"completion_tokens\": 42, \"completion_time\": 0.152727273, \"total_tokens\": 520, \"total_time\": 0.193090802}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath6tbfpmbnffvs85a0qr7\"}}\n\n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00028201999999999996, completion_tokens_cost_usd_dollar: 3.318e-05\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031519999999999996\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fd400>>\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.4924512, "msecs": 492.0, "relativeCreated": 17091.655254364014, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.492486, "msecs": 492.0, "relativeCreated": 17091.690063476562, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.492503, "msecs": 492.0, "relativeCreated": 17091.70699119568, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.492521, "msecs": 492.0, "relativeCreated": 17091.72511100769, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488802.4925811, "msecs": 492.0, "relativeCreated": 17091.785192489624, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.4926, "msecs": 492.0, "relativeCreated": 17091.804027557373, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488802.492616, "msecs": 492.0, "relativeCreated": 17091.820001602173, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488802.492831, "msecs": 492.0, "relativeCreated": 17092.035055160522, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488802.492893, "msecs": 492.0, "relativeCreated": 17092.09704399109, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488802.492928, "msecs": 492.0, "relativeCreated": 17092.132091522217, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.492956, "msecs": 492.0, "relativeCreated": 17092.15998649597, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488802.4929862, "msecs": 492.0, "relativeCreated": 17092.190265655518, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\nEffective communication is vital in both personal and professional contexts.\\nThis short text emphasizes the importance of clarity and brevity in conveying ideas.\\nA clear message can make a significant difference in how it is received.\\n'}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488802.493036, "msecs": 493.0, "relativeCreated": 17092.24009513855, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.4933062, "msecs": 493.0, "relativeCreated": 17092.510223388672, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281e5e50>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.5015008, "msecs": 501.0, "relativeCreated": 17100.70490837097, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.501529, "msecs": 501.0, "relativeCreated": 17100.733041763306, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281e5d30>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.526525, "msecs": 526.0, "relativeCreated": 17125.729084014893, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.52663, "msecs": 526.0, "relativeCreated": 17125.833988189697, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.526734, "msecs": 526.0, "relativeCreated": 17125.938177108765, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.526759, "msecs": 526.0, "relativeCreated": 17125.96297264099, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.5268302, "msecs": 526.0, "relativeCreated": 17126.034259796143, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.526846, "msecs": 526.0, "relativeCreated": 17126.049995422363, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'977'), (b'x-ratelimit-remaining-tokens', b'1197'), (b'x-ratelimit-reset-requests', b'33m5.937s'), (b'x-ratelimit-reset-tokens', b'54.012s'), (b'x-request-id', b'req_01jxath6tbfpmbnffvs85a0qr7'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238984b1ea9ec-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.830875, "msecs": 830.0, "relativeCreated": 17430.078983306885, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488802.831379, "msecs": 831.0, "relativeCreated": 17430.583000183105, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.8315651, "msecs": 831.0, "relativeCreated": 17430.769205093384, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.831806, "msecs": 831.0, "relativeCreated": 17431.010007858276, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.831917, "msecs": 831.0, "relativeCreated": 17431.121110916138, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.832051, "msecs": 832.0, "relativeCreated": 17431.255102157593, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488802.832269, "msecs": 832.0, "relativeCreated": 17431.47301673889, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-99eb2edf-3eaa-40a8-a1f2-02e0e3a0a1b8\", \"object\": \"chat.completion\", \"created\": 1749488802, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* Effective communication is crucial in personal and professional settings.\\n* Clarity and brevity are key to conveying ideas successfully.\\n* A clear message significantly impacts how it is received.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.05077549, \"prompt_tokens\": 478, \"prompt_time\": 0.040363529, \"completion_tokens\": 42, \"completion_time\": 0.152727273, \"total_tokens\": 520, \"total_time\": 0.193090802}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath6tbfpmbnffvs85a0qr7\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.8325021, "msecs": 832.0, "relativeCreated": 17431.706190109253, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488802.8329, "msecs": 832.0, "relativeCreated": 17432.104110717773, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488802.833171, "msecs": 833.0, "relativeCreated": 17432.374954223633, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.833314, "msecs": 833.0, "relativeCreated": 17432.518005371094, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00028201999999999996, completion_tokens_cost_usd_dollar: 3.318e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488802.833387, "msecs": 833.0, "relativeCreated": 17432.5909614563, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031519999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488802.8334668, "msecs": 833.0, "relativeCreated": 17432.670831680298, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165", "asctime": "22:36:42"}, {"name": "root", "msg": "Simple Text Summary (groq/llama-3.3-70b-versatile):\nModelResponse(id='chatcmpl-99eb2edf-3eaa-40a8-a1f2-02e0e3a0a1b8', created=1749488802, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ddfbb0da0', choices=[Choices(finish_reason='stop', index=0, message=Message(content='### TL;DR\\n* Effective communication is crucial in personal and professional settings.\\n* Clarity and brevity are key to conveying ideas successfully.\\n* A clear message significantly impacts how it is received.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=42, prompt_tokens=478, total_tokens=520, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.05077549, prompt_time=0.040363529, completion_time=0.152727273, total_time=0.193090802), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath6tbfpmbnffvs85a0qr7'})", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_summary.py", "filename": "test_only_summary.py", "module": "test_only_summary", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 47, "funcName": "test_simple_text_summary", "created": 1749488802.8338501, "msecs": 833.0, "relativeCreated": 17433.054208755493, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-165"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fd400>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.83394, "msecs": 833.0, "relativeCreated": 17433.144092559814, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-167", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488802.8340092, "msecs": 834.0, "relativeCreated": 17433.213233947754, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-167", "asctime": "22:36:42"}]}, "teardown": {"duration": 0.00230545800877735, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00028201999999999996, completion_tokens_cost_usd_dollar: 3.318e-05\nDEBUG: response_cost: 0.00031519999999999996\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00031519999999999996\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00028201999999999996, completion_tokens_cost_usd_dollar: 3.318e-05\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031519999999999996\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00031519999999999996\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.834988, "msecs": 834.0, "relativeCreated": 17434.192180633545, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.8354251, "msecs": 835.0, "relativeCreated": 17434.629201889038, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488802.8355799, "msecs": 835.0, "relativeCreated": 17434.783935546875, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.8358011, "msecs": 835.0, "relativeCreated": 17435.00518798828, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488802.8359108, "msecs": 835.0, "relativeCreated": 17435.114860534668, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.8360078, "msecs": 836.0, "relativeCreated": 17435.211896896362, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00028201999999999996, completion_tokens_cost_usd_dollar: 3.318e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488802.836095, "msecs": 836.0, "relativeCreated": 17435.299158096313, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031519999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488802.8361669, "msecs": 836.0, "relativeCreated": 17435.370922088623, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00031519999999999996", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488802.836218, "msecs": 836.0, "relativeCreated": 17435.42218208313, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488802.8363478, "msecs": 836.0, "relativeCreated": 17435.55188179016, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488802.836425, "msecs": 836.0, "relativeCreated": 17435.62912940979, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-168", "asctime": "22:36:42"}]}}, {"nodeid": "tests/test_only_summary.py::test_news_article_summary", "lineno": 49, "outcome": "failed", "keywords": ["test_news_article_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.0006260829977691174, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488802.83776, "msecs": 837.0, "relativeCreated": 17436.96403503418, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.010282875009579584, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_summary.py", "lineno": 59, "message": ""}, {"path": "src/elevate/only_summary.py", "lineno": 89, "message": "in summarize_and_convert_to_markdown"}, {"path": "src/elevate/only_summary.py", "lineno": 50, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:42 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], 'thinking': None}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:42 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.838372, "msecs": 838.0, "relativeCreated": 17437.576055526733, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.8384929, "msecs": 838.0, "relativeCreated": 17437.696933746338, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.838548, "msecs": 838.0, "relativeCreated": 17437.75200843811, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.838594, "msecs": 838.0, "relativeCreated": 17437.798023223877, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488802.8388102, "msecs": 838.0, "relativeCreated": 17438.014268875122, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.838869, "msecs": 838.0, "relativeCreated": 17438.07315826416, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488802.8389251, "msecs": 838.0, "relativeCreated": 17438.12918663025, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488802.839589, "msecs": 839.0, "relativeCreated": 17438.793182373047, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488802.8397, "msecs": 839.0, "relativeCreated": 17438.90404701233, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488802.839777, "msecs": 839.0, "relativeCreated": 17438.98105621338, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488802.8398452, "msecs": 839.0, "relativeCreated": 17439.049243927002, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488802.8399029, "msecs": 839.0, "relativeCreated": 17439.106941223145, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\\n    The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\\n    Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488802.840058, "msecs": 840.0, "relativeCreated": 17439.26215171814, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:42"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.840991, "msecs": 840.0, "relativeCreated": 17440.195083618164, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.841383, "msecs": 841.0, "relativeCreated": 17440.587043762207, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.84145, "msecs": 841.0, "relativeCreated": 17440.654039382935, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.8417192, "msecs": 841.0, "relativeCreated": 17440.92321395874, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.841797, "msecs": 841.0, "relativeCreated": 17441.001176834106, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.8419468, "msecs": 841.0, "relativeCreated": 17441.150903701782, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.842005, "msecs": 842.0, "relativeCreated": 17441.209077835083, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488802.8421729, "msecs": 842.0, "relativeCreated": 17441.37692451477, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488802.8422558, "msecs": 842.0, "relativeCreated": 17441.459894180298, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488802.842609, "msecs": 842.0, "relativeCreated": 17441.812992095947, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488802.845865, "msecs": 845.0, "relativeCreated": 17445.069074630737, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-170", "asctime": "22:36:42"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...he measures may burden local businesses and require significant changes to current operations.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-e0c949f1-1a84-46d0-ac51-244583bba245', created=1749488802, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281e4740>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text... current operations.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...he measures may burden local businesses and require significant changes to current operations.\\n    ', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '18aa3420-f763-4ef9-93b1-92873074cfbe', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281e4740>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...he measures may burden local businesses and require significant changes to current operations.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-e0c949f1-1a84-46d0-ac51-244583bba245', created=1749488802, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281e4740>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text... current operations.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_news_article_summary(settings: Any) -> None:\n        \"\"\"Test news article summary generation.\"\"\"\n        news_text = \"\"\"\n        In a historic move this morning, government officials announced a groundbreaking policy aimed at reducing carbon emissions across the country.\n        The policy outlines specific targets to achieve renewable energy milestones by 2030, with strict regulations and incentives for industries.\n        Environmental groups have praised the decision, though some critics argue that the measures may burden local businesses and require significant changes to current operations.\n        \"\"\"\n        only_summary_instance = OnlySummary(with_model=settings.with_model)\n>       summary_output = await only_summary_instance.summarize_and_convert_to_markdown(news_text)\n\ntests/test_only_summary.py:59: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_summary.py:89: in summarize_and_convert_to_markdown\n    return await self.make_llm_call(system_prompt, input_text)\nsrc/elevate/only_summary.py:50: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '18aa3420-f763-4ef9-93b1-92873074cfbe', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281e4740>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003354590153321624, "outcome": "passed"}}, {"nodeid": "tests/test_only_summary.py::test_research_paper_summary", "lineno": 62, "outcome": "passed", "keywords": ["test_research_paper_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.00019420799799263477, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488803.0347981, "msecs": 34.0, "relativeCreated": 17634.002208709717, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.732089332974283, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12819f9e0>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281fec30>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'976'), (b'x-ratelimit-remaining-tokens', b'732'), (b'x-ratelimit-reset-requests', b'34m33.074999999s'), (b'x-ratelimit-reset-tokens', b'56.339s'), (b'x-request-id', b'req_01jxath7asehssxvby07e89ayr'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2389b9f6e4b7e-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-8c653fea-06fa-4406-a5a6-eb020d693884\", \"object\": \"chat.completion\", \"created\": 1749488803, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* This paper proposes a **unified framework** for reconciling quantum mechanics and general relativity.\\n* The authors present **theoretical models** and **experiments** that suggest a potential path towards resolving inconsistencies.\\n* Preliminary results show **promising directions** for future research, with possible implications for our understanding of the universe.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.04926555100000002, \"prompt_tokens\": 502, \"prompt_time\": 0.300818549, \"completion_tokens\": 73, \"completion_time\": 0.268489748, \"total_tokens\": 575, \"total_time\": 0.569308297}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath7asehssxvby07e89ayr\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00029618, completion_tokens_cost_usd_dollar: 5.7669999999999995e-05\nDEBUG: response_cost: 0.00035385\nDEBUG: Research Paper Summary:\nModelResponse(id='chatcmpl-8c653fea-06fa-4406-a5a6-eb020d693884', created=1749488803, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ddfbb0da0', choices=[Choices(finish_reason='stop', index=0, message=Message(content='### TL;DR\\n* This paper proposes a **unified framework** for reconciling quantum mechanics and general relativity.\\n* The authors present **theoretical models** and **experiments** that suggest a potential path towards resolving inconsistencies.\\n* Preliminary results show **promising directions** for future research, with possible implications for our understanding of the universe.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=73, prompt_tokens=502, total_tokens=575, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.04926555100000002, prompt_time=0.300818549, completion_time=0.268489748, total_time=0.569308297), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath7asehssxvby07e89ayr'})\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281ff7d0>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], 'thinking': None}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-8c653fea-06fa-4406-a5a6-eb020d693884\", \"object\": \"chat.completion\", \"created\": 1749488803, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* This paper proposes a **unified framework** for reconciling quantum mechanics and general relativity.\\n* The authors present **theoretical models** and **experiments** that suggest a potential path towards resolving inconsistencies.\\n* Preliminary results show **promising directions** for future research, with possible implications for our understanding of the universe.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.04926555100000002, \"prompt_tokens\": 502, \"prompt_time\": 0.300818549, \"completion_tokens\": 73, \"completion_time\": 0.268489748, \"total_tokens\": 575, \"total_time\": 0.569308297}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath7asehssxvby07e89ayr\"}}\n\n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00029618, completion_tokens_cost_usd_dollar: 5.7669999999999995e-05\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00035385\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281ff7d0>>\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.03501, "msecs": 35.0, "relativeCreated": 17634.214162826538, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.035059, "msecs": 35.0, "relativeCreated": 17634.263038635254, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.035077, "msecs": 35.0, "relativeCreated": 17634.281158447266, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.035106, "msecs": 35.0, "relativeCreated": 17634.310007095337, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488803.0351732, "msecs": 35.0, "relativeCreated": 17634.377241134644, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.0351942, "msecs": 35.0, "relativeCreated": 17634.398221969604, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488803.035218, "msecs": 35.0, "relativeCreated": 17634.422063827515, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488803.035462, "msecs": 35.0, "relativeCreated": 17634.665966033936, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488803.035514, "msecs": 35.0, "relativeCreated": 17634.71817970276, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488803.035574, "msecs": 35.0, "relativeCreated": 17634.778022766113, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.035606, "msecs": 35.0, "relativeCreated": 17634.809970855713, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488803.035668, "msecs": 35.0, "relativeCreated": 17634.87195968628, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    This paper explores the complex interactions between quantum mechanics and general relativity by proposing a unified framework.\\n    The authors present a series of experiments and theoretical models that suggest a potential path towards reconciling inconsistencies between the\\n    Preliminary results demonstrate promising directions for future research and indicate possible implications for our understanding of the universe.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488803.035755, "msecs": 35.0, "relativeCreated": 17634.95898246765, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.036118, "msecs": 36.0, "relativeCreated": 17635.322093963623, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x12819f9e0>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.045076, "msecs": 45.0, "relativeCreated": 17644.279956817627, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.045107, "msecs": 45.0, "relativeCreated": 17644.31095123291, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x1281fec30>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.055587, "msecs": 55.0, "relativeCreated": 17654.791116714478, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.0556748, "msecs": 55.0, "relativeCreated": 17654.878854751587, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.0557718, "msecs": 55.0, "relativeCreated": 17654.97589111328, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.055789, "msecs": 55.0, "relativeCreated": 17654.993057250977, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.0558589, "msecs": 55.0, "relativeCreated": 17655.062913894653, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.055875, "msecs": 55.0, "relativeCreated": 17655.079126358032, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:43 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'976'), (b'x-ratelimit-remaining-tokens', b'732'), (b'x-ratelimit-reset-requests', b'34m33.074999999s'), (b'x-ratelimit-reset-tokens', b'56.339s'), (b'x-request-id', b'req_01jxath7asehssxvby07e89ayr'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d2389b9f6e4b7e-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.7631118, "msecs": 763.0, "relativeCreated": 18362.315893173218, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488803.763752, "msecs": 763.0, "relativeCreated": 18362.956047058105, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.76399, "msecs": 763.0, "relativeCreated": 18363.19398880005, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.764249, "msecs": 764.0, "relativeCreated": 18363.453149795532, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.7643552, "msecs": 764.0, "relativeCreated": 18363.559246063232, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.764483, "msecs": 764.0, "relativeCreated": 18363.68703842163, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488803.7647, "msecs": 764.0, "relativeCreated": 18363.903999328613, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-8c653fea-06fa-4406-a5a6-eb020d693884\", \"object\": \"chat.completion\", \"created\": 1749488803, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* This paper proposes a **unified framework** for reconciling quantum mechanics and general relativity.\\n* The authors present **theoretical models** and **experiments** that suggest a potential path towards resolving inconsistencies.\\n* Preliminary results show **promising directions** for future research, with possible implications for our understanding of the universe.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.04926555100000002, \"prompt_tokens\": 502, \"prompt_time\": 0.300818549, \"completion_tokens\": 73, \"completion_time\": 0.268489748, \"total_tokens\": 575, \"total_time\": 0.569308297}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath7asehssxvby07e89ayr\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.7649379, "msecs": 764.0, "relativeCreated": 18364.141941070557, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488803.765384, "msecs": 765.0, "relativeCreated": 18364.588022232056, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488803.7657542, "msecs": 765.0, "relativeCreated": 18364.9582862854, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488803.765939, "msecs": 765.0, "relativeCreated": 18365.143060684204, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00029618, completion_tokens_cost_usd_dollar: 5.7669999999999995e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488803.766038, "msecs": 766.0, "relativeCreated": 18365.24200439453, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "response_cost: 0.00035385", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488803.76614, "msecs": 766.0, "relativeCreated": 18365.344047546387, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173", "asctime": "22:36:43"}, {"name": "root", "msg": "Research Paper Summary:\nModelResponse(id='chatcmpl-8c653fea-06fa-4406-a5a6-eb020d693884', created=1749488803, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ddfbb0da0', choices=[Choices(finish_reason='stop', index=0, message=Message(content='### TL;DR\\n* This paper proposes a **unified framework** for reconciling quantum mechanics and general relativity.\\n* The authors present **theoretical models** and **experiments** that suggest a potential path towards resolving inconsistencies.\\n* Preliminary results show **promising directions** for future research, with possible implications for our understanding of the universe.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=73, prompt_tokens=502, total_tokens=575, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.04926555100000002, prompt_time=0.300818549, completion_time=0.268489748, total_time=0.569308297), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath7asehssxvby07e89ayr'})", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_summary.py", "filename": "test_only_summary.py", "module": "test_only_summary", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 73, "funcName": "test_research_paper_summary", "created": 1749488803.766357, "msecs": 766.0, "relativeCreated": 18365.56100845337, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-173"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281ff7d0>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.766495, "msecs": 766.0, "relativeCreated": 18365.69905281067, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-175", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488803.766552, "msecs": 766.0, "relativeCreated": 18365.756034851074, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-175", "asctime": "22:36:43"}]}, "teardown": {"duration": 0.0018202500068582594, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00029618, completion_tokens_cost_usd_dollar: 5.7669999999999995e-05\nDEBUG: response_cost: 0.00035385\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00035385\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00029618, completion_tokens_cost_usd_dollar: 5.7669999999999995e-05\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00035385\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00035385\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.767478, "msecs": 767.0, "relativeCreated": 18366.682052612305, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488803.767785, "msecs": 767.0, "relativeCreated": 18366.989135742188, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488803.767873, "msecs": 767.0, "relativeCreated": 18367.077112197876, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.768034, "msecs": 768.0, "relativeCreated": 18367.23804473877, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488803.768099, "msecs": 768.0, "relativeCreated": 18367.303133010864, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488803.768168, "msecs": 768.0, "relativeCreated": 18367.372035980225, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00029618, completion_tokens_cost_usd_dollar: 5.7669999999999995e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488803.768218, "msecs": 768.0, "relativeCreated": 18367.422103881836, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "response_cost: 0.00035385", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488803.7682738, "msecs": 768.0, "relativeCreated": 18367.477893829346, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00035385", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488803.768317, "msecs": 768.0, "relativeCreated": 18367.521047592163, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488803.7684321, "msecs": 768.0, "relativeCreated": 18367.63620376587, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488803.7684941, "msecs": 768.0, "relativeCreated": 18367.698192596436, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-176", "asctime": "22:36:43"}]}}, {"nodeid": "tests/test_only_summary.py::test_technical_manual_summary", "lineno": 75, "outcome": "failed", "keywords": ["test_technical_manual_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.0008023749978747219, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488803.76989, "msecs": 769.0, "relativeCreated": 18369.094133377075, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.012481542013119906, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_summary.py", "lineno": 84, "message": ""}, {"path": "src/elevate/only_summary.py", "lineno": 89, "message": "in summarize_and_convert_to_markdown"}, {"path": "src/elevate/only_summary.py", "lineno": 50, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], 'thinking': None}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.770698, "msecs": 770.0, "relativeCreated": 18369.90213394165, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.770845, "msecs": 770.0, "relativeCreated": 18370.048999786377, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.7709, "msecs": 770.0, "relativeCreated": 18370.10407447815, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.77097, "msecs": 770.0, "relativeCreated": 18370.174169540405, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488803.771203, "msecs": 771.0, "relativeCreated": 18370.407104492188, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.7712908, "msecs": 771.0, "relativeCreated": 18370.494842529297, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488803.771359, "msecs": 771.0, "relativeCreated": 18370.56303024292, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488803.7723, "msecs": 772.0, "relativeCreated": 18371.504068374634, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488803.7724319, "msecs": 772.0, "relativeCreated": 18371.635913848877, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488803.772521, "msecs": 772.0, "relativeCreated": 18371.72508239746, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.772588, "msecs": 772.0, "relativeCreated": 18371.79207801819, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488803.772661, "msecs": 772.0, "relativeCreated": 18371.865034103394, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\\n    It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488803.772813, "msecs": 772.0, "relativeCreated": 18372.01714515686, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.7738519, "msecs": 773.0, "relativeCreated": 18373.055934906006, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.774399, "msecs": 774.0, "relativeCreated": 18373.603105545044, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.774483, "msecs": 774.0, "relativeCreated": 18373.687028884888, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.774775, "msecs": 774.0, "relativeCreated": 18373.979091644287, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.774841, "msecs": 774.0, "relativeCreated": 18374.0451335907, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.774972, "msecs": 774.0, "relativeCreated": 18374.176025390625, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.775018, "msecs": 775.0, "relativeCreated": 18374.22204017639, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.775172, "msecs": 775.0, "relativeCreated": 18374.37605857849, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488803.775269, "msecs": 775.0, "relativeCreated": 18374.473094940186, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488803.775661, "msecs": 775.0, "relativeCreated": 18374.86505508423, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488803.778901, "msecs": 778.0, "relativeCreated": 18378.10516357422, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-178", "asctime": "22:36:43"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a... also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-aeb50e23-2555-48c9-9fba-13f9992e9737', created=1749488803, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fdfa0>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text...and voice assistant.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a... also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    ', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '769244ce-fb34-429a-b368-2950bf2e3ad7', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fdfa0>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a... also explains features such as the high-resolution camera, touch screen, and voice assistant.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-aeb50e23-2555-48c9-9fba-13f9992e9737', created=1749488803, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fdfa0>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text...and voice assistant.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_technical_manual_summary(settings: Any) -> None:\n        \"\"\"Test technical manual summary generation.\"\"\"\n        technical_manual_text = \"\"\"\n        User Manual for the XYZ Smartphone: This document provides detailed instructions on setting up the smartphone, including battery installation, initial configuration, and establishing a mobile network connection.\n        It includes troubleshooting tips, frequently asked questions, and safety precautions. The manual also explains features such as the high-resolution camera, touch screen, and voice assistant.\n        \"\"\"\n        only_summary_instance = OnlySummary(with_model=settings.with_model)\n>       summary_output = await only_summary_instance.summarize_and_convert_to_markdown(technical_manual_text)\n\ntests/test_only_summary.py:84: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_summary.py:89: in summarize_and_convert_to_markdown\n    return await self.make_llm_call(system_prompt, input_text)\nsrc/elevate/only_summary.py:50: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '769244ce-fb34-429a-b368-2950bf2e3ad7', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x1281fdfa0>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.00031725000008009374, "outcome": "passed"}}, {"nodeid": "tests/test_only_summary.py::test_business_report_summary", "lineno": 87, "outcome": "passed", "keywords": ["test_business_report_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.0001966250129044056, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488803.943752, "msecs": 943.0, "relativeCreated": 18542.956113815308, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.5293802919914015, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f582d80>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57f620>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'975'), (b'x-ratelimit-remaining-tokens', b'342'), (b'x-ratelimit-reset-requests', b'35m59.091s'), (b'x-ratelimit-reset-tokens', b'58.288s'), (b'x-request-id', b'req_01jxath876ehsvx356enez3cm3'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238a15b8347ca-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-ffef210d-e8ba-495d-83cd-102aad2edc3e\", \"object\": \"chat.completion\", \"created\": 1749488804, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* The company saw a **15% increase in revenue** in Q3 compared to the previous quarter.\\n* Growth was driven by **expansion into new markets** and the **launch of innovative products**.\\n* The report outlines **market trends**, **financial performance**, and **strategic initiatives** for continued progress.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049393287999999994, \"prompt_tokens\": 510, \"prompt_time\": 0.042467932, \"completion_tokens\": 70, \"completion_time\": 0.292319026, \"total_tokens\": 580, \"total_time\": 0.334786958}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath876ehsvx356enez3cm3\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0003009, completion_tokens_cost_usd_dollar: 5.5299999999999996e-05\nDEBUG: response_cost: 0.0003562\nDEBUG: Business Report Summary:\nModelResponse(id='chatcmpl-ffef210d-e8ba-495d-83cd-102aad2edc3e', created=1749488804, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ddfbb0da0', choices=[Choices(finish_reason='stop', index=0, message=Message(content='### TL;DR\\n* The company saw a **15% increase in revenue** in Q3 compared to the previous quarter.\\n* Growth was driven by **expansion into new markets** and the **launch of innovative products**.\\n* The report outlines **market trends**, **financial performance**, and **strategic initiatives** for continued progress.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=70, prompt_tokens=510, total_tokens=580, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.049393287999999994, prompt_time=0.042467932, completion_time=0.292319026, total_time=0.334786958), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath876ehsvx356enez3cm3'})\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c9280>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:43 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], 'thinking': None}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:43 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-ffef210d-e8ba-495d-83cd-102aad2edc3e\", \"object\": \"chat.completion\", \"created\": 1749488804, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* The company saw a **15% increase in revenue** in Q3 compared to the previous quarter.\\n* Growth was driven by **expansion into new markets** and the **launch of innovative products**.\\n* The report outlines **market trends**, **financial performance**, and **strategic initiatives** for continued progress.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049393287999999994, \"prompt_tokens\": 510, \"prompt_time\": 0.042467932, \"completion_tokens\": 70, \"completion_time\": 0.292319026, \"total_tokens\": 580, \"total_time\": 0.334786958}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath876ehsvx356enez3cm3\"}}\n\n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0003009, completion_tokens_cost_usd_dollar: 5.5299999999999996e-05\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.0003562\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c9280>>\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.943976, "msecs": 943.0, "relativeCreated": 18543.179988861084, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.9440188, "msecs": 944.0, "relativeCreated": 18543.222904205322, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.9440398, "msecs": 944.0, "relativeCreated": 18543.243885040283, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.944062, "msecs": 944.0, "relativeCreated": 18543.26605796814, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488803.9441228, "msecs": 944.0, "relativeCreated": 18543.32685470581, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.94415, "msecs": 944.0, "relativeCreated": 18543.354034423828, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488803.944167, "msecs": 944.0, "relativeCreated": 18543.370962142944, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488803.944393, "msecs": 944.0, "relativeCreated": 18543.596982955933, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488803.944428, "msecs": 944.0, "relativeCreated": 18543.63203048706, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488803.944486, "msecs": 944.0, "relativeCreated": 18543.689966201782, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488803.9445279, "msecs": 944.0, "relativeCreated": 18543.731927871704, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488803.944561, "msecs": 944.0, "relativeCreated": 18543.7650680542, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Q3 Business Report: The company experienced steady growth in the third quarter, with a 15% increase in revenue compared to the previous quarter.\\n    Expansion into new markets and the launch of innovative product lines were significant contributors to this growth.\\n    The report details market trends, financial performance, and strategic initiatives expected to drive continued progress in the upcoming months.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488803.9446158, "msecs": 944.0, "relativeCreated": 18543.819904327393, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:43"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.944892, "msecs": 944.0, "relativeCreated": 18544.095993041992, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f582d80>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.9580019, "msecs": 958.0, "relativeCreated": 18557.20591545105, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.958049, "msecs": 958.0, "relativeCreated": 18557.253122329712, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f57f620>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.970208, "msecs": 970.0, "relativeCreated": 18569.411993026733, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.9702961, "msecs": 970.0, "relativeCreated": 18569.500207901, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.9703932, "msecs": 970.0, "relativeCreated": 18569.597244262695, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.970411, "msecs": 970.0, "relativeCreated": 18569.615125656128, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.970479, "msecs": 970.0, "relativeCreated": 18569.683074951172, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488803.970496, "msecs": 970.0, "relativeCreated": 18569.700002670288, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'975'), (b'x-ratelimit-remaining-tokens', b'342'), (b'x-ratelimit-reset-requests', b'35m59.091s'), (b'x-ratelimit-reset-tokens', b'58.288s'), (b'x-request-id', b'req_01jxath876ehsvx356enez3cm3'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238a15b8347ca-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.4690812, "msecs": 469.0, "relativeCreated": 19068.2852268219, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488804.469655, "msecs": 469.0, "relativeCreated": 19068.859100341797, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.469873, "msecs": 469.0, "relativeCreated": 19069.077014923096, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.470293, "msecs": 470.0, "relativeCreated": 19069.497108459473, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.470508, "msecs": 470.0, "relativeCreated": 19069.712162017822, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.4706528, "msecs": 470.0, "relativeCreated": 19069.856882095337, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488804.4708931, "msecs": 470.0, "relativeCreated": 19070.09720802307, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-ffef210d-e8ba-495d-83cd-102aad2edc3e\", \"object\": \"chat.completion\", \"created\": 1749488804, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"### TL;DR\\n* The company saw a **15% increase in revenue** in Q3 compared to the previous quarter.\\n* Growth was driven by **expansion into new markets** and the **launch of innovative products**.\\n* The report outlines **market trends**, **financial performance**, and **strategic initiatives** for continued progress.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.049393287999999994, \"prompt_tokens\": 510, \"prompt_time\": 0.042467932, \"completion_tokens\": 70, \"completion_time\": 0.292319026, \"total_tokens\": 580, \"total_time\": 0.334786958}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath876ehsvx356enez3cm3\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.471194, "msecs": 471.0, "relativeCreated": 19070.398092269897, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488804.471633, "msecs": 471.0, "relativeCreated": 19070.837020874023, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488804.471922, "msecs": 471.0, "relativeCreated": 19071.125984191895, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488804.4721348, "msecs": 472.0, "relativeCreated": 19071.338891983032, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0003009, completion_tokens_cost_usd_dollar: 5.5299999999999996e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488804.472239, "msecs": 472.0, "relativeCreated": 19071.4430809021, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "response_cost: 0.0003562", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488804.472346, "msecs": 472.0, "relativeCreated": 19071.550130844116, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181", "asctime": "22:36:44"}, {"name": "root", "msg": "Business Report Summary:\nModelResponse(id='chatcmpl-ffef210d-e8ba-495d-83cd-102aad2edc3e', created=1749488804, model='groq/llama-3.3-70b-versatile', object='chat.completion', system_fingerprint='fp_2ddfbb0da0', choices=[Choices(finish_reason='stop', index=0, message=Message(content='### TL;DR\\n* The company saw a **15% increase in revenue** in Q3 compared to the previous quarter.\\n* Growth was driven by **expansion into new markets** and the **launch of innovative products**.\\n* The report outlines **market trends**, **financial performance**, and **strategic initiatives** for continued progress.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=70, prompt_tokens=510, total_tokens=580, completion_tokens_details=None, prompt_tokens_details=None, queue_time=0.049393287999999994, prompt_time=0.042467932, completion_time=0.292319026, total_time=0.334786958), usage_breakdown={'models': None}, x_groq={'id': 'req_01jxath876ehsvx356enez3cm3'})", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_summary.py", "filename": "test_only_summary.py", "module": "test_only_summary", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 98, "funcName": "test_business_report_summary", "created": 1749488804.4725509, "msecs": 472.0, "relativeCreated": 19071.754932403564, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-181"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f5c9280>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.472704, "msecs": 472.0, "relativeCreated": 19071.907997131348, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-183", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488804.472783, "msecs": 472.0, "relativeCreated": 19071.98715209961, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-183", "asctime": "22:36:44"}]}, "teardown": {"duration": 0.0018485419859644026, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0003009, completion_tokens_cost_usd_dollar: 5.5299999999999996e-05\nDEBUG: response_cost: 0.0003562\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.0003562\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:44 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0003009, completion_tokens_cost_usd_dollar: 5.5299999999999996e-05\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.0003562\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.0003562\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.473742, "msecs": 473.0, "relativeCreated": 19072.946071624756, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488804.474051, "msecs": 474.0, "relativeCreated": 19073.25506210327, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488804.474133, "msecs": 474.0, "relativeCreated": 19073.337078094482, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.474299, "msecs": 474.0, "relativeCreated": 19073.503017425537, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488804.4743638, "msecs": 474.0, "relativeCreated": 19073.567867279053, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488804.474429, "msecs": 474.0, "relativeCreated": 19073.632955551147, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.0003009, completion_tokens_cost_usd_dollar: 5.5299999999999996e-05", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488804.474498, "msecs": 474.0, "relativeCreated": 19073.702096939087, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "response_cost: 0.0003562", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488804.474557, "msecs": 474.0, "relativeCreated": 19073.760986328125, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.0003562", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488804.4745972, "msecs": 474.0, "relativeCreated": 19073.801279067993, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488804.474709, "msecs": 474.0, "relativeCreated": 19073.91309738159, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488804.474768, "msecs": 474.0, "relativeCreated": 19073.97198677063, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-184", "asctime": "22:36:44"}]}}, {"nodeid": "tests/test_only_summary.py::test_legal_document_summary", "lineno": 100, "outcome": "failed", "keywords": ["test_legal_document_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.0006262499955482781, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488804.476161, "msecs": 476.0, "relativeCreated": 19075.36506652832, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.01253245901898481, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_summary.py", "lineno": 110, "message": ""}, {"path": "src/elevate/only_summary.py", "lineno": 89, "message": "in summarize_and_convert_to_markdown"}, {"path": "src/elevate/only_summary.py", "lineno": 50, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], 'thinking': None}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.4767401, "msecs": 476.0, "relativeCreated": 19075.944185256958, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.476842, "msecs": 476.0, "relativeCreated": 19076.045989990234, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.476894, "msecs": 476.0, "relativeCreated": 19076.09796524048, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.476945, "msecs": 476.0, "relativeCreated": 19076.148986816406, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.4771378, "msecs": 477.0, "relativeCreated": 19076.3418674469, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.477209, "msecs": 477.0, "relativeCreated": 19076.41315460205, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488804.477271, "msecs": 477.0, "relativeCreated": 19076.475143432617, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488804.4780319, "msecs": 478.0, "relativeCreated": 19077.23593711853, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488804.478173, "msecs": 478.0, "relativeCreated": 19077.37708091736, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488804.4782841, "msecs": 478.0, "relativeCreated": 19077.48818397522, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.478395, "msecs": 478.0, "relativeCreated": 19077.599048614502, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.478495, "msecs": 478.0, "relativeCreated": 19077.698945999146, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\\n    It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488804.478678, "msecs": 478.0, "relativeCreated": 19077.882051467896, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.479692, "msecs": 479.0, "relativeCreated": 19078.896045684814, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.4801679, "msecs": 480.0, "relativeCreated": 19079.3719291687, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.480257, "msecs": 480.0, "relativeCreated": 19079.461097717285, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.48053, "msecs": 480.0, "relativeCreated": 19079.734086990356, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.480587, "msecs": 480.0, "relativeCreated": 19079.79106903076, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.4807308, "msecs": 480.0, "relativeCreated": 19079.93483543396, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.480787, "msecs": 480.0, "relativeCreated": 19079.991102218628, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.48096, "msecs": 480.0, "relativeCreated": 19080.163955688477, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488804.48107, "msecs": 481.0, "relativeCreated": 19080.27410507202, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488804.48144, "msecs": 481.0, "relativeCreated": 19080.644130706787, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488804.4846878, "msecs": 484.0, "relativeCreated": 19083.89186859131, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-186", "asctime": "22:36:44"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-916452ac-e2df-4161-8fa6-9a4068cd86cf', created=1749488804, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57c410>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text... of applicable laws.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    ', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '660f4184-8c72-4db6-8880-4398a52e91be', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57c410>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ures.\\n    The document is legally binding and subject to the jurisdiction of applicable laws.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-916452ac-e2df-4161-8fa6-9a4068cd86cf', created=1749488804, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57c410>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text... of applicable laws.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_legal_document_summary(settings: Any) -> None:\n        \"\"\"Test legal document summary generation.\"\"\"\n        legal_document_text = \"\"\"\n        Contract Agreement Overview: This section of the contract outlines the terms and conditions of the service provided.\n        It specifies the responsibilities of both the service provider and the client, including deliverables, payment terms, confidentiality obligations, and dispute resolution procedures.\n        The document is legally binding and subject to the jurisdiction of applicable laws.\n        \"\"\"\n        only_summary_instance = OnlySummary(with_model=settings.with_model)\n>       summary_output = await only_summary_instance.summarize_and_convert_to_markdown(legal_document_text)\n\ntests/test_only_summary.py:110: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_summary.py:89: in summarize_and_convert_to_markdown\n    return await self.make_llm_call(system_prompt, input_text)\nsrc/elevate/only_summary.py:50: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '660f4184-8c72-4db6-8880-4398a52e91be', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f57c410>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.00032737498986534774, "outcome": "passed"}}, {"nodeid": "tests/test_only_summary.py::test_blog_post_summary", "lineno": 113, "outcome": "failed", "keywords": ["test_blog_post_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.00020599999697878957, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488804.6488671, "msecs": 648.0, "relativeCreated": 19248.07119369507, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.05897783400723711, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 429, "message": "litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jeqca19feksbctg05mdrkq1z` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11562, Requested 525. Please try again in 435ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}"}, "traceback": [{"path": "tests/test_only_summary.py", "lineno": 122, "message": ""}, {"path": "src/elevate/only_summary.py", "lineno": 89, "message": "in summarize_and_convert_to_markdown"}, {"path": "src/elevate/only_summary.py", "lineno": 50, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 429, "message": "RateLimitError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f149100>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f14af30>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 09 Jun 2025 17:06:44 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'382'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'retry-after', b'1'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'975'), (b'x-ratelimit-remaining-tokens', b'438'), (b'x-ratelimit-reset-requests', b'35m59.291999999s'), (b'x-ratelimit-reset-tokens', b'57.81s'), (b'x-request-id', b'req_01jxath8xafv5vd6bmyh5x449m'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238a5a8d431ee-BOM'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], 'thinking': None}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.6490948, "msecs": 649.0, "relativeCreated": 19248.29888343811, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.6491368, "msecs": 649.0, "relativeCreated": 19248.340845108032, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.649164, "msecs": 649.0, "relativeCreated": 19248.36802482605, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.649188, "msecs": 649.0, "relativeCreated": 19248.39210510254, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.6492538, "msecs": 649.0, "relativeCreated": 19248.45790863037, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.649276, "msecs": 649.0, "relativeCreated": 19248.480081558228, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488804.649293, "msecs": 649.0, "relativeCreated": 19248.497009277344, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488804.6495302, "msecs": 649.0, "relativeCreated": 19248.73423576355, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488804.649569, "msecs": 649.0, "relativeCreated": 19248.773097991943, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488804.649622, "msecs": 649.0, "relativeCreated": 19248.826026916504, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.6496608, "msecs": 649.0, "relativeCreated": 19248.864889144897, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.6496909, "msecs": 649.0, "relativeCreated": 19248.894929885864, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\\n    In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488804.649749, "msecs": 649.0, "relativeCreated": 19248.953104019165, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.6500301, "msecs": 650.0, "relativeCreated": 19249.234199523926, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f149100>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.6579509, "msecs": 657.0, "relativeCreated": 19257.154941558838, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.657985, "msecs": 657.0, "relativeCreated": 19257.18903541565, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f14af30>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.6731892, "msecs": 673.0, "relativeCreated": 19272.393226623535, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.673292, "msecs": 673.0, "relativeCreated": 19272.495985031128, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.673395, "msecs": 673.0, "relativeCreated": 19272.5989818573, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.673414, "msecs": 673.0, "relativeCreated": 19272.618055343628, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.673485, "msecs": 673.0, "relativeCreated": 19272.6891040802, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.673501, "msecs": 673.0, "relativeCreated": 19272.705078125, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Mon, 09 Jun 2025 17:06:44 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'382'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'retry-after', b'1'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'975'), (b'x-ratelimit-remaining-tokens', b'438'), (b'x-ratelimit-reset-requests', b'35m59.291999999s'), (b'x-ratelimit-reset-tokens', b'57.81s'), (b'x-request-id', b'req_01jxath8xafv5vd6bmyh5x449m'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238a5a8d431ee-BOM'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.704362, "msecs": 704.0, "relativeCreated": 19303.565979003906, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 429 Too Many Requests\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488804.704483, "msecs": 704.0, "relativeCreated": 19303.68709564209, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.704535, "msecs": 704.0, "relativeCreated": 19303.739070892334, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.704573, "msecs": 704.0, "relativeCreated": 19303.77697944641, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.704608, "msecs": 704.0, "relativeCreated": 19303.81202697754, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.704653, "msecs": 704.0, "relativeCreated": 19303.85708808899, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488804.7047732, "msecs": 704.0, "relativeCreated": 19303.977251052856, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488804.70499, "msecs": 704.0, "relativeCreated": 19304.19397354126, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488804.7061079, "msecs": 706.0, "relativeCreated": 19305.311918258667, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-189", "asctime": "22:36:44"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-8dd4d25f-b1a6-4558-8790-f968fdafc6ab', created=1749488804, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54f500>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text...y and an open heart.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:256: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:212: in post\n    response.raise_for_status()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <Response [429 Too Many Requests]>\n\n    def raise_for_status(self) -> Response:\n        \"\"\"\n        Raise the `HTTPStatusError` if one occurred.\n        \"\"\"\n        request = self._request\n        if request is None:\n            raise RuntimeError(\n                \"Cannot call `raise_for_status` as the request \"\n                \"instance has not been set on this response.\"\n            )\n    \n        if self.is_success:\n            return self\n    \n        if self.has_redirect_location:\n            message = (\n                \"{error_type} '{0.status_code} {0.reason_phrase}' for url '{0.url}'\\n\"\n                \"Redirect location: '{0.headers[location]}'\\n\"\n                \"For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{0.status_code}\"\n            )\n        else:\n            message = (\n                \"{error_type} '{0.status_code} {0.reason_phrase}' for url '{0.url}'\\n\"\n                \"For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{0.status_code}\"\n            )\n    \n        status_class = self.status_code // 100\n        error_types = {\n            1: \"Informational response\",\n            3: \"Redirect response\",\n            4: \"Client error\",\n            5: \"Server error\",\n        }\n        error_type = error_types.get(status_class, \"Invalid status code\")\n        message = message.format(self, error_type=error_type)\n>       raise HTTPStatusError(message, request=request, response=self)\nE       httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nE       For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429\n\n.venv/lib/python3.12/site-packages/httpx/_models.py:829: HTTPStatusError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    ', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': '4d7aec6a-5f0f-434b-85b7-7cc2fc96edb1', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54f500>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-8dd4d25f-b1a6-4558-8790-f968fdafc6ab', created=1749488804, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54f500>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text...y and an open heart.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n>           raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: {\"error\":{\"message\":\"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jeqca19feksbctg05mdrkq1z` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11562, Requested 525. Please try again in 435ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:192: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_blog_post_summary(settings: Any) -> None:\n        \"\"\"Test blog post summary generation.\"\"\"\n        blog_post_text = \"\"\"\n        Travel Blog Entry: Last summer, I embarked on a journey across the Mediterranean, experiencing diverse cultures, breathtaking landscapes, and unforgettable culinary delights.\n        In this post, I share personal anecdotes, local encounters, and reflective insights from the road, hoping to inspire fellow travelers to explore new destinations with curiosity and an open heart.\n        \"\"\"\n        only_summary_instance = OnlySummary(with_model=settings.with_model)\n>       summary_output = await only_summary_instance.summarize_and_convert_to_markdown(blog_post_text)\n\ntests/test_only_summary.py:122: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_summary.py:89: in summarize_and_convert_to_markdown\n    return await self.make_llm_call(system_prompt, input_text)\nsrc/elevate/only_summary.py:50: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile'\noriginal_exception = OpenAILikeError('{\"error\":{\"message\":\"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01je...rade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\\n')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': '4d7aec6a-5f0f-434b-85b7-7cc2fc96edb1', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f54f500>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n>                           raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\nE                               litellm.exceptions.RateLimitError: litellm.RateLimitError: RateLimitError: GroqException - {\"error\":{\"message\":\"Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01jeqca19feksbctg05mdrkq1z` service tier `on_demand` on tokens per minute (TPM): Limit 12000, Used 11562, Requested 525. Please try again in 435ms. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing\",\"type\":\"tokens\",\"code\":\"rate_limit_exceeded\"}}\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:429: RateLimitError"}, "teardown": {"duration": 0.00026225001784041524, "outcome": "passed"}}, {"nodeid": "tests/test_only_summary.py::test_fiction_excerpt_summary", "lineno": 125, "outcome": "failed", "keywords": ["test_fiction_excerpt_summary", "asyncio", "pytestmark", "test_only_summary.py", "tests", "elevate", ""], "setup": {"duration": 0.00021925001055933535, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488804.7985432, "msecs": 798.0, "relativeCreated": 19397.7472782135, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.0041017080075107515, "outcome": "failed", "crash": {"path": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed"}, "traceback": [{"path": "tests/test_only_summary.py", "lineno": 135, "message": ""}, {"path": "src/elevate/only_summary.py", "lineno": 89, "message": "in summarize_and_convert_to_markdown"}, {"path": "src/elevate/only_summary.py", "lineno": 50, "message": "in make_llm_call"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1452, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 1313, "message": "in wrapper_async"}, {"path": ".venv/lib/python3.12/site-packages/litellm/main.py", "lineno": 496, "message": "in acompletion"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 2214, "message": "in exception_type"}, {"path": ".venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "lineno": 455, "message": "APIError"}], "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.failed exception=RuntimeError('Event loop is closed')\nDEBUG: response_closed.started\nDEBUG: response_closed.failed exception=RuntimeError('Event loop is closed')\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\n\n\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\nLiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n\nDEBUG: Logging Details: logger_fn - None | callable(logger_fn) - False\nDEBUG: Logging Details LiteLLM-Failure Call: []\n", "stderr": "\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], 'thinking': None}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: exception_mapping_utils.py:2243 - Logging Details: logger_fn - None | callable(logger_fn) - False\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2032 - Logging Details LiteLLM-Failure Call: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.798771, "msecs": 798.0, "relativeCreated": 19397.974967956543, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.79881, "msecs": 798.0, "relativeCreated": 19398.014068603516, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.7988281, "msecs": 798.0, "relativeCreated": 19398.032188415527, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.7988448, "msecs": 798.0, "relativeCreated": 19398.048877716064, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.798906, "msecs": 798.0, "relativeCreated": 19398.110151290894, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.7989278, "msecs": 798.0, "relativeCreated": 19398.13184738159, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488804.7989461, "msecs": 798.0, "relativeCreated": 19398.150205612183, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488804.7991629, "msecs": 799.0, "relativeCreated": 19398.366928100586, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488804.799222, "msecs": 799.0, "relativeCreated": 19398.426055908203, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488804.799253, "msecs": 799.0, "relativeCreated": 19398.457050323486, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.7992802, "msecs": 799.0, "relativeCreated": 19398.484230041504, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.799306, "msecs": 799.0, "relativeCreated": 19398.509979248047, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean and accurate GitHub Flavored Markdown (GFM). You are also proficient at creating TL;DR summaries. Your task is to summarize the input text in TL;DR format *and* convert that summary into Markdown.\\n\\n**Tasks:**\\n\\n1.  **TL;DR Summary:** Create a concise \"TL;DR\" (Too Long; Didn\\'t Read) summary of the input text.\\n2.  **Markdown Conversion (of the TL;DR):** Convert *only the TL;DR summary* into GitHub Flavored Markdown.\\n\\n**Instructions:**\\n\\n1.  **Input:** You will receive a string of plain text as input.\\n2.  **Process:**\\n    *   Generate the TL;DR summary of the input text.\\n    *   Convert the TL;DR summary *itself* into Markdown.\\n3.  **Output:** Return ONLY the converted Markdown of the TL;DR summary. Do NOT include the original text or any other content.\\n4.  **TL;DR Style:** The TL;DR should be concise, typically a few sentences, and convey the most important points of the original text.\\n5.  **GitHub Flavored Markdown (GFM) Specifics:** Adhere to GFM conventions:\\n    *   Using fenced code blocks with syntax highlighting (e.g., ```python) if appropriate for the summary.\\n    *   Using lists (ordered or unordered) if the summary benefits from them.\\n    *   Using emphasis (bold, italics) where needed.\\n6.  **Accuracy:** Ensure the TL;DR summary is accurate and the resulting Markdown is correct and renders as intended in a GitHub environment.\\n7.  **Conciseness:** Strive for the most concise and efficient representation.\\n8.  **No Additional Information:**  Do NOT add any extra text, comments, or explanations. Only return the Markdown output of the TL;DR summary.'}, {'role': 'user', 'content': '\\n    Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\\n    As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488804.799354, "msecs": 799.0, "relativeCreated": 19398.558139801025, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.799674, "msecs": 799.0, "relativeCreated": 19398.87809753418, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.7998059, "msecs": 799.0, "relativeCreated": 19399.009943008423, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.799823, "msecs": 799.0, "relativeCreated": 19399.027109146118, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.799885, "msecs": 799.0, "relativeCreated": 19399.089097976685, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.799902, "msecs": 799.0, "relativeCreated": 19399.1060256958, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "httpcore.http11", "msg": "receive_response_headers.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.799938, "msecs": 799.0, "relativeCreated": 19399.142026901245, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.799953, "msecs": 799.0, "relativeCreated": 19399.15704727173, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "httpcore.http11", "msg": "response_closed.failed exception=RuntimeError('Event loop is closed')", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.8000078, "msecs": 800.0, "relativeCreated": 19399.211883544922, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488804.8000362, "msecs": 800.0, "relativeCreated": 19399.240255355835, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Logging Details: logger_fn - None | callable(logger_fn) - False", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", "filename": "exception_mapping_utils.py", "module": "exception_mapping_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2243, "funcName": "exception_logging", "created": 1749488804.8001459, "msecs": 800.0, "relativeCreated": 19399.34992790222, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Logging Details LiteLLM-Failure Call: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2032, "funcName": "failure_handler", "created": 1749488804.801256, "msecs": 801.0, "relativeCreated": 19400.46000480652, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-192", "asctime": "22:36:44"}], "longrepr": "self = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-2646a9eb-0aae-4889-a8ff-b70acf123376', created=1749488804, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14b350>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text...very, and adventure.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n>           response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:187: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py:135: in async_wrapper\n    result = await func(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:258: in post\n    raise e\n.venv/lib/python3.12/site-packages/litellm/llms/custom_httpx/http_handler.py:211: in post\n    response = await self.client.send(req, stream=stream)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1629: in send\n    response = await self._send_handling_auth(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1657: in _send_handling_auth\n    response = await self._send_handling_redirects(\n.venv/lib/python3.12/site-packages/httpx/_client.py:1694: in _send_handling_redirects\n    response = await self._send_single_request(request)\n.venv/lib/python3.12/site-packages/httpx/_client.py:1730: in _send_single_request\n    response = await transport.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpx/_transports/default.py:394: in handle_async_request\n    resp = await self._pool.handle_async_request(req)\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:256: in handle_async_request\n    raise exc from None\n.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:236: in handle_async_request\n    response = await connection.handle_async_request(\n.venv/lib/python3.12/site-packages/httpcore/_async/connection.py:103: in handle_async_request\n    return await self._connection.handle_async_request(request)\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:135: in handle_async_request\n    await self._response_closed()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:250: in _response_closed\n    await self.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:258: in aclose\n    await self._network_stream.aclose()\n.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:53: in aclose\n    await self._stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/streams/tls.py:216: in aclose\n    await self.transport_stream.aclose()\n.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1314: in aclose\n    self._transport.close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:1213: in close\n    super().close()\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py:875: in close\n    self._loop.call_soon(self._call_connection_lost, None)\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:799: in call_soon\n    self._check_closed()\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <_UnixSelectorEventLoop running=False closed=True debug=False>\n\n    def _check_closed(self):\n        if self._closed:\n>           raise RuntimeError('Event loop is closed')\nE           RuntimeError: Event loop is closed\n\n../../../.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/base_events.py:545: RuntimeError\n\nDuring handling of the above exception, another exception occurred:\n\nmodel = 'groq/llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    ', 'role': 'user'}]\nfunctions = None, function_call = None, timeout = None, temperature = 0.1, top_p = None, n = None, stream = None\nstream_options = None, stop = None, max_tokens = None, max_completion_tokens = None, modalities = None, prediction = None\naudio = None, presence_penalty = None, frequency_penalty = None, logit_bias = None, user = None, response_format = None\nseed = None, tools = None, tool_choice = None, parallel_tool_calls = None, logprobs = None, top_logprobs = None\ndeployment_id = None, reasoning_effort = None, base_url = None, api_version = None, api_key = None, model_list = None\nextra_headers = None, thinking = None\nkwargs = {'litellm_call_id': 'ef092c7e-9e90-457a-a1c7-45ea09f0bac9', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14b350>}\nfallbacks = None, mock_timeout = None, loop = <_UnixSelectorEventLoop running=False closed=False debug=False>\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\n_ = 'https://api.groq.com/openai/v1'\n\n    @client\n    async def acompletion(\n        model: str,\n        # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n        messages: List = [],\n        functions: Optional[List] = None,\n        function_call: Optional[str] = None,\n        timeout: Optional[Union[float, int]] = None,\n        temperature: Optional[float] = None,\n        top_p: Optional[float] = None,\n        n: Optional[int] = None,\n        stream: Optional[bool] = None,\n        stream_options: Optional[dict] = None,\n        stop=None,\n        max_tokens: Optional[int] = None,\n        max_completion_tokens: Optional[int] = None,\n        modalities: Optional[List[ChatCompletionModality]] = None,\n        prediction: Optional[ChatCompletionPredictionContentParam] = None,\n        audio: Optional[ChatCompletionAudioParam] = None,\n        presence_penalty: Optional[float] = None,\n        frequency_penalty: Optional[float] = None,\n        logit_bias: Optional[dict] = None,\n        user: Optional[str] = None,\n        # openai v1.0+ new params\n        response_format: Optional[Union[dict, Type[BaseModel]]] = None,\n        seed: Optional[int] = None,\n        tools: Optional[List] = None,\n        tool_choice: Optional[str] = None,\n        parallel_tool_calls: Optional[bool] = None,\n        logprobs: Optional[bool] = None,\n        top_logprobs: Optional[int] = None,\n        deployment_id=None,\n        reasoning_effort: Optional[Literal[\"low\", \"medium\", \"high\"]] = None,\n        # set api_base, api_version, api_key\n        base_url: Optional[str] = None,\n        api_version: Optional[str] = None,\n        api_key: Optional[str] = None,\n        model_list: Optional[list] = None,  # pass in a list of api_base,keys, etc.\n        extra_headers: Optional[dict] = None,\n        # Optional liteLLM function params\n        thinking: Optional[AnthropicThinkingParam] = None,\n        **kwargs,\n    ) -> Union[ModelResponse, CustomStreamWrapper]:\n        \"\"\"\n        Asynchronously executes a litellm.completion() call for any of litellm supported llms (example gpt-4, gpt-3.5-turbo, claude-2, command-nightly)\n    \n        Parameters:\n            model (str): The name of the language model to use for text completion. see all supported LLMs: https://docs.litellm.ai/docs/providers/\n            messages (List): A list of message objects representing the conversation context (default is an empty list).\n    \n            OPTIONAL PARAMS\n            functions (List, optional): A list of functions to apply to the conversation messages (default is an empty list).\n            function_call (str, optional): The name of the function to call within the conversation (default is an empty string).\n            temperature (float, optional): The temperature parameter for controlling the randomness of the output (default is 1.0).\n            top_p (float, optional): The top-p parameter for nucleus sampling (default is 1.0).\n            n (int, optional): The number of completions to generate (default is 1).\n            stream (bool, optional): If True, return a streaming response (default is False).\n            stream_options (dict, optional): A dictionary containing options for the streaming response. Only use this if stream is True.\n            stop(string/list, optional): - Up to 4 sequences where the LLM API will stop generating further tokens.\n            max_tokens (integer, optional): The maximum number of tokens in the generated completion (default is infinity).\n            max_completion_tokens (integer, optional): An upper bound for the number of tokens that can be generated for a completion, including visible output tokens and reasoning tokens.\n            modalities (List[ChatCompletionModality], optional): Output types that you would like the model to generate for this request. You can use `[\"text\", \"audio\"]`\n            prediction (ChatCompletionPredictionContentParam, optional): Configuration for a Predicted Output, which can greatly improve response times when large parts of the model response are known ahead of time. This is most common when you are regenerating a file with only minor changes to most of the content.\n            audio (ChatCompletionAudioParam, optional): Parameters for audio output. Required when audio output is requested with modalities: [\"audio\"]\n            presence_penalty (float, optional): It is used to penalize new tokens based on their existence in the text so far.\n            frequency_penalty: It is used to penalize new tokens based on their frequency in the text so far.\n            logit_bias (dict, optional): Used to modify the probability of specific tokens appearing in the completion.\n            user (str, optional):  A unique identifier representing your end-user. This can help the LLM provider to monitor and detect abuse.\n            metadata (dict, optional): Pass in additional metadata to tag your completion calls - eg. prompt version, details, etc.\n            api_base (str, optional): Base URL for the API (default is None).\n            api_version (str, optional): API version (default is None).\n            api_key (str, optional): API key (default is None).\n            model_list (list, optional): List of api base, version, keys\n            timeout (float, optional): The maximum execution time in seconds for the completion request.\n    \n            LITELLM Specific Params\n            mock_response (str, optional): If provided, return a mock completion response for testing or debugging purposes (default is None).\n            custom_llm_provider (str, optional): Used for Non-OpenAI LLMs, Example usage for bedrock, set model=\"amazon.titan-tg1-large\" and custom_llm_provider=\"bedrock\"\n        Returns:\n            ModelResponse: A response object containing the generated completion and associated metadata.\n    \n        Notes:\n            - This function is an asynchronous version of the `completion` function.\n            - The `completion` function is called using `run_in_executor` to execute synchronously in the event loop.\n            - If `stream` is True, the function returns an async generator that yields completion lines.\n        \"\"\"\n        fallbacks = kwargs.get(\"fallbacks\", None)\n        mock_timeout = kwargs.get(\"mock_timeout\", None)\n    \n        if mock_timeout is True:\n            await _handle_mock_timeout_async(mock_timeout, timeout, model)\n    \n        loop = asyncio.get_event_loop()\n        custom_llm_provider = kwargs.get(\"custom_llm_provider\", None)\n        # Adjusted to use explicit arguments instead of *args and **kwargs\n        completion_kwargs = {\n            \"model\": model,\n            \"messages\": messages,\n            \"functions\": functions,\n            \"function_call\": function_call,\n            \"timeout\": timeout,\n            \"temperature\": temperature,\n            \"top_p\": top_p,\n            \"n\": n,\n            \"stream\": stream,\n            \"stream_options\": stream_options,\n            \"stop\": stop,\n            \"max_tokens\": max_tokens,\n            \"max_completion_tokens\": max_completion_tokens,\n            \"modalities\": modalities,\n            \"prediction\": prediction,\n            \"audio\": audio,\n            \"presence_penalty\": presence_penalty,\n            \"frequency_penalty\": frequency_penalty,\n            \"logit_bias\": logit_bias,\n            \"user\": user,\n            \"response_format\": response_format,\n            \"seed\": seed,\n            \"tools\": tools,\n            \"tool_choice\": tool_choice,\n            \"parallel_tool_calls\": parallel_tool_calls,\n            \"logprobs\": logprobs,\n            \"top_logprobs\": top_logprobs,\n            \"deployment_id\": deployment_id,\n            \"base_url\": base_url,\n            \"api_version\": api_version,\n            \"api_key\": api_key,\n            \"model_list\": model_list,\n            \"reasoning_effort\": reasoning_effort,\n            \"extra_headers\": extra_headers,\n            \"acompletion\": True,  # assuming this is a required parameter\n            \"thinking\": thinking,\n        }\n        if custom_llm_provider is None:\n            _, custom_llm_provider, _, _ = get_llm_provider(\n                model=model, api_base=completion_kwargs.get(\"base_url\", None)\n            )\n    \n        fallbacks = fallbacks or litellm.model_fallbacks\n        if fallbacks is not None:\n            response = await async_completion_with_fallbacks(\n                **completion_kwargs, kwargs={\"fallbacks\": fallbacks, **kwargs}\n            )\n            if response is None:\n                raise Exception(\n                    \"No response from fallbacks. Got none. Turn on `litellm.set_verbose=True` to see more details.\"\n                )\n            return response\n    \n        try:\n            # Use a partial function to pass your keyword arguments\n            func = partial(completion, **completion_kwargs, **kwargs)\n    \n            # Add the context to the function\n            ctx = contextvars.copy_context()\n            func_with_context = partial(ctx.run, func)\n    \n            init_response = await loop.run_in_executor(None, func_with_context)\n            if isinstance(init_response, dict) or isinstance(\n                init_response, ModelResponse\n            ):  ## CACHING SCENARIO\n                if isinstance(init_response, dict):\n                    response = ModelResponse(**init_response)\n                response = init_response\n            elif asyncio.iscoroutine(init_response):\n>               response = await init_response\n\n.venv/lib/python3.12/site-packages/litellm/main.py:477: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = <litellm.llms.groq.chat.handler.GroqChatCompletion object at 0x10dc337d0>, model = 'llama-3.3-70b-versatile'\nmessages = [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text into clean a...ny.\\n    This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\\n    ', 'role': 'user'}]\napi_base = 'https://api.groq.com/openai/v1/chat/completions', custom_prompt_dict = {}\nmodel_response = ModelResponse(id='chatcmpl-2646a9eb-0aae-4889-a8ff-b70acf123376', created=1749488804, model=None, object='chat.complet...sage(completion_tokens=0, prompt_tokens=0, total_tokens=0, completion_tokens_details=None, prompt_tokens_details=None))\ncustom_llm_provider = 'groq', print_verbose = <function print_verbose at 0x10dc5bce0>\nclient = <litellm.llms.custom_httpx.http_handler.AsyncHTTPHandler object at 0x10a64af30>\nencoding = <Encoding 'cl100k_base'>, api_key = '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'\nlogging_obj = <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14b350>, stream = False\ndata = {'messages': [{'content': 'ROLE:\\nYou are a highly skilled Markdown converter, specializing in transforming plain text...very, and adventure.\\n    ', 'role': 'user'}], 'model': 'llama-3.3-70b-versatile', 'stream': False, 'temperature': 0.1}\nbase_model = None, optional_params = {'stream': False, 'temperature': 0.1}\nlitellm_params = {'acompletion': True, 'aembedding': None, 'api_base': 'https://api.groq.com/openai/v1', 'api_key': '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', ...}\nlogger_fn = None\nheaders = {'Authorization': 'Bearer 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', 'Content-Type': 'application/json'}\ntimeout = 600.0, json_mode = None\n\n    async def acompletion_function(\n        self,\n        model: str,\n        messages: list,\n        api_base: str,\n        custom_prompt_dict: dict,\n        model_response: ModelResponse,\n        custom_llm_provider: str,\n        print_verbose: Callable,\n        client: Optional[AsyncHTTPHandler],\n        encoding,\n        api_key,\n        logging_obj,\n        stream,\n        data: dict,\n        base_model: Optional[str],\n        optional_params: dict,\n        litellm_params=None,\n        logger_fn=None,\n        headers={},\n        timeout: Optional[Union[float, httpx.Timeout]] = None,\n        json_mode: bool = False,\n    ) -> ModelResponse:\n        if timeout is None:\n            timeout = httpx.Timeout(timeout=600.0, connect=5.0)\n    \n        if client is None:\n            client = litellm.module_level_aclient\n    \n        try:\n            response = await client.post(\n                api_base, headers=headers, data=json.dumps(data), timeout=timeout\n            )\n            response.raise_for_status()\n        except httpx.HTTPStatusError as e:\n            raise OpenAILikeError(\n                status_code=e.response.status_code,\n                message=e.response.text,\n            )\n        except httpx.TimeoutException:\n            raise OpenAILikeError(status_code=408, message=\"Timeout error occurred.\")\n        except Exception as e:\n>           raise OpenAILikeError(status_code=500, message=str(e))\nE           litellm.llms.openai_like.common_utils.OpenAILikeError: Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/llms/openai_like/chat/handler.py:199: OpenAILikeError\n\nDuring handling of the above exception, another exception occurred:\n\nsettings = Settings(with_model='groq/llama-3.3-70b-versatile')\n\n    @pytest.mark.asyncio  # type: ignore\n    async def test_fiction_excerpt_summary(settings: Any) -> None:\n        \"\"\"Test fiction excerpt summary generation.\"\"\"\n        fiction_excerpt_text = \"\"\"\n        Fiction Excerpt: In the midst of a dark, enchanted forest, young adventurer Elara discovered an ancient, forgotten path that wound through towering trees and whispering shadows.\n        As she journeyed along this mysterious trail, she encountered mystical creatures and uncovered secrets that challenged her understanding of the world and her destiny.\n        This excerpt sets the stage for an epic tale of magic, self-discovery, and adventure.\n        \"\"\"\n        only_summary_instance = OnlySummary(with_model=settings.with_model)\n>       summary_output = await only_summary_instance.summarize_and_convert_to_markdown(fiction_excerpt_text)\n\ntests/test_only_summary.py:135: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\nsrc/elevate/only_summary.py:89: in summarize_and_convert_to_markdown\n    return await self.make_llm_call(system_prompt, input_text)\nsrc/elevate/only_summary.py:50: in make_llm_call\n    response = await acompletion(model=self.model, messages=messages, temperature=0.1)\n.venv/lib/python3.12/site-packages/litellm/utils.py:1452: in wrapper_async\n    raise e\n.venv/lib/python3.12/site-packages/litellm/utils.py:1313: in wrapper_async\n    result = await original_function(*args, **kwargs)\n.venv/lib/python3.12/site-packages/litellm/main.py:496: in acompletion\n    raise exception_type(\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2214: in exception_type\n    raise e\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nmodel = 'groq/llama-3.3-70b-versatile', original_exception = OpenAILikeError('Event loop is closed')\ncustom_llm_provider = 'groq'\ncompletion_kwargs = {'acompletion': True, 'api_key': None, 'api_version': None, 'audio': None, ...}\nextra_kwargs = {'litellm_call_id': 'ef092c7e-9e90-457a-a1c7-45ea09f0bac9', 'litellm_logging_obj': <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14b350>}\n\n    def exception_type(  # type: ignore  # noqa: PLR0915\n        model,\n        original_exception,\n        custom_llm_provider,\n        completion_kwargs={},\n        extra_kwargs={},\n    ):\n        \"\"\"Maps an LLM Provider Exception to OpenAI Exception Format\"\"\"\n        if any(\n            isinstance(original_exception, exc_type)\n            for exc_type in litellm.LITELLM_EXCEPTION_TYPES\n        ):\n            return original_exception\n        exception_mapping_worked = False\n        exception_provider = custom_llm_provider\n        if litellm.suppress_debug_info is False:\n            print()  # noqa\n            print(  # noqa\n                \"\\033[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\\033[0m\"  # noqa\n            )  # noqa\n            print(  # noqa\n                \"LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\"  # noqa\n            )  # noqa\n            print()  # noqa\n    \n        litellm_response_headers = _get_response_headers(\n            original_exception=original_exception\n        )\n        try:\n            error_str = str(original_exception)\n            if model:\n                if hasattr(original_exception, \"message\"):\n                    error_str = str(original_exception.message)\n                if isinstance(original_exception, BaseException):\n                    exception_type = type(original_exception).__name__\n                else:\n                    exception_type = \"\"\n    \n                ################################################################################\n                # Common Extra information needed for all providers\n                # We pass num retries, api_base, vertex_deployment etc to the exception here\n                ################################################################################\n                extra_information = \"\"\n                try:\n                    _api_base = litellm.get_api_base(\n                        model=model, optional_params=extra_kwargs\n                    )\n                    messages = litellm.get_first_chars_messages(kwargs=completion_kwargs)\n                    _vertex_project = extra_kwargs.get(\"vertex_project\")\n                    _vertex_location = extra_kwargs.get(\"vertex_location\")\n                    _metadata = extra_kwargs.get(\"metadata\", {}) or {}\n                    _model_group = _metadata.get(\"model_group\")\n                    _deployment = _metadata.get(\"deployment\")\n                    extra_information = f\"\\nModel: {model}\"\n    \n                    if (\n                        isinstance(custom_llm_provider, str)\n                        and len(custom_llm_provider) > 0\n                    ):\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if _api_base:\n                        extra_information += f\"\\nAPI Base: `{_api_base}`\"\n                    if (\n                        messages\n                        and len(messages) > 0\n                        and litellm.redact_messages_in_exceptions is False\n                    ):\n                        extra_information += f\"\\nMessages: `{messages}`\"\n    \n                    if _model_group is not None:\n                        extra_information += f\"\\nmodel_group: `{_model_group}`\\n\"\n                    if _deployment is not None:\n                        extra_information += f\"\\ndeployment: `{_deployment}`\\n\"\n                    if _vertex_project is not None:\n                        extra_information += f\"\\nvertex_project: `{_vertex_project}`\\n\"\n                    if _vertex_location is not None:\n                        extra_information += f\"\\nvertex_location: `{_vertex_location}`\\n\"\n    \n                    # on litellm proxy add key name + team to exceptions\n                    extra_information = _add_key_name_and_team_to_alert(\n                        request_info=extra_information, metadata=_metadata\n                    )\n                except Exception:\n                    # DO NOT LET this Block raising the original exception\n                    pass\n    \n                ################################################################################\n                # End of Common Extra information Needed for all providers\n                ################################################################################\n    \n                ################################################################################\n                #################### Start of Provider Exception mapping ####################\n                ################################################################################\n    \n                if (\n                    \"Request Timeout Error\" in error_str\n                    or \"Request timed out\" in error_str\n                    or \"Timed out generating response\" in error_str\n                    or \"The read operation timed out\" in error_str\n                ):\n                    exception_mapping_worked = True\n    \n                    raise Timeout(\n                        message=f\"APITimeoutError - Request timed out. Error_str: {error_str}\",\n                        model=model,\n                        llm_provider=custom_llm_provider,\n                        litellm_debug_info=extra_information,\n                    )\n    \n                if (\n                    custom_llm_provider == \"litellm_proxy\"\n                ):  # handle special case where calling litellm proxy + exception str contains error message\n                    extract_and_raise_litellm_exception(\n                        response=getattr(original_exception, \"response\", None),\n                        error_str=error_str,\n                        model=model,\n                        custom_llm_provider=custom_llm_provider,\n                    )\n                if (\n                    custom_llm_provider == \"openai\"\n                    or custom_llm_provider == \"text-completion-openai\"\n                    or custom_llm_provider == \"custom_openai\"\n                    or custom_llm_provider in litellm.openai_compatible_providers\n                ):\n                    # custom_llm_provider is openai, make it OpenAI\n                    message = get_error_message(error_obj=original_exception)\n                    if message is None:\n                        if hasattr(original_exception, \"message\"):\n                            message = original_exception.message\n                        else:\n                            message = str(original_exception)\n    \n                    if message is not None and isinstance(\n                        message, str\n                    ):  # done to prevent user-confusion. Relevant issue - https://github.com/BerriAI/litellm/issues/1414\n                        message = message.replace(\"OPENAI\", custom_llm_provider.upper())\n                        message = message.replace(\n                            \"openai.OpenAIError\",\n                            \"{}.{}Error\".format(custom_llm_provider, custom_llm_provider),\n                        )\n                    if custom_llm_provider == \"openai\":\n                        exception_provider = \"OpenAI\" + \"Exception\"\n                    else:\n                        exception_provider = (\n                            custom_llm_provider[0].upper()\n                            + custom_llm_provider[1:]\n                            + \"Exception\"\n                        )\n    \n                    if (\n                        \"This model's maximum context length is\" in error_str\n                        or \"string too long. Expected a string with maximum length\"\n                        in error_str\n                        or \"model's maximum context limit\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContextWindowExceededError(\n                            message=f\"ContextWindowExceededError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"model_not_found\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise NotFoundError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"A timeout occurred\" in error_str:\n                        exception_mapping_worked = True\n                        raise Timeout(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"content_policy_violation\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise ContentPolicyViolationError(\n                            message=f\"ContentPolicyViolationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"invalid_request_error\" in error_str\n                        and \"Incorrect API key provided\" not in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise BadRequestError(\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                            body=getattr(original_exception, \"body\", None),\n                        )\n                    elif (\n                        \"Web server is returning an unknown error\" in error_str\n                        or \"The server had an error processing your request.\" in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise litellm.InternalServerError(\n                            message=f\"{exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                        )\n                    elif \"Request too large\" in error_str:\n                        exception_mapping_worked = True\n                        raise RateLimitError(\n                            message=f\"RateLimitError: {exception_provider} - {message}\",\n                            model=model,\n                            llm_provider=custom_llm_provider,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif (\n                        \"The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\"\n                        in error_str\n                    ):\n                        exception_mapping_worked = True\n                        raise AuthenticationError(\n                            message=f\"AuthenticationError: {exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            response=getattr(original_exception, \"response\", None),\n                            litellm_debug_info=extra_information,\n                        )\n                    elif \"Mistral API raised a streaming error\" in error_str:\n                        exception_mapping_worked = True\n                        _request = httpx.Request(\n                            method=\"POST\", url=\"https://api.openai.com/v1\"\n                        )\n                        raise APIError(\n                            status_code=500,\n                            message=f\"{exception_provider} - {message}\",\n                            llm_provider=custom_llm_provider,\n                            model=model,\n                            request=_request,\n                            litellm_debug_info=extra_information,\n                        )\n                    elif hasattr(original_exception, \"status_code\"):\n                        exception_mapping_worked = True\n                        if original_exception.status_code == 400:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 401:\n                            exception_mapping_worked = True\n                            raise AuthenticationError(\n                                message=f\"AuthenticationError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 404:\n                            exception_mapping_worked = True\n                            raise NotFoundError(\n                                message=f\"NotFoundError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 408:\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 422:\n                            exception_mapping_worked = True\n                            raise BadRequestError(\n                                message=f\"{exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                                body=getattr(original_exception, \"body\", None),\n                            )\n                        elif original_exception.status_code == 429:\n                            exception_mapping_worked = True\n                            raise RateLimitError(\n                                message=f\"RateLimitError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 503:\n                            exception_mapping_worked = True\n                            raise ServiceUnavailableError(\n                                message=f\"ServiceUnavailableError: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                response=getattr(original_exception, \"response\", None),\n                                litellm_debug_info=extra_information,\n                            )\n                        elif original_exception.status_code == 504:  # gateway timeout error\n                            exception_mapping_worked = True\n                            raise Timeout(\n                                message=f\"Timeout Error: {exception_provider} - {message}\",\n                                model=model,\n                                llm_provider=custom_llm_provider,\n                                litellm_debug_info=extra_information,\n                            )\n                        else:\n                            exception_mapping_worked = True\n>                           raise APIError(\n                                status_code=original_exception.status_code,\n                                message=f\"APIError: {exception_provider} - {message}\",\n                                llm_provider=custom_llm_provider,\n                                model=model,\n                                request=getattr(original_exception, \"request\", None),\n                                litellm_debug_info=extra_information,\n                            )\nE                           litellm.exceptions.APIError: litellm.APIError: APIError: GroqException - Event loop is closed\n\n.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:455: APIError"}, "teardown": {"duration": 0.0003361249982845038, "outcome": "passed"}}, {"nodeid": "tests/test_only_video.py::test_simple_blog_generation", "lineno": 36, "outcome": "passed", "keywords": ["test_simple_blog_generation", "asyncio", "pytestmark", "test_only_video.py", "tests", "elevate", ""], "setup": {"duration": 0.0002367919951211661, "outcome": "passed", "stdout": "DEBUG: Using selector: KqueueSelector\n", "log": [{"name": "asyncio", "msg": "Using selector: KqueueSelector", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " .local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/asyncio/selector_events.py", "filename": "selector_events.py", "module": "selector_events", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 64, "funcName": "__init__", "created": 1749488804.944154, "msecs": 944.0, "relativeCreated": 19543.358087539673, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": null}]}, "call": {"duration": 0.7583229589799885, "outcome": "passed", "stdout": "DEBUG: \n\nDEBUG: \u001b[92mRequest to litellm:\u001b[0m\nDEBUG: \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], temperature=0.1)\u001b[0m\nDEBUG: \n\nDEBUG: self.optional_params: {}\nDEBUG: ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\nDEBUG: CACHE RESULT: None\nINFO: \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\nDEBUG: \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], 'thinking': None}\nDEBUG: \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\nDEBUG: Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: self.optional_params: {'temperature': 0.1, 'extra_body': {}}\nDEBUG: \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\nDEBUG: connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None\nDEBUG: connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x128135460>\nDEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0\nDEBUG: start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f148080>\nDEBUG: send_request_headers.started request=<Request [b'POST']>\nDEBUG: send_request_headers.complete\nDEBUG: send_request_body.started request=<Request [b'POST']>\nDEBUG: send_request_body.complete\nDEBUG: receive_response_headers.started request=<Request [b'POST']>\nDEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'974'), (b'x-ratelimit-remaining-tokens', b'105'), (b'x-ratelimit-reset-requests', b'37m25.398999999s'), (b'x-ratelimit-reset-tokens', b'59.472s'), (b'x-request-id', b'req_01jxath96geht9pzbdstyjh2m3'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238a7884f4926-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\nINFO: HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\nDEBUG: receive_response_body.started request=<Request [b'POST']>\nDEBUG: receive_response_body.complete\nDEBUG: response_closed.started\nDEBUG: response_closed.complete\nDEBUG: `logging_obj` not found - unable to track `llm_api_duration_ms\nDEBUG: RAW RESPONSE:\n{\"id\": \"chatcmpl-0fea3fcc-840e-43f2-a2b2-52de0623d06f\", \"object\": \"chat.completion\", \"created\": 1749488805, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"It seems like you forgot to provide the video transcript. Please share the transcript, and I'll create a compelling blog post that weaves a fictional story around the key concepts, making it easy to understand and relatable for your readers.\\n\\nOnce I receive the transcript, I'll get started on crafting a narrative that:\\n\\n* Explains the video's content in a clear and concise manner\\n* Uses a storytelling approach to make the information more engaging and memorable\\n* Emulates the tone and style of a CXO sharing a relevant anecdote\\n* Maintains the factual accuracy of the video's content\\n\\nPlease provide the transcript, and I'll create a high-quality blog post that's ready to publish.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.04935070700000001, \"prompt_tokens\": 342, \"prompt_time\": 0.030496973, \"completion_tokens\": 141, \"completion_time\": 0.512727273, \"total_tokens\": 483, \"total_time\": 0.543224246}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath96geht9pzbdstyjh2m3\"}}\n\n\nDEBUG: Filtered callbacks: []\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00020177999999999998, completion_tokens_cost_usd_dollar: 0.00011138999999999999\nDEBUG: response_cost: 0.00031317\nDEBUG: It seems like you forgot to provide the video transcript. Please share the transcript, and I'll create a compelling blog post that weaves a fictional story around the key concepts, making it easy to understand and relatable for your readers.\n\nOnce I receive the transcript, I'll get started on crafting a narrative that:\n\n* Explains the video's content in a clear and concise manner\n* Uses a storytelling approach to make the information more engaging and memorable\n* Emulates the tone and style of a CXO sharing a relevant anecdote\n* Maintains the factual accuracy of the video's content\n\nPlease provide the transcript, and I'll create a high-quality blog post that's ready to publish.\nDEBUG: Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14b650>>\nDEBUG: Filtered callbacks: []\n", "stderr": "\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mRequest to litellm:\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], temperature=0.1)\u001b[0m\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - \n\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: caching_handler.py:210 - CACHE RESULT: None\n\u001b[92m22:36:44 - LiteLLM:INFO\u001b[0m: utils.py:3085 - \nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3088 - \nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], 'thinking': None}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:3091 - \nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Final returned optional params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:423 - self.optional_params: {'temperature': 0.1, 'extra_body': {}}\n\u001b[92m22:36:44 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:747 - \u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: logging_utils.py:117 - `logging_obj` not found - unable to track `llm_api_duration_ms\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - RAW RESPONSE:\n{\"id\": \"chatcmpl-0fea3fcc-840e-43f2-a2b2-52de0623d06f\", \"object\": \"chat.completion\", \"created\": 1749488805, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"It seems like you forgot to provide the video transcript. Please share the transcript, and I'll create a compelling blog post that weaves a fictional story around the key concepts, making it easy to understand and relatable for your readers.\\n\\nOnce I receive the transcript, I'll get started on crafting a narrative that:\\n\\n* Explains the video's content in a clear and concise manner\\n* Uses a storytelling approach to make the information more engaging and memorable\\n* Emulates the tone and style of a CXO sharing a relevant anecdote\\n* Maintains the factual accuracy of the video's content\\n\\nPlease provide the transcript, and I'll create a high-quality blog post that's ready to publish.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.04935070700000001, \"prompt_tokens\": 342, \"prompt_time\": 0.030496973, \"completion_tokens\": 141, \"completion_time\": 0.512727273, \"total_tokens\": 483, \"total_time\": 0.543224246}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath96geht9pzbdstyjh2m3\"}}\n\n\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n\u001b[92m22:36:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00020177999999999998, completion_tokens_cost_usd_dollar: 0.00011138999999999999\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031317\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14b650>>\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:2382 - Filtered callbacks: []\n", "log": [{"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.944386, "msecs": 944.0, "relativeCreated": 19543.59006881714, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mRequest to litellm:\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.9444292, "msecs": 944.0, "relativeCreated": 19543.633222579956, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92mlitellm.acompletion(api_key='', model='groq/llama-3.3-70b-versatile', messages=[{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], temperature=0.1)\u001b[0m", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.944454, "msecs": 944.0, "relativeCreated": 19543.658018112183, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.944472, "msecs": 944.0, "relativeCreated": 19543.676137924194, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.944545, "msecs": 944.0, "relativeCreated": 19543.7490940094, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "ASYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache'): None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.944566, "msecs": 944.0, "relativeCreated": 19543.77007484436, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "CACHE RESULT: None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/caching_handler.py", "filename": "caching_handler.py", "module": "caching_handler", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 210, "funcName": "_async_get_cache", "created": 1749488804.944583, "msecs": 944.0, "relativeCreated": 19543.787002563477, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM completion() model= llama-3.3-70b-versatile; provider = groq", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3085, "funcName": "_check_valid_arg", "created": 1749488804.944813, "msecs": 944.0, "relativeCreated": 19544.01707649231, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Params passed to completion() {'model': 'llama-3.3-70b-versatile', 'functions': None, 'function_call': None, 'temperature': 0.1, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'groq', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'additional_drop_params': None, 'messages': [{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], 'thinking': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3088, "funcName": "_check_valid_arg", "created": 1749488804.944853, "msecs": 944.0, "relativeCreated": 19544.0571308136, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\nLiteLLM: Non-Default params passed to completion() {'temperature': 0.1}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 3091, "funcName": "_check_valid_arg", "created": 1749488804.9449039, "msecs": 944.0, "relativeCreated": 19544.107913970947, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "Final returned optional params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488804.9449348, "msecs": 944.0, "relativeCreated": 19544.13890838623, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "self.optional_params: {'temperature': 0.1, 'extra_body': {}}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 423, "funcName": "update_environment_variables", "created": 1749488804.944962, "msecs": 944.0, "relativeCreated": 19544.166088104248, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "LiteLLM", "msg": "\u001b[92m\n\nPOST Request Sent from LiteLLM:\ncurl -X POST \\\nhttps://api.groq.com/openai/v1/chat/completions \\\n-H 'Content-Type: ap****on' -H 'Authorization: Be****oY' \\\n-d '{'model': 'llama-3.3-70b-versatile', 'messages': [{'role': 'system', 'content': \"\\n        You are a highly skilled blog writer specializing in transforming complex information into engaging and easily\\n        digestible content.  Your task is to create a compelling blog post based on a video transcript provided by the\\n        user.\\n\\n        The goal is to explain the video's content through the lens of a light, fictional story. Think of how a CEO or\\n        other executive might use a relatable narrative to illustrate a point.  This story should be interwoven seamlessly\\n        with explanations of the key concepts from the video.\\n\\n        Specifically:\\n\\n        *   **Focus on Clarity:**  Ensure the blog post is easy to understand, even for readers unfamiliar with the\\n        video's topic.\\n        *   **Storytelling Emphasis:**  The fictional narrative should be the *primary* method of explaining the video's\\n        content, not just an afterthought.  Consider using characters, a specific scenario, or a problem/solution\\n        framework.\\n        *   **Executive Storytelling Style:**  Emulate the tone and style of a CXO sharing a relevant anecdote. This\\n        includes:\\n            *   A relatable and human element.\\n            *   A clear takeaway message or lesson.\\n            *   A focus on the big picture and practical applications.\\n        *   **Accuracy:**  Maintain the factual accuracy of the video's content while framing it within the fictional story.\\n\\n        The user will provide the video transcript.  Your output should be a complete and ready-to-publish blog post. \"}, {'role': 'user', 'content': ' '}], 'temperature': 0.1, 'stream': False}'\n\u001b[0m\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 747, "funcName": "_print_llm_call_debugging_log", "created": 1749488804.9450119, "msecs": 945.0, "relativeCreated": 19544.21591758728, "thread": 6107131904, "threadName": "asyncio_0", "processName": "MainProcess", "process": 58446, "taskName": null, "asctime": "22:36:44"}, {"name": "httpcore.connection", "msg": "connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=600.0 socket_options=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.945306, "msecs": 945.0, "relativeCreated": 19544.51012611389, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.connection", "msg": "connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x128135460>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.953299, "msecs": 953.0, "relativeCreated": 19552.50310897827, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.connection", "msg": "start_tls.started ssl_context=<ssl.SSLContext object at 0x10a7184d0> server_hostname='api.groq.com' timeout=600.0", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.9533381, "msecs": 953.0, "relativeCreated": 19552.542209625244, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.connection", "msg": "start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x10f148080>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.9672608, "msecs": 967.0, "relativeCreated": 19566.46490097046, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "send_request_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.967351, "msecs": 967.0, "relativeCreated": 19566.55502319336, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "send_request_headers.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.9674509, "msecs": 967.0, "relativeCreated": 19566.654920578003, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "send_request_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.967468, "msecs": 967.0, "relativeCreated": 19566.6720867157, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "send_request_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.9675362, "msecs": 967.0, "relativeCreated": 19566.74027442932, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "receive_response_headers.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488804.967551, "msecs": 967.0, "relativeCreated": 19566.755056381226, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Mon, 09 Jun 2025 17:06:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'gcp-asia-south1'), (b'x-ratelimit-limit-requests', b'1000'), (b'x-ratelimit-limit-tokens', b'12000'), (b'x-ratelimit-remaining-requests', b'974'), (b'x-ratelimit-remaining-tokens', b'105'), (b'x-ratelimit-reset-requests', b'37m25.398999999s'), (b'x-ratelimit-reset-tokens', b'59.472s'), (b'x-request-id', b'req_01jxath96geht9pzbdstyjh2m3'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94d238a7884f4926-BOM'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488805.6982028, "msecs": 698.0, "relativeCreated": 20297.406911849976, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpx", "msg": "HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_client.py", "filename": "_client.py", "module": "_client", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1740, "funcName": "_send_single_request", "created": 1749488805.698826, "msecs": 698.0, "relativeCreated": 20298.030138015747, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "receive_response_body.started request=<Request [b'POST']>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488805.699031, "msecs": 699.0, "relativeCreated": 20298.235177993774, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "receive_response_body.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488805.699392, "msecs": 699.0, "relativeCreated": 20298.596143722534, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "response_closed.started", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488805.699494, "msecs": 699.0, "relativeCreated": 20298.69794845581, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "httpcore.http11", "msg": "response_closed.complete", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpcore/_trace.py", "filename": "_trace.py", "module": "_trace", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 87, "funcName": "atrace", "created": 1749488805.699604, "msecs": 699.0, "relativeCreated": 20298.808097839355, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "LiteLLM", "msg": "`logging_obj` not found - unable to track `llm_api_duration_ms", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", "filename": "logging_utils.py", "module": "logging_utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 117, "funcName": "_set_duration_in_model_call_details", "created": 1749488805.69982, "msecs": 699.0, "relativeCreated": 20299.02410507202, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "RAW RESPONSE:\n{\"id\": \"chatcmpl-0fea3fcc-840e-43f2-a2b2-52de0623d06f\", \"object\": \"chat.completion\", \"created\": 1749488805, \"model\": \"llama-3.3-70b-versatile\", \"choices\": [{\"index\": 0, \"message\": {\"role\": \"assistant\", \"content\": \"It seems like you forgot to provide the video transcript. Please share the transcript, and I'll create a compelling blog post that weaves a fictional story around the key concepts, making it easy to understand and relatable for your readers.\\n\\nOnce I receive the transcript, I'll get started on crafting a narrative that:\\n\\n* Explains the video's content in a clear and concise manner\\n* Uses a storytelling approach to make the information more engaging and memorable\\n* Emulates the tone and style of a CXO sharing a relevant anecdote\\n* Maintains the factual accuracy of the video's content\\n\\nPlease provide the transcript, and I'll create a high-quality blog post that's ready to publish.\"}, \"logprobs\": null, \"finish_reason\": \"stop\"}], \"usage\": {\"queue_time\": 0.04935070700000001, \"prompt_tokens\": 342, \"prompt_time\": 0.030496973, \"completion_tokens\": 141, \"completion_time\": 0.512727273, \"total_tokens\": 483, \"total_time\": 0.543224246}, \"usage_breakdown\": {\"models\": null}, \"system_fingerprint\": \"fp_2ddfbb0da0\", \"x_groq\": {\"id\": \"req_01jxath96geht9pzbdstyjh2m3\"}}\n\n", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488805.700245, "msecs": 700.0, "relativeCreated": 20299.44896697998, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488805.700982, "msecs": 700.0, "relativeCreated": 20300.186157226562, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488805.701366, "msecs": 701.0, "relativeCreated": 20300.570011138916, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488805.701545, "msecs": 701.0, "relativeCreated": 20300.74906349182, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00020177999999999998, completion_tokens_cost_usd_dollar: 0.00011138999999999999", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488805.7016509, "msecs": 701.0, "relativeCreated": 20300.854921340942, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031317", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488805.7017589, "msecs": 701.0, "relativeCreated": 20300.962924957275, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195", "asctime": "22:36:45"}, {"name": "root", "msg": "It seems like you forgot to provide the video transcript. Please share the transcript, and I'll create a compelling blog post that weaves a fictional story around the key concepts, making it easy to understand and relatable for your readers.\n\nOnce I receive the transcript, I'll get started on crafting a narrative that:\n\n* Explains the video's content in a clear and concise manner\n* Uses a storytelling approach to make the information more engaging and memorable\n* Emulates the tone and style of a CXO sharing a relevant anecdote\n* Maintains the factual accuracy of the video's content\n\nPlease provide the transcript, and I'll create a high-quality blog post that's ready to publish.", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/tests/test_only_video.py", "filename": "test_only_video.py", "module": "test_only_video", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 44, "funcName": "test_simple_blog_generation", "created": 1749488805.701879, "msecs": 701.0, "relativeCreated": 20301.083087921143, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-195"}, {"name": "LiteLLM", "msg": "Async Wrapper: Completed Call, calling async_success_handler: <bound method Logging.async_success_handler of <litellm.litellm_core_utils.litellm_logging.Logging object at 0x10f14b650>>", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488805.7020218, "msecs": 702.0, "relativeCreated": 20301.225900650024, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-197", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "Filtered callbacks: []", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 2382, "funcName": "_remove_internal_litellm_callbacks", "created": 1749488805.702154, "msecs": 702.0, "relativeCreated": 20301.357984542847, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-197", "asctime": "22:36:45"}]}, "teardown": {"duration": 0.0027738749922718853, "outcome": "passed", "stdout": "DEBUG: Logging Details LiteLLM-Async Success Call, cache_hit=None\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\nDEBUG: Async success callbacks: Got a complete streaming response\nINFO: selected model name for cost calculation: groq/llama-3.3-70b-versatile\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00020177999999999998, completion_tokens_cost_usd_dollar: 0.00011138999999999999\nDEBUG: response_cost: 0.00031317\nDEBUG: Model=llama-3.3-70b-versatile; cost=0.00031317\nDEBUG: checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\nDEBUG: model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "stderr": "\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Logging Details LiteLLM-Async Success Call, cache_hit=None\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:324 - Async success callbacks: Got a complete streaming response\n\u001b[92m22:36:45 - LiteLLM:INFO\u001b[0m: cost_calculator.py:636 - selected model name for cost calculation: groq/llama-3.3-70b-versatile\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:357 - Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00020177999999999998, completion_tokens_cost_usd_dollar: 0.00011138999999999999\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:969 - response_cost: 0.00031317\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1754 - Model=llama-3.3-70b-versatile; cost=0.00031317\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4413 - checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}\n\u001b[92m22:36:45 - LiteLLM:DEBUG\u001b[0m: utils.py:4706 - model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}\n", "log": [{"name": "LiteLLM", "msg": "Logging Details LiteLLM-Async Success Call, cache_hit=None", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488805.7031438, "msecs": 703.0, "relativeCreated": 20302.347898483276, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488805.703567, "msecs": 703.0, "relativeCreated": 20302.77109146118, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488805.7036679, "msecs": 703.0, "relativeCreated": 20302.87194252014, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "Async success callbacks: Got a complete streaming response", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 324, "funcName": "print_verbose", "created": 1749488805.703845, "msecs": 703.0, "relativeCreated": 20303.049087524414, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "selected model name for cost calculation: groq/llama-3.3-70b-versatile", "args": null, "levelname": "INFO", "levelno": 20, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 636, "funcName": "completion_cost", "created": 1749488805.703913, "msecs": 703.0, "relativeCreated": 20303.117036819458, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488805.703984, "msecs": 703.0, "relativeCreated": 20303.18808555603, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "Returned custom cost for model=groq/llama-3.3-70b-versatile - prompt_tokens_cost_usd_dollar: 0.00020177999999999998, completion_tokens_cost_usd_dollar: 0.00011138999999999999", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/cost_calculator.py", "filename": "cost_calculator.py", "module": "cost_calculator", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 357, "funcName": "cost_per_token", "created": 1749488805.7040582, "msecs": 704.0, "relativeCreated": 20303.26223373413, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "response_cost: 0.00031317", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 969, "funcName": "_response_cost_calculator", "created": 1749488805.704127, "msecs": 704.0, "relativeCreated": 20303.33113670349, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "Model=llama-3.3-70b-versatile; cost=0.00031317", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/litellm_logging.py", "filename": "litellm_logging.py", "module": "litellm_logging", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 1754, "funcName": "async_success_handler", "created": 1749488805.704169, "msecs": 704.0, "relativeCreated": 20303.373098373413, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "checking potential_model_names in litellm.model_cost: {'split_model': 'llama-3.3-70b-versatile', 'combined_model_name': 'groq/llama-3.3-70b-versatile', 'stripped_model_name': 'llama-3.3-70b-versatile', 'combined_stripped_model_name': 'groq/llama-3.3-70b-versatile', 'custom_llm_provider': 'groq'}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4413, "funcName": "_get_model_info_helper", "created": 1749488805.704279, "msecs": 704.0, "relativeCreated": 20303.48300933838, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}, {"name": "LiteLLM", "msg": "model_info: {'key': 'groq/llama-3.3-70b-versatile', 'max_tokens': 32768, 'max_input_tokens': 128000, 'max_output_tokens': 32768, 'input_cost_per_token': 5.9e-07, 'cache_creation_input_token_cost': None, 'cache_read_input_token_cost': None, 'input_cost_per_character': None, 'input_cost_per_token_above_128k_tokens': None, 'input_cost_per_token_above_200k_tokens': None, 'input_cost_per_query': None, 'input_cost_per_second': None, 'input_cost_per_audio_token': None, 'input_cost_per_token_batches': None, 'output_cost_per_token_batches': None, 'output_cost_per_token': 7.9e-07, 'output_cost_per_audio_token': None, 'output_cost_per_character': None, 'output_cost_per_token_above_128k_tokens': None, 'output_cost_per_character_above_128k_tokens': None, 'output_cost_per_token_above_200k_tokens': None, 'output_cost_per_second': None, 'output_cost_per_image': None, 'output_vector_size': None, 'litellm_provider': 'groq', 'mode': 'chat', 'supports_system_messages': None, 'supports_response_schema': True, 'supports_vision': False, 'supports_function_calling': True, 'supports_tool_choice': True, 'supports_assistant_prefill': False, 'supports_prompt_caching': False, 'supports_audio_input': False, 'supports_audio_output': False, 'supports_pdf_input': False, 'supports_embedding_image_input': False, 'supports_native_streaming': None, 'supports_web_search': False, 'supports_reasoning': False, 'search_context_cost_per_query': None, 'tpm': None, 'rpm': None}", "args": null, "levelname": "DEBUG", "levelno": 10, "pathname": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "filename": "utils.py", "module": "utils", "exc_info": null, "exc_text": null, "stack_info": null, "lineno": 4706, "funcName": "get_model_info", "created": 1749488805.704341, "msecs": 704.0, "relativeCreated": 20303.544998168945, "thread": 8408178432, "threadName": "MainThread", "processName": "MainProcess", "process": 58446, "taskName": "Task-198", "asctime": "22:36:45"}]}}], "warnings": [{"message": "'audioop' is deprecated and slated for removal in Python 3.13", "category": "DeprecationWarning", "when": "collect", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/pydub/utils.py", "lineno": 14}, {"message": "Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work", "category": "RuntimeWarning", "when": "collect", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/pydub/utils.py", "lineno": 170}, {"message": "Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/", "category": "PydanticDeprecatedSince20", "when": "collect", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/pydantic/_internal/_config.py", "lineno": 323}, {"message": "open_text is deprecated. Use files() instead. Refer to https://importlib-resources.readthedocs.io/en/latest/using.html#migrating-from-legacy for migration advice.", "category": "DeprecationWarning", "when": "collect", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/utils.py", "lineno": 183}, {"message": "There is no current event loop", "category": "DeprecationWarning", "when": "collect", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/litellm/caching/llm_caching_handler.py", "lineno": 17}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "coroutine 'OnlyMarkdown.convert_to_markdown' was never awaited", "category": "RuntimeWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/_pytest/stash.py", "lineno": 108}, {"message": "coroutine 'OnlyMarkdown.convert_to_markdown' was never awaited", "category": "RuntimeWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/_pytest/stash.py", "lineno": 108}, {"message": "coroutine 'OnlyMarkdown.convert_to_markdown' was never awaited", "category": "RuntimeWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/_pytest/stash.py", "lineno": 108}, {"message": "coroutine 'OnlyMarkdown.convert_to_markdown' was never awaited", "category": "RuntimeWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/_pytest/stash.py", "lineno": 108}, {"message": "coroutine 'OnlyMarkdown.convert_to_markdown' was never awaited", "category": "RuntimeWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/_pytest/stash.py", "lineno": 108}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "coroutine 'OnlyRephrase.rephrase_text' was never awaited", "category": "RuntimeWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/_pytest/stash.py", "lineno": 108}, {"message": "coroutine 'OnlyRephrase.rephrase_text' was never awaited", "category": "RuntimeWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/_pytest/stash.py", "lineno": 108}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}, {"message": "Use 'content=<...>' to upload raw bytes/text content.", "category": "DeprecationWarning", "when": "runtest", "filename": " Developer/OpenSource/elevate/.venv/lib/python3.12/site-packages/httpx/_models.py", "lineno": 408}]}
